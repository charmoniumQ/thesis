---
references:
- id: abbottUphillStruggle2006
  abstract: >-
    The first vaccine against Lyme disease was withdrawn because patients
    distrusted it. Should market forces be allowed to shape the next one, asks
    Alison Abbott.
  accessed:
    - year: 2022
      month: 8
      day: 26
  author:
    - family: Abbott
      given: Alison
  citation-key: abbottUphillStruggle2006
  container-title: Nature
  DOI: 10.1038/439524a
  ISSN: 1476-4687
  issue: '7076'
  issued:
    - year: 2006
      month: 2
      day: 1
  language: en
  license: 2006 Nature Publishing Group
  number: '7076'
  page: 524-525
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Uphill Struggle
  type: article-journal
  URL: https://www.nature.com/articles/439524a
  volume: '439'

- id: abdalkareemWhyDevelopersUse2017
  abstract: >-
    Code reuse is traditionally seen as good practice. Recent trends have pushed
    the concept of code reuse to an extreme, by using packages that implement
    simple and trivial tasks, which we call `trivial packages'. A recent
    incident where a trivial package led to the breakdown of some of the most
    popular web applications such as Facebook and Netflix made it imperative to
    question the growing use of trivial packages. Therefore, in this paper, we
    mine more than 230,000 npm packages and 38,000 JavaScript applications in
    order to study the prevalence of trivial packages. We found that trivial
    packages are common and are increasing in popularity, making up 16.8% of the
    studied npm packages. We performed a survey with 88 Node.js developers who
    use trivial packages to understand the reasons and drawbacks of their use.
    Our survey revealed that trivial packages are used because they are
    perceived to be well implemented and tested pieces of code. However,
    developers are concerned about maintaining and the risks of breakages due to
    the extra dependencies trivial packages introduce. To objectively verify the
    survey results, we empirically validate the most cited reason and drawback
    and find that, contrary to developers' beliefs, only 45.2% of trivial
    packages even have tests. However, trivial packages appear to be `deployment
    tested' and to have similar test, usage and community interest as
    non-trivial packages. On the other hand, we found that 11.5% of the studied
    trivial packages have more than 20 dependencies. Hence, developers should be
    careful about which trivial packages they decide to use.
  accessed:
    - year: 2022
      month: 4
      day: 13
  author:
    - family: Abdalkareem
      given: Rabe
    - family: Nourry
      given: Olivier
    - family: Wehaibi
      given: Sultan
    - family: Mujahid
      given: Suhaib
    - family: Shihab
      given: Emad
  citation-key: abdalkareemWhyDevelopersUse2017
  collection-title: ESEC/FSE 2017
  container-title: >-
    Proceedings of the 2017 11th Joint Meeting on Foundations of Software
    Engineering
  DOI: 10.1145/3106237.3106267
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5105-8
  issued:
    - year: 2017
      month: 8
      day: 21
  page: 385–395
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Why do developers use trivial packages? an empirical case study on npm
  title-short: Why do developers use trivial packages?
  type: paper-conference
  URL: https://doi.org/10.1145/3106237.3106267

- id: abeykoonHPTMTParallelOperators2022
  abstract: >-
    Data-intensive applications are becoming commonplace in all science
    disciplines. They are comprised of a rich set of sub-domains such as data
    engineering, deep learning, and machine learning. These applications are
    built around efficient data abstractions and operators that suit the
    applications of different domains. Often lack of a clear definition of data
    structures and operators in the field has led to other implementations that
    do not work well together. The HPTMT architecture that we proposed recently,
    identifies a set of data structures, operators, and an execution model for
    creating rich data applications that links all aspects of data engineering
    and data science together efficiently. This paper elaborates and illustrates
    this architecture using an end-to-end application with deep learning and
    data engineering parts working together. Our analysis show that the proposed
    system architecture is better suited for high performance computing
    environments compared to the current big data processing systems.
    Furthermore our proposed system emphasizes the importance of efficient
    compact data structures such as Apache Arrow tabular data representation
    defined for high performance. Thus the system integration we proposed scales
    a sequential computation to a distributed computation retaining optimum
    performance along with highly usable application programming interface.
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Abeykoon
      given: Vibhatha
    - family: Kamburugamuve
      given: Supun
    - family: Widanage
      given: Chathura
    - family: Perera
      given: Niranda
    - family: Uyar
      given: Ahmet
    - family: Kanewala
      given: Thejaka Amila
    - family: Laszewski
      given: Gregor
      non-dropping-particle: von
    - family: Fox
      given: Geoffrey
  citation-key: abeykoonHPTMTParallelOperators2022
  container-title: Frontiers in Big Data
  container-title-short: Front. Big Data
  DOI: 10.3389/fdata.2021.756041
  ISSN: 2624-909X
  issued:
    - year: 2022
      month: 2
      day: 7
  page: '756041'
  source: DOI.org (Crossref)
  title: >-
    HPTMT Parallel Operators for High Performance Data Science and Data
    Engineering
  type: article-journal
  URL: https://www.frontiersin.org/articles/10.3389/fdata.2021.756041/full
  volume: '4'

- id: abeysooriyaGeneNameErrors2021
  abstract: >-
    Erroneous conversion of gene names into other dates and other data types has
    been a frustration for computational biologists for years. We hypothesized
    that such errors in supplementary files might diminish after a report in
    2016 highlighting the extent of the problem. To assess this, we performed a
    scan of supplementary files published in PubMed Central from 2014 to 2020.
    Overall, gene name errors continued to accumulate unabated in the period
    after 2016. An improved scanning software we developed identified gene name
    errors in 30.9% (3,436/11,117) of articles with supplementary Excel gene
    lists; a figure significantly higher than previously estimated. This is due
    to gene names being converted not just to dates and floating-point numbers,
    but also to internal date format (five-digit numbers). These findings
    further reinforce that spreadsheets are ill-suited to use with large genomic
    data.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Abeysooriya
      given: Mandhri
    - family: Soria
      given: Megan
    - family: Kasu
      given: Mary Sravya
    - family: Ziemann
      given: Mark
  citation-key: abeysooriyaGeneNameErrors2021
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1008984
  ISSN: 1553-7358
  issue: '7'
  issued:
    - year: 2021
      month: 7
      day: 30
  language: en
  page: e1008984
  publisher: Public Library of Science
  source: PLoS Journals
  title: 'Gene name errors: Lessons not learned'
  title-short: Gene name errors
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008984
  volume: '17'

- id: AboutCodeScanCode
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: AboutCodeScanCode
  note: 'interest: 91'
  title: AboutCode - ScanCode
  type: webpage
  URL: https://www.aboutcode.org/projects/scancode.html

- id: abramsonRelativeDebuggingNew1996
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Abramson
      given: David
    - family: Foster
      given: Ian
    - family: Michalakes
      given: John
    - family: Sosič
      given: Rok
  citation-key: abramsonRelativeDebuggingNew1996
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/240455.240475
  ISSN: 0001-0782
  issue: '11'
  issued:
    - year: 1996
      month: 11
      day: 1
  note: 'interest: 80'
  page: 69–77
  source: Nov. 1996
  title: 'Relative debugging: a new methodology for debugging scientific applications'
  title-short: Relative debugging
  type: article-journal
  URL: https://doi.org/10.1145/240455.240475
  volume: '39'

- id: abramsonTranslationalResearchComputer2019
  abstract: >-
    There are benefits to formalizing translational computer science (TCS) to
    complement traditional modes of computer science research, as has been done
    for translational medicine. TCS has the potential to accelerate the impact
    of computer science research overall.
  author:
    - family: Abramson
      given: David
    - family: Parashar
      given: Manish
  citation-key: abramsonTranslationalResearchComputer2019
  container-title: Computer
  DOI: 10.1109/MC.2019.2925650
  ISSN: 1558-0814
  issue: '9'
  issued:
    - year: 2019
      month: 9
  note: 'interset: 89'
  page: 16-23
  source: IEEE Xplore
  title: Translational Research in Computer Science
  type: article-journal
  volume: '52'

- id: acminc.staffArtifactReviewBadging2020
  abstract: Result and Artifact Review documentation and badges - V.1.1
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: ACM Inc. staff
      given: ''
  citation-key: acminc.staffArtifactReviewBadging2020
  issued:
    - year: 2020
      month: 8
      day: 24
  language: en
  title: Artifact Review and Badging
  type: webpage
  URL: >-
    https://www.acm.org/publications/policies/artifact-review-and-badging-current

- id: adamsResearchRiskRansomware2021
  accessed:
    - year: 2022
      month: 5
      day: 23
  author:
    - family: Adams
      given: Andrew
    - family: Siu
      given: Tom
    - family: Songer
      given: Julie
    - family: Welch
      given: Von
  citation-key: adamsResearchRiskRansomware2021
  issued:
    - year: 2021
      month: 7
  language: en
  license: https://creativecommons.org/licenses/by/4.0/
  publisher: NSF Cybersecurity Center of Excellence, Trusted CI, trustedci.org
  source: scholarworks.iu.edu
  title: 'Research at Risk: Ransomware attack on Physics and Astronomy Case Study'
  title-short: Research at Risk
  type: report
  URL: http://hdl.handle.net/2022/26638

- id: adeeBadBugsWorst2013
  abstract: >-
    Clever software makes our lives easier but a glitch can have disastrous
    consequences – here are six of the worst
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Adee
      given: Sally
  citation-key: adeeBadBugsWorst2013
  container-title: New Scientist
  issued:
    - year: 2013
      month: 6
      day: 5
  language: en-US
  title: 'Bad bugs: The worst disasters caused by software fails'
  title-short: Bad bugs
  type: post-weblog
  URL: https://www.newscientist.com/gallery/software-bugs/

- id: adorfHowProfessionallyDevelop2019
  abstract: >-
    A critical challenge in scientific computing is balancing developing
    high-quality software with the need for immediate scientific progress. We
    present a flexible approach that emphasizes writing specialized code that is
    refactored only when future immediate scientific goals demand it. Our lazy
    refactoring technique, which calls for code with clearly defined interfaces
    and sharply delimited scopes to maximize reuse and integrability, helps
    reduce total development time and accelerates the production of scientific
    results. We offer guidelines for how to implement such code, as well as
    criteria to aid in the evaluation of existing tools. To demonstrate their
    application, we showcase the development progression of tools for particle
    simulations originating from the Glotzer Group at the University of
    Michigan. We emphasize the evolution of these tools into a loosely
    integrated software stack of highly reusable software that can be maintained
    to ensure the long-term stability of established research workflows.
  author:
    - family: Adorf
      given: Carl S.
    - family: Ramasubramani
      given: Vyas
    - family: Anderson
      given: Joshua A.
    - family: Glotzer
      given: Sharon C.
  citation-key: adorfHowProfessionallyDevelop2019
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2018.2882355
  ISSN: 1558-366X
  issue: '2'
  issued:
    - year: 2019
      month: 3
  note: 'interest: 87'
  page: 66-79
  source: IEEE Xplore
  title: How to Professionally Develop Reusable Scientific Software—And When Not To
  type: article-journal
  volume: '21'

- id: ahujaImplementationFOAFAIISO2019
  abstract: >-
    Web 2.0 delivers the information which is then displayed in human readable
    content, omitting the crucial information which can be drawn from the data
    by the applications. Web 3.0 or semantic web is an extension to the current
    web, with an ambition to determine the drawbacks of the current web. The
    semantic web has already proven its influence in several communities around
    the globe, such as social media, music industry, healthcare domain, online
    blogs or articles, etc.; Among the several tools and technologies,
    ontologies or vocabularies are the foundation pillar for the semantic web.
    In this paper, the developed system aims at improving the collaboration and
    academic relations among staff which is directly related to our education
    community by providing a better networking platform which lets the agents
    discuss their achievements, titles, domain interests, and various other
    activities. Results have been analyzed to show how new facts, information
    can be implied from the presented knowledge of several agents and help
    generate a relationship graph by utilizing various semantic tools. The
    system discussed in this paper processes all the information in a format
    which can be understood by both humans and the machines, to interpret the
    underlying meaning about it and provide effective results.
  accessed:
    - year: 2023
      month: 6
      day: 5
  author:
    - family: Ahuja
      given: Himanshu
    - family: R
      given: Sivakumar
  citation-key: ahujaImplementationFOAFAIISO2019
  container-title: International Journal of Electrical and Computer Engineering (IJECE)
  container-title-short: IJECE
  DOI: 10.11591/ijece.v9i5.pp4302-4310
  ISSN: 2722-2578, 2088-8708
  issue: '5'
  issued:
    - year: 2019
      month: 10
      day: 1
  page: '4302'
  source: DOI.org (Crossref)
  title: >-
    Implementation of FOAF, AIISO and DOAP ontologies for creating an academic
    community network using semantic frameworks
  type: article-journal
  URL: http://ijece.iaescore.com/index.php/IJECE/article/view/18022
  volume: '9'

- id: al-saadiExaWorksWorkflowsExascale2021
  abstract: >-
    Exascale computers will offer transformative capabilities to combine
    data-driven and learning-based approaches with traditional simulation
    applications to accelerate scientific discovery and insight. These software
    combinations and integrations, however, are difficult to achieve due to
    challenges of coordination and deployment of heterogeneous software
    components on diverse and massive platforms. We present the ExaWorks
    project, which can address many of these challenges: ExaWorks is leading a
    co-design process to create a workflow Software Development Toolkit (SDK)
    consisting of a wide range of workflow management tools that can be composed
    and interoperate through common interfaces. We describe the initial set of
    tools and interfaces supported by the SDK, efforts to make them easier to
    apply to complex science challenges, and examples of their application to
    exemplar cases. Furthermore, we discuss how our project is working with the
    workflows community, large computing facilities as well as HPC platform
    vendors to sustainably address the requirements of workflows at the
    exascale.
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Al-Saadi
      given: Aymen
    - family: Ahn
      given: Dong H.
    - family: Babuji
      given: Yadu
    - family: Chard
      given: Kyle
    - family: Corbett
      given: James
    - family: Hategan
      given: Mihael
    - family: Herbein
      given: Stephen
    - family: Jha
      given: Shantenu
    - family: Laney
      given: Daniel
    - family: Merzky
      given: Andre
    - family: Munson
      given: Todd
    - family: Salim
      given: Michael
    - family: Titov
      given: Mikhail
    - family: Turilli
      given: Matteo
    - family: Uram
      given: Thomas D.
    - family: Wozniak
      given: Justin M.
  citation-key: al-saadiExaWorksWorkflowsExascale2021
  container-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  DOI: 10.1109/WORKS54523.2021.00012
  event-place: St. Louis, MO, USA
  event-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  ISBN: 978-1-66541-136-3
  issued:
    - year: 2021
      month: 11
  page: 50-57
  publisher: IEEE
  publisher-place: St. Louis, MO, USA
  source: DOI.org (Crossref)
  title: 'ExaWorks: Workflows for Exascale'
  title-short: ExaWorks
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/9652623/

- id: alamReusabilityChallengesScientific2023
  abstract: >-
    Scientific workflow has become essential in software engineering because it
    provides a structured approach to designing, executing, and analyzing
    scientific experiments. Software developers and researchers have developed
    hundreds of scientific workflow management systems so scientists in various
    domains can benefit from them by automating repetitive tasks, enhancing
    collaboration, and ensuring the reproducibility of their results. However,
    even for expert users, workflow creation is a complex task due to the
    dramatic growth of tools and data heterogeneity. Thus, scientists attempt to
    reuse existing workflows shared in workflow repositories. Unfortunately,
    several challenges prevent scientists from reusing those workflows. In this
    study, we thus first attempted to identify those reusability challenges. We
    also offered an action list and evidence-based guidelines to promote the
    reusability of scientific workflows. Our intensive manual investigation
    examined the reusability of existing workflows and exposed several
    challenges. The challenges preventing reusability include tool upgrading,
    tool support unavailability, design flaws, incomplete workflows, failure to
    load a workflow, etc. Such challenges and our action list offered guidelines
    to future workflow composers to create better workflows with enhanced
    reusability. In the future, we plan to develop a recommender system using
    reusable workflows that can assist scientists in creating effective and
    error-free workflows.
  accessed:
    - year: 2023
      month: 10
      day: 16
  author:
    - family: Alam
      given: Khairul
    - family: Roy
      given: Banani
    - family: Serebrenik
      given: Alexander
  citation-key: alamReusabilityChallengesScientific2023
  DOI: 10.48550/arXiv.2309.07291
  issued:
    - year: 2023
      month: 9
      day: 13
  number: arXiv:2309.07291
  publisher: arXiv
  source: arXiv.org
  title: 'Reusability Challenges of Scientific Workflows: A Case Study for Galaxy'
  title-short: Reusability Challenges of Scientific Workflows
  type: article
  URL: http://arxiv.org/abs/2309.07291

- id: allberyMissingSysVinitSupport2016
  accessed:
    - year: 2023
      month: 9
      day: 27
  author:
    - family: Allbery
      given: Russ
  citation-key: allberyMissingSysVinitSupport2016
  container-title: LWN.net
  issued:
    - year: 2016
      month: 8
      day: 30
  title: Is missing SysV-init support a bug?
  type: post-weblog
  URL: https://lwn.net/Articles/698822/

- id: allenLookingLeapingCreating2015
  abstract: >-
    What lessons can be learned from examining numerous efforts to create a
    repository or directory of scientist-written software for a discipline?
    Astronomy has seen a number of efforts to build such a resource, one of
    which is the Astrophysics Source Code Library (ASCL). The ASCL (ascl.net)
    was founded in 1999, had a period of dormancy, and was restarted in 2010.
    When taking over responsibility for the ASCL in 2010, the new editor sought
    to answer the opening question, hoping this would better inform the work to
    be done. We also provide specific steps the ASCL is taking to try to improve
    code sharing and discovery in astronomy and share recent improvements to the
    resource.
  accessed:
    - year: 2023
      month: 6
      day: 16
  author:
    - family: Allen
      given: Alice
    - family: Schmidt
      given: Judy
  citation-key: allenLookingLeapingCreating2015
  DOI: 10.48550/arXiv.1407.5378
  issued:
    - year: 2015
      month: 8
      day: 18
  number: arXiv:1407.5378
  publisher: arXiv
  source: arXiv.org
  title: 'Looking before leaping: Creating a software registry'
  title-short: Looking before leaping
  type: article
  URL: http://arxiv.org/abs/1407.5378
  version: '3'

- id: alliezAttributingReferencingResearch2020
  abstract: >-
    Software is a fundamental pillar of modern scientific research, across all
    fields and disciplines. However, there is a lack of adequate means to cite
    and reference software due to the complexity of the problem in terms of
    authorship, roles, and credits. This complexity is further increased when it
    is considered over the lifetime of a software that can span up to several
    decades. Building upon the internal experience of Inria, the French research
    institute for digital sciences, we provide in this article a contribution to
    the ongoing efforts in order to develop proper guidelines and
    recommendations for software citation and reference. Namely, we recommend:
    first, a richer taxonomy for software contributions with a qualitative
    scale; second, to put humans at the heart of the evaluation; and third, to
    distinguish citation from reference.
  author:
    - family: Alliez
      given: Pierre
    - family: Cosmo
      given: Roberto Di
    - family: Guedj
      given: Benjamin
    - family: Girault
      given: Alain
    - family: Hacid
      given: Mohand-Saïd
    - family: Legrand
      given: Arnaud
    - family: Rougier
      given: Nicolas
  citation-key: alliezAttributingReferencingResearch2020
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2019.2949413
  ISSN: 1558-366X
  issue: '1'
  issued:
    - year: 2020
      month: 1
  page: 39-52
  source: IEEE Xplore
  title: >-
    Attributing and Referencing (Research) Software: Best Practices and Outlook
    From Inria
  title-short: Attributing and Referencing (Research) Software
  type: article-journal
  volume: '22'

- id: allisonReproducibilityTragedyErrors2016
  abstract: >-
    Mistakes in peer-reviewed papers are easy to find but hard to fix, report
    David B. Allison and colleagues.
  accessed:
    - year: 2022
      month: 9
      day: 27
  author:
    - family: Allison
      given: David B.
    - family: Brown
      given: Andrew W.
    - family: George
      given: Brandon J.
    - family: Kaiser
      given: Kathryn A.
  citation-key: allisonReproducibilityTragedyErrors2016
  container-title: Nature
  DOI: 10.1038/530027a
  ISSN: 1476-4687
  issue: '7588'
  issued:
    - year: 2016
      month: 2
  language: en
  license: 2016 Nature Publishing Group
  note: 'interest: 76'
  number: '7588'
  page: 27-29
  publisher: Nature Publishing Group
  source: www.nature.com
  title: 'Reproducibility: A tragedy of errors'
  title-short: Reproducibility
  type: article-journal
  URL: https://www.nature.com/articles/530027a
  volume: '530'

- id: altschulBasicLocalAlignment1990
  abstract: >-
    A new approach to rapid sequence comparison, basic local alignment search
    tool (BLAST), directly approximates alignments that optimize a measure of
    local similarity, the maximal segment pair (MSP) score. Recent mathematical
    results on the stochastic properties of MSP scores allow an analysis of the
    performance of this method as well as the statistical significance of
    alignments it generates. The basic algorithm is simple and robust; it can be
    implemented in a number of ways and applied in a variety of contexts
    including straight-forward DNA and protein sequence database searches, motif
    searches, gene identification searches, and in the analysis of multiple
    regions of similarity in long DNA sequences. In addition to its flexibility
    and tractability to mathematical analysis, BLAST is an order of magnitude
    faster than existing sequence comparison tools of comparable sensitivity.
  accessed:
    - year: 2023
      month: 12
      day: 4
  author:
    - family: Altschul
      given: Stephen F.
    - family: Gish
      given: Warren
    - family: Miller
      given: Webb
    - family: Myers
      given: Eugene W.
    - family: Lipman
      given: David J.
  citation-key: altschulBasicLocalAlignment1990
  container-title: Journal of Molecular Biology
  container-title-short: Journal of Molecular Biology
  DOI: 10.1016/S0022-2836(05)80360-2
  ISSN: 0022-2836
  issue: '3'
  issued:
    - year: 1990
      month: 10
      day: 5
  page: 403-410
  source: ScienceDirect
  title: Basic local alignment search tool
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0022283605803602
  volume: '215'

- id: amarnathHetSchedQualityofMissionAware2022
  abstract: >-
    Systems-on-Chips (SoCs) that power autonomous vehicles (AVs) must meet
    stringent performance and safety requirements prior to deployment. With
    increasing complexity in AV applications, the system needs to meet these
    real-time demands of multiple safety-critical applications simultaneously. A
    typical AV-SoC is a heterogeneous multiprocessor consisting of accelerators
    supported by general-purpose cores. Such heterogeneity, while needed for
    power-performance efficiency, complicates the art of task scheduling. In
    this paper, we demonstrate that hardware heterogeneity impacts the
    scheduler's effectiveness and that optimizing for only the real-time aspect
    of applications is not sufficient in AVs. Therefore, a more holistic
    approach is required -- one that considers global Quality-of-Mission (QoM)
    metrics, as defined in the paper. We then propose HetSched, a multi-step
    scheduler that leverages dynamic runtime information about the underlying
    heterogeneous hardware platform, along with the applications' real-time
    constraints and the task traffic in the system to optimize overall mission
    performance. HetSched proposes two scheduling policies: MSstat and MSdyn and
    scheduling optimizations like task pruning, hybrid heterogeneous ranking and
    rank update. HetSched improves overall mission performance on average by
    4.6x, 2.6x and 2.6x when compared against CPATH, ADS and 2lvl-EDF
    (state-of-the-art real-time schedulers built for heterogeneous systems),
    respectively, and achieves an average of 53.3% higher hardware utilization,
    while meeting 100% critical deadlines for real-world applications of
    autonomous vehicles. Furthermore, when used as part of an SoC design space
    exploration loop, in comparison to prior schedulers, HetSched reduces the
    number of processing elements required by an SoC to safely complete AV's
    missions by 35% on average while achieving 2.7x lower energy-mission time
    product.
  accessed:
    - year: 2022
      month: 4
      day: 17
  author:
    - family: Amarnath
      given: Aporva
    - family: Pal
      given: Subhankar
    - family: Kassa
      given: Hiwot
    - family: Vega
      given: Augusto
    - family: Buyuktosunoglu
      given: Alper
    - family: Franke
      given: Hubertus
    - family: Wellman
      given: John-David
    - family: Dreslinski
      given: Ronald
    - family: Bose
      given: Pradip
  citation-key: amarnathHetSchedQualityofMissionAware2022
  container-title: arXiv:2203.13396 [cs]
  issued:
    - year: 2022
      month: 3
      day: 24
  source: arXiv.org
  title: 'HetSched: Quality-of-Mission Aware Scheduling for Autonomous Vehicle SoCs'
  title-short: HetSched
  type: article-journal
  URL: http://arxiv.org/abs/2203.13396

- id: amrheinScientistsRiseStatistical2019
  abstract: >-
    Valentin Amrhein, Sander Greenland, Blake McShane and more than 800
    signatories call for an end to hyped claims and the dismissal of possibly
    crucial effects.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Amrhein
      given: Valentin
    - family: Greenland
      given: Sander
    - family: McShane
      given: Blake
  citation-key: amrheinScientistsRiseStatistical2019
  container-title: Nature
  DOI: 10.1038/d41586-019-00857-9
  issue: '7748'
  issued:
    - year: 2019
      month: 3
  language: en
  license: 2021 Nature
  note: |-
    Bandiera_abtest: a
    Cg_type: Comment
    Subject_term: Research data, Research management
  number: '7748'
  page: 305-307
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Scientists rise up against statistical significance
  type: article-journal
  URL: https://www.nature.com/articles/d41586-019-00857-9
  volume: '567'

- id: andaVariabilityReproducibilitySoftware2009
  abstract: >-
    The scientific study of a phenomenon requires it to be reproducible. Mature
    engineering industries are recognized by projects and products that are, to
    some extent, reproducible. Yet, reproducibility in software engineering (SE)
    has not been investigated thoroughly, despite the fact that lack of
    reproducibility has both practical and scientific consequences. We report a
    longitudinal multiple-case study of variations and reproducibility in
    software development, from bidding to deployment, on the basis of the same
    requirement specification. In a call for tender to 81 companies, 35
    responded. Four of them developed the system independently. The firm price,
    planned schedule, and planned development process, had, respectively, “low,”
    “low,” and “medium” reproducibilities. The contractor's costs, actual lead
    time, and schedule overrun of the projects had, respectively, “medium,”
    “high,” and “low” reproducibilities. The quality dimensions of the delivered
    products, reliability, usability, and maintainability had, respectively,
    “low,” "high,” and “low” reproducibilities. Moreover, variability for
    predictable reasons is also included in the notion of reproducibility. We
    found that the observed outcome of the four development projects matched our
    expectations, which were formulated partially on the basis of SE folklore.
    Nevertheless, achieving more reproducibility in SE remains a great challenge
    for SE research, education, and industry.
  author:
    - family: Anda
      given: Bente C.D.
    - family: Sjøberg
      given: Dag I.K.
    - family: Mockus
      given: Audris
  citation-key: andaVariabilityReproducibilitySoftware2009
  container-title: IEEE Transactions on Software Engineering
  DOI: 10.1109/TSE.2008.89
  ISSN: 1939-3520
  issue: '3'
  issued:
    - year: 2009
      month: 5
  note: 'interest: 93'
  page: 407-429
  source: IEEE Xplore
  title: >-
    Variability and Reproducibility in Software Engineering: A Study of Four
    Companies that Developed the Same System
  title-short: Variability and Reproducibility in Software Engineering
  type: article-journal
  volume: '35'

- id: annanEditorialPublicationGeoscientific2013
  abstract: >-
    <p><strong class="journal-contentHeaderColor">Abstract.</strong> In 2008,
    the first volume of the European Geosciences Union (EGU) journal
    Geoscientific Model Development (GMD) was published. GMD was founded because
    we perceived there to be a need for a space to publish comprehensive
    descriptions of numerical models in the geosciences. The journal is now well
    established, with the submission rate increasing over time. However, there
    are several aspects of model publication that we believe could be further
    improved. In this editorial we assess the lessons learned over the first few
    years of the journal's life, and describe some changes to GMD's editorial
    policy, which will ensure that the models and model developments are
    published in such a way that they are of maximum value to the community.
    <br><br> These changes to editorial policy mostly focus on improving the
    rigour of the review process through a stricter requirement for access to
    the materials necessary to test the behaviour of the models. <br><br>
    Throughout this editorial, "must" means that the stated actions are
    required, and the paper cannot be published without them; "strongly
    encouraged" means that we encourage the action, but papers can still be
    published if the criteria are not met; "may" means that the action may be
    carried out by the authors or referees, if they so wish. <br><br> We have
    reviewed and rationalised the manuscript types into five new categories. For
    all papers which are primarily based on a specific numerical model, the
    changes are as follows:

    - The paper must be accompanied by the code, or means of accessing the code,
    for the purpose of peer-review. If the code is normally distributed in a way
    which could compromise the anonymity of the referees, then the code must be
    made available to the editor. The referee/editor is not required to review
    the code in any way, but they may do so if they so wish.

    - All papers must include a section at the end of the paper entitled "Code
    availability". In this section, instructions for obtaining the code (e.g.
    from a supplement, or from a website) should be included; alternatively,
    contact information should be given where the code can be obtained on
    request, or the reasons why the code is not available should be clearly
    stated.

    - We strongly encourage authors to upload any user manuals associated with
    the code.

    - For models where this is practicable, we strongly encourage referees to
    compile the code, and run test cases supplied by the authors where
    appropriate.For models which have been previously described in the "grey"
    literature (e.g. as internal institutional documents), we strongly encourage
    authors to include this grey literature as a supplement, when this is
    allowed by the original authors. 

    - All papers must include a model name and version number (or other unique
    identifier) in the title. 

    -It is our perception that, since Geoscientific Model Development (GMD) was
    founded, it has become increasingly common to see model descriptions
    published in other more traditional journals, so we hope that our insights
    may be of general value to the wider geoscientific community.
  accessed:
    - year: 2023
      month: 1
      day: 30
  author:
    - family: Annan
      given: J.
    - family: Hargreaves
      given: D.
    - family: Lunt
      given: D.
    - family: Ridgwell
      given: A.
    - family: Rutt
      given: I.
    - family: Sander
      given: R.
  citation-key: annanEditorialPublicationGeoscientific2013
  container-title: Geoscientific Model Development
  DOI: 10.5194/gmd-6-1233-2013
  ISSN: 1991-959X
  issue: '4'
  issued:
    - year: 2013
      month: 8
      day: 14
  language: English
  page: 1233-1242
  publisher: Copernicus GmbH
  source: gmd.copernicus.org
  title: 'Editorial: The publication of geoscientific model developments v1.0'
  title-short: Editorial
  type: article-journal
  URL: https://gmd.copernicus.org/articles/6/1233/2013/
  volume: '6'

- id: armstrongMessWeRe2014
  abstract: >-
    Joe Armstrong is one of the inventors of Erlang. When at the Ericsson
    computer science lab in 1986, he was part of the team who designed and
    implemented the first version of Erlang. He has written several Erlang books
    including Programming Erlang Software for a Concurrent World. Joe has a PhD
    in computer science from the Royal Institute of Technology in Stockholm,
    Sweden.
  accessed:
    - year: 2024
      month: 2
      day: 22
  author:
    - family: Armstrong
      given: Joe
  citation-key: armstrongMessWeRe2014
  event-place: Strange Loop Conference
  issued:
    - year: 2014
      month: 9
      day: 19
  publisher-place: Strange Loop Conference
  title: The Mess We're In
  type: speech
  URL: https://www.youtube.com/watch?v=lKXe3HUG2l4

- id: armstrongPerformanceOpenSource
  citation-key: armstrongPerformanceOpenSource
  editor:
    - family: Armstrong
      given: Tavish
  language: English
  number-of-pages: '182'
  source: Amazon
  title: The Performance of Open Source Applications
  type: book
  URL: http://aosabook.org/en/index.html

- id: arthurKmeansAdvantagesCareful2007
  abstract: >-
    The k-means method is a widely used clustering technique that seeks to
    minimize the average squared distance between points in the same cluster.
    Although it offers no accuracy guarantees, its simplicity and speed are very
    appealing in practice. By augmenting k-means with a very simple, randomized
    seeding technique, we obtain an algorithm that is Θ(logk)-competitive with
    the optimal clustering. Preliminary experiments show that our augmentation
    improves both the speed and the accuracy of k-means, often quite
    dramatically.
  accessed:
    - year: 2024
      month: 2
      day: 9
  author:
    - family: Arthur
      given: David
    - family: Vassilvitskii
      given: Sergei
  citation-key: arthurKmeansAdvantagesCareful2007
  collection-title: SODA '07
  container-title: >-
    Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
    algorithms
  event-place: USA
  ISBN: 978-0-89871-624-5
  issued:
    - year: 2007
      month: 1
      day: 7
  page: 1027–1035
  publisher: Society for Industrial and Applied Mathematics
  publisher-place: USA
  source: ACM Digital Library
  title: 'k-means++: the advantages of careful seeding'
  title-short: k-means++
  type: paper-conference

- id: arvanitouSoftwareEngineeringPractices2021
  abstract: >-
    Background: The development of scientific software applications is far from
    trivial, due to the constant increase in the necessary complexity of these
    applications, their increasing size, and their need for intensive
    maintenance and reuse.


    Aim: To this end, developers of scientific software (who usually lack a
    formal computer science background) need to use appropriate software
    engineering (SE) practices. This paper describes the results of a systematic
    mapping study on the use of SE for scientific application development and
    their impact on software quality.


    Method: To achieve this goal we have performed a systematic mapping study on
    359 papers. We first describe a catalog of SE practices used in scientific
    software development. Then, we discuss the quality attributes of interest
    that drive the application of these practices, as well as tentative
    side-effects of applying the practices on qualities.


    Results: The main findings indicate that scientific software developers are
    focusing on practices that improve implementation productivity, such as code
    reuse, use of third-party libraries, and the application of “good”
    programming techniques. In addition, apart from the finding that performance
    is a key-driver for many of these applications, scientific software
    developers also find maintainability and productivity to be important.


    Conclusions: The results of the study are compared to existing literature,
    are interpreted under a software engineering prism, and various implications
    for researchers and practitioners are provided. One of the key findings of
    the study, which is considered as important for driving future research
    endeavors is the lack of evidence on the trade-offs that need to be made
    when applying a software practice, i.e., negative (indirect) effects on
    other quality attributes.
  accessed:
    - year: 2022
      month: 6
      day: 3
  author:
    - family: Arvanitou
      given: Elvira-Maria
    - family: Ampatzoglou
      given: Apostolos
    - family: Chatzigeorgiou
      given: Alexander
    - family: Carver
      given: Jeffrey C.
  citation-key: arvanitouSoftwareEngineeringPractices2021
  container-title: Journal of Systems and Software
  container-title-short: Journal of Systems and Software
  DOI: 10.1016/j.jss.2020.110848
  ISSN: '01641212'
  issued:
    - year: 2021
      month: 2
  language: en
  note: 'interest: 95'
  page: '110848'
  source: DOI.org (Crossref)
  title: >-
    Software engineering practices for scientific software development: A
    systematic mapping study
  title-short: Software engineering practices for scientific software development
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0164121220302387
  volume: '172'

- id: atkinsonScientificWorkflowsPresent2017
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Atkinson
      given: Malcolm
    - family: Gesing
      given: Sandra
    - family: Montagnat
      given: Johan
    - family: Taylor
      given: Ian
  citation-key: atkinsonScientificWorkflowsPresent2017
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.05.041
  ISSN: 0167739X
  issued:
    - year: 2017
      month: 10
  language: en
  note: 'interest: 95'
  page: 216-227
  source: DOI.org (Crossref)
  title: 'Scientific workflows: Past, present and future'
  title-short: Scientific workflows
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0167739X17311202
  volume: '75'

- id: aug24FallDatacenterSoftware2022
  abstract: >-
    The research community has long predicted the death of Moore’s law and
    attendant growth in datacenter hardware speeds. In a few years, datacenter
    networks will grow an order of magnitude from 40Gb …
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Aug 24
      given: Irene Zhang
      dropping-particle: 'on'
    - literal: '2022'
  citation-key: aug24FallDatacenterSoftware2022
  container-title: SIGARCH
  issued:
    - year: 2022
      month: 8
      day: 24
  language: en-US
  title: The Fall of Datacenter Software
  type: post-weblog
  URL: https://www.sigarch.org/the-fall-of-datacenter-software/

- id: avilaSUSSINGMERGERTREES2014
  abstract: >-
    Merger tree codes are routinely used to follow the growth and merger of dark
    matter haloes in simulations of cosmic structure formation. Whereas in
    Srisawat et. al. we compared the trees built using a wide variety of such
    codes, here we study the influence of the underlying halo catalogue upon the
    resulting trees. We observe that the specifics of halo finding itself
    greatly influences the constructed merger trees. We find that the choices
    made to define the halo mass are of prime importance. For instance, amongst
    many potential options different finders select self-bound objects or
    spherical regions of defined overdensity, decide whether or not to include
    substructures within the mass returned and vary in their initial particle
    selection. The impact of these decisions is seen in tree length (the period
    of time a particularly halo can be traced back through the simulation),
    branching ratio (essentially the merger rate of subhaloes) and mass
    evolution. We therefore conclude that the choice of the underlying halo
    finder is more relevant to the process of building merger trees than the
    tree builder itself. We also report on some built-in features of specific
    merger tree codes that (sometimes) help to improve the quality of the merger
    trees produced.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Avila
      given: Santiago
    - family: Knebe
      given: Alexander
    - family: Pearce
      given: Frazer R.
    - family: Schneider
      given: Aurel
    - family: Srisawat
      given: Chaichalit
    - family: Thomas
      given: Peter A.
    - family: Behroozi
      given: Peter
    - family: Elahi
      given: Pascal J.
    - family: Han
      given: Jiaxin
    - family: Mao
      given: Yao-Yuan
    - family: Onions
      given: Julian
    - family: Rodriguez-Gomez
      given: Vicente
    - family: Tweed
      given: Dylan
  citation-key: avilaSUSSINGMERGERTREES2014
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/stu799
  ISSN: 1365-2966, 0035-8711
  issue: '4'
  issued:
    - year: 2014
      month: 7
      day: 11
  language: en
  page: 3488-3501
  source: DOI.org (Crossref)
  title: 'SUSSING MERGER TREES: the influence of the halo finder'
  title-short: SUSSING MERGER TREES
  type: article-journal
  URL: >-
    http://academic.oup.com/mnras/article/441/4/3488/1223418/SUSSING-MERGER-TREES-the-influence-of-the-halo
  volume: '441'

- id: AxiomComputerAlgebra
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: AxiomComputerAlgebra
  title: Axiom Computer Algebra System
  type: webpage
  URL: http://www.axiom-developer.org/

- id: aycockGoodWormsHuman2008
  abstract: >-
    The extent of Internet censorship in countries like China is regularly
    tested, but the testing methods used from within a censored country can
    entail risk for humans. A benevolent worm can be used for testing instead:
    the worm’s selfreplication, long the bane of suggested benevolent viruses
    and worms, is shown to be essential here. We describe the design of this
    benevolent worm, along with some other related applications for it. A full
    technical, ethical, and legal analysis is provided.
  accessed:
    - year: 2023
      month: 10
      day: 12
  author:
    - family: Aycock
      given: John
    - family: Maurushat
      given: Alana
  citation-key: aycockGoodWormsHuman2008
  container-title: ACM SIGCAS Computers and Society
  container-title-short: SIGCAS Comput. Soc.
  DOI: 10.1145/1361255.1361256
  ISSN: 0095-2737
  issue: '1'
  issued:
    - year: 2008
      month: 3
  language: en
  page: 28-39
  source: DOI.org (Crossref)
  title: '"Good" worms and human rights'
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1361255.1361256
  volume: '38'

- id: babujiParslPervasiveParallel2019
  abstract: >-
    High-level programming languages such as Python are increasingly used to
    provide intuitive interfaces to libraries written in lower-level languages
    and for assembling applications from various components. This migration
    towards orchestration rather than implementation, coupled with the growing
    need for parallel computing (e.g., due to big data and the end of Moore's
    law), necessitates rethinking how parallelism is expressed in programs.
    Here, we present Parsl, a parallel scripting library that augments Python
    with simple, scalable, and flexible constructs for encoding parallelism.
    These constructs allow Parsl to construct a dynamic dependency graph of
    components that it can then execute efficiently on one or many processors.
    Parsl is designed for scalability, with an extensible set of executors
    tailored to different use cases, such as low-latency, high-throughput, or
    extreme-scale execution. We show, via experiments on the Blue Waters
    supercomputer, that Parsl executors can allow Python scripts to execute
    components with as little as 5 ms of overhead, scale to more than 250000
    workers across more than 8000 nodes, and process upward of 1200 tasks per
    second. Other Parsl features simplify the construction and execution of
    composite programs by supporting elastic provisioning and scaling of
    infrastructure, fault-tolerant execution, and integrated wide-area data
    management. We show that these capabilities satisfy the needs of many-task,
    interactive, online, and machine learning applications in fields such as
    biology, cosmology, and materials science.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Babuji
      given: Yadu
    - family: Woodard
      given: Anna
    - family: Li
      given: Zhuozhao
    - family: Katz
      given: Daniel S.
    - family: Clifford
      given: Ben
    - family: Kumar
      given: Rohan
    - family: Lacinski
      given: Lukasz
    - family: Chard
      given: Ryan
    - family: Wozniak
      given: Justin M.
    - family: Foster
      given: Ian
    - family: Wilde
      given: Michael
    - family: Chard
      given: Kyle
  citation-key: babujiParslPervasiveParallel2019
  collection-title: HPDC '19
  container-title: >-
    Proceedings of the 28th International Symposium on High-Performance Parallel
    and Distributed Computing
  DOI: 10.1145/3307681.3325400
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6670-0
  issued:
    - year: 2019
      month: 6
      day: 17
  page: 25–36
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Parsl: Pervasive Parallel Programming in Python'
  title-short: Parsl
  type: paper-conference
  URL: https://doi.org/10.1145/3307681.3325400

- id: babujiScalableParallelProgramming2019
  abstract: >-
    Python is increasingly the lingua franca of scientific computing. It is used
    as a higher level language to wrap lower-level libraries and to compose
    scripts from various independent components. However, scaling and moving
    Python programs from laptops to supercomputers remains a challenge. Here we
    present Parsl, a parallel scripting library for Python. Parsl makes it
    straightforward for developers to implement parallelism in Python by
    annotating functions that can be executed asynchronously and in parallel,
    and to scale analyses from a laptop to thousands of nodes on a supercomputer
    or distributed system. We examine how Parsl is implemented, focusing on
    syntax and usage. We describe two scientific use cases in which Parsl's
    intuitive and scalable parallelism is used.
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Babuji
      given: Yadu
    - family: Woodard
      given: Anna
    - family: Li
      given: Zhuozhao
    - family: Katz
      given: Daniel S.
    - family: Clifford
      given: Ben
    - family: Foster
      given: Ian
    - family: Wilde
      given: Michael
    - family: Chard
      given: Kyle
  citation-key: babujiScalableParallelProgramming2019
  container-title: >-
    Proceedings of the Practice and Experience in Advanced Research Computing on
    Rise of the Machines (learning)
  DOI: 10.1145/3332186.3332231
  event-place: Chicago IL USA
  event-title: 'PEARC ''19: Practice and Experience in Advanced Research Computing'
  ISBN: 978-1-4503-7227-5
  issued:
    - year: 2019
      month: 7
      day: 28
  language: en
  page: 1-8
  publisher: ACM
  publisher-place: Chicago IL USA
  source: DOI.org (Crossref)
  title: Scalable Parallel Programming in Python with Parsl
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3332186.3332231

- id: badiaWorkflowsScienceChallenge2017
  abstract: >-
    Workflows have been used traditionally as a mean to describe and implement
    the computing usually parametric studies and explorations searching for the
    best solution  that  scientific researchers want to perform. 


    A workflow is not only the computing application, but a way of documenting a
    process.  Science workflows may be of very different nature depending on the
    area of research, matching the actual experiment that the scientist want to
    perform. 


    Workflow Management Systems are environments that offer the researchers
    tools to define, publish, execute and document their workflows. 


    In some cases, the science workflows are used to generate data; in other
    cases are used to analyse existing data; only in a few cases, workflows are
    used both to generate and analyse  data. The design of experiments is in
    some cases generated blindly, without a clear idea of which points are
    relevant to be computed/simulated, ending up with huge amount of computation
    that is performed following a brute-force strategy. 


    However, the evolution of systems and the large amount of data generated by
    the applications require an in-situ analysis of the data, thus requiring new
    solutions to develop workflows that includes both the
    simulation/computational part and the analytic part. What is more, the fact
    that both components, computation and analytics, can be run together  will
    enable the possibility of defining more dynamic workflows, with new
    computations being decided by the analytics in a more efficient way.


    The first part of the paper will review current approaches that a set of
    scientific communities follows in the development of their workflows. Due to
    the election of several scientific communities and use cases using a
    specific Workflow Management System, this survey maybe incomplete with
    regard a complete revision of the literature about workflows, but we expect
    that the reader appreaciates the effort performed in trying to see the
    scientific communities needs and requirements. 


    The second part of the paper will propose a new software architecture to
    develop a new  family of end-to-end workflows that enables the management
    of  dynamic workflows composed of simulations, analytics and visualization,
    including inputs/outputs from streams.
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Badia
      given: Rosa M
    - family: Ayguade
      given: Eduard
    - family: Labarta
      given: Jesus
  citation-key: badiaWorkflowsScienceChallenge2017
  container-title: Supercomputing Frontiers and Innovations
  container-title-short: JSFI
  DOI: 10.14529/jsfi170102
  ISSN: '23138734'
  issue: '1'
  issued:
    - year: 2017
      month: 3
  note: 'interest: 95'
  source: DOI.org (Crossref)
  title: >-
    Workflows for science: a challenge when facing the convergence of HPC and
    Big Data
  title-short: Workflows for science
  type: article-journal
  URL: https://superfri.org/index.php/superfri/article/view/125
  volume: '4'

- id: baezWhatWeCan
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Baez
      given: John
  citation-key: baezWhatWeCan
  title: What We Can Do About Science Journals
  type: webpage
  URL: https://math.ucr.edu/home/baez/journals.html

- id: baggerlyDerivingChemosensitivityCell2009
  abstract: >-
    High-throughput biological assays such as microarrays let us ask very
    detailed questions about how diseases operate, and promise to let us
    personalize therapy. Data processing, however, is often not described well
    enough to allow for exact reproduction of the results, leading to exercises
    in "forensic bioinformatics" where aspects of raw data and reported results
    are used to infer what methods must have been employed. Unfortunately, poor
    documentation can shift from an inconvenience to an active danger when it
    obscures not just methods but errors. In this report we examine several
    related papers purporting to use microarray-based signatures of drug
    sensitivity derived from cell lines to predict patient response. Patients in
    clinical trials are currently being allocated to treatment arms on the basis
    of these results. However, we show in five case studies that the results
    incorporate several simple errors that may be putting patients at risk. One
    theme that emerges is that the most common errors are simple (e.g., row or
    column offsets); conversely, it is our experience that the most simple
    errors are common. We then discuss steps we are taking to avoid such errors
    in our own investigations.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Baggerly
      given: Keith A.
    - family: Coombes
      given: Kevin R.
  citation-key: baggerlyDerivingChemosensitivityCell2009
  container-title: The Annals of Applied Statistics
  ISSN: 1932-6157
  issue: '4'
  issued:
    - year: 2009
  note: 'interest: 74'
  page: 1309-1334
  publisher: Institute of Mathematical Statistics
  source: JSTOR
  title: >-
    Deriving chemosensitivity from cell lines: forensic bioinformatics and
    reproducible research in high-throughput biology
  type: article-journal
  URL: https://www.jstor.org/stable/27801549
  volume: '3'

- id: bagozziLegacyTechnologyAcceptance2007
  abstract: >-
    This article presents a critique of a number of shortcomings with the
    technology acceptance model (TAM) and points to specific remedies in each
    case. In addition, I present a model for the purposes of providing a
    foundation for a paradigm shift. The model consists first of a decision
    making core (goal desire → goal intention → action desire → action
    intention) that is grounded in basic decision making variables/processes of
    a universal nature. The decision core also contains a mechanism for
    self-regulation that moderates the effects of desires on intentions. Second,
    added to the decision making core are a number of causes and effects of
    decisions and self-regulatory reasoning, with the aim of introducing
    potential contingent, contextual nuances for understanding decision making.
    Many of the causal variables here are contained within TAM or its
    extensions; also considered are new variables grounded in emotional,
    group/social/cultural, and goal-directed behavior research.
  accessed:
    - year: 2022
      month: 5
      day: 27
  author:
    - family: Bagozzi
      given: Richard
  citation-key: bagozziLegacyTechnologyAcceptance2007
  container-title: Journal of the Association for Information Systems
  container-title-short: JAIS
  DOI: 10.17705/1jais.00122
  ISSN: '15369323'
  issue: '4'
  issued:
    - year: 2007
      month: 4
  page: 244-254
  source: DOI.org (Crossref)
  title: >-
    The Legacy of the Technology Acceptance Model and a Proposal for a Paradigm
    Shift
  type: article-journal
  URL: https://aisel.aisnet.org/jais/vol8/iss4/12/
  volume: '8'

- id: baileyDangerDeathAre
  abstract: >-
    Assessing risk is something everyone must do every day. Yet few are very
    good at it, and there are significant consequences of the public’s
    collective inability to accurately assess risk. As a first and…
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Bailey
      given: David H.
    - family: Borwein
      given: Jonathan
  citation-key: baileyDangerDeathAre
  container-title: The Conversation
  language: en
  title: 'Danger of death: are we programmed to miscalculate risk?'
  title-short: Danger of death
  type: article-newspaper
  URL: >-
    http://theconversation.com/danger-of-death-are-we-programmed-to-miscalculate-risk-4598

- id: baileyHowStopMedia
  abstract: >-
    Few of us have the time or expertise to sift through all of the scientific
    papers published every day to determine which research is important and
    relevant to our lives. In this sense, science journalists…
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Bailey
      given: David H.
    - family: Borwein
      given: Jonathan
  citation-key: baileyHowStopMedia
  container-title: The Conversation
  language: en
  title: How to stop the media reporting science fiction as fact
  type: article-newspaper
  URL: >-
    http://theconversation.com/how-to-stop-the-media-reporting-science-fiction-as-fact-10252

- id: bakerHowQualityControl2016
  abstract: >-
    It may not be sexy, but quality assurance is becoming a crucial part of lab
    life.
  accessed:
    - year: 2022
      month: 9
      day: 27
  author:
    - family: Baker
      given: Monya
  citation-key: bakerHowQualityControl2016
  container-title: Nature
  DOI: 10.1038/529456a
  ISSN: 1476-4687
  issue: '7587'
  issued:
    - year: 2016
      month: 1
      day: 1
  language: en
  license: 2016 Nature Publishing Group
  note: 'interest: 71'
  number: '7587'
  page: 456-458
  publisher: Nature Publishing Group
  source: www.nature.com
  title: How quality control could save your science
  type: article-journal
  URL: https://www.nature.com/articles/529456a
  volume: '529'

- id: bakhvalovHowGetConsistent
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Bakhvalov
      given: Denis
  citation-key: bakhvalovHowGetConsistent
  container-title: EasyPerf
  note: 'score: 70'
  title: How to get consistent results when benchmarking on Linux?
  type: post-weblog
  URL: https://easyperf.net/blog/2019/08/02/Perf-measurement-environment-on-Linux

- id: balakrishnanOPUSLightweightSystem2013
  accessed:
    - year: 2023
      month: 7
      day: 6
  author:
    - family: Balakrishnan
      given: Nikilesh
    - family: Bytheway
      given: Thomas
    - family: Sohan
      given: Ripduman
    - family: Hopper
      given: Andy
  citation-key: balakrishnanOPUSLightweightSystem2013
  event-title: 5th USENIX Workshop on the Theory and Practice of Provenance (TaPP 13)
  issued:
    - year: 2013
  language: en
  source: www.usenix.org
  title: '{OPUS}: A Lightweight System for Observational Provenance in User Space'
  title-short: OPUS
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/tapp13/technical-sessions/presentation/balakrishnan

- id: balasubramanianRADICALCybertoolsMiddlewareBuilding2019
  abstract: >-
    RADICAL-Cybertools (RCT) are a set of software systems that serve as
    middleware to develop efficient and effective tools for scientific
    computing. Specifically, RCT enable executing many-task applications at
    extreme scale and on a variety of computing infrastructures. RCT are
    building blocks, designed to work as stand-alone systems, integrated among
    themselves or integrated with third-party systems. RCT enables innovative
    science in multiple domains, including but not limited to biophysics,
    climate science and particle physics, consuming hundreds of millions of core
    hours. This paper provides an overview of RCT systems, their impact, and the
    architectural principles and software engineering underlying RCT
  accessed:
    - year: 2023
      month: 5
      day: 6
  author:
    - family: Balasubramanian
      given: Vivek
    - family: Jha
      given: Shantenu
    - family: Merzky
      given: Andre
    - family: Turilli
      given: Matteo
  citation-key: balasubramanianRADICALCybertoolsMiddlewareBuilding2019
  DOI: 10.48550/arXiv.1904.03085
  issued:
    - year: 2019
      month: 4
      day: 5
  note: 'interest: 85'
  number: arXiv:1904.03085
  publisher: arXiv
  source: arXiv.org
  title: 'RADICAL-Cybertools: Middleware Building Blocks for Scalable Science'
  title-short: RADICAL-Cybertools
  type: article
  URL: http://arxiv.org/abs/1904.03085

- id: barbaHardRoadReproducibility2016
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Barba
      given: Lorena A.
  citation-key: barbaHardRoadReproducibility2016
  container-title: Science
  DOI: 10.1126/science.354.6308.142
  issue: '6308'
  issued:
    - year: 2016
      month: 10
      day: 7
  page: 142-142
  publisher: American Association for the Advancement of Science
  source: science.org (Atypon)
  title: The hard road to reproducibility
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.354.6308.142
  volume: '354'

- id: barbaReproducibilityPIManifesto2012
  abstract: >-
    Slides for lightning talk at the ICERM workshop on "Reproducibility in
    Computational and Experimental Mathematics", December 2012. Shared under
    CC-BY.
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Barba
      given: Lorena A.
  citation-key: barbaReproducibilityPIManifesto2012
  DOI: 10.6084/m9.figshare.104539.v1
  genre: presentation
  issued:
    - year: 2012
      month: 12
      day: 13
  language: en
  publisher: figshare
  title: Reproducibility PI Manifesto
  type: speech
  URL: >-
    https://figshare.com/articles/presentation/Reproducibility_PI_Manifesto/104539/1

- id: bardramWritingComputerScience2007
  author:
    - family: Bardram
      given: Jakob E
  citation-key: bardramWritingComputerScience2007
  issued:
    - year: 2007
  language: en
  title: Writing a (Computer Science) Paper
  type: speech

- id: bargaAutomaticCaptureEfficient2008
  abstract: >-
    For the first provenance challenge, we introduce a layered model to
    represent workflow provenance that allows navigation from an abstract model
    of the experiment to instance data collected during a specific experiment
    run. We outline modest extensions to a commercial workflow engine so it will
    automatically capture provenance at workflow runtime. We also present an
    approach to store this provenance data in a relational database. Finally, we
    demonstrate how core provenance queries in the challenge can be expressed in
    SQL and discuss the merits of our layered representation. Copyright © 2007
    John Wiley & Sons, Ltd.
  accessed:
    - year: 2023
      month: 7
      day: 18
  author:
    - family: Barga
      given: Roger S.
    - family: Digiampietri
      given: Luciano A.
  citation-key: bargaAutomaticCaptureEfficient2008
  container-title: 'Concurrency and Computation: Practice and Experience'
  DOI: 10.1002/cpe.1235
  ISSN: 1532-0634
  issue: '5'
  issued:
    - year: 2008
  language: en
  license: Copyright © 2007 John Wiley & Sons, Ltd.
  page: 419-429
  source: Wiley Online Library
  title: Automatic capture and efficient storage of e-Science experiment provenance
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1235
  volume: '20'

- id: barnesPublishYourComputer2010
  abstract: >-
    Freely provided working code — whatever its quality — improves programming
    and enables others to engage with your research, says Nick Barnes.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Barnes
      given: Nick
  citation-key: barnesPublishYourComputer2010
  container-title: Nature
  DOI: 10.1038/467753a
  ISSN: 1476-4687
  issue: '7317'
  issued:
    - year: 2010
      month: 10
  language: en
  license: 2010 Nature Publishing Group
  number: '7317'
  page: 753-753
  publisher: Nature Publishing Group
  source: www.nature.com
  title: 'Publish your computer code: it is good enough'
  title-short: Publish your computer code
  type: article-journal
  URL: https://www.nature.com/articles/467753a
  volume: '467'

- id: bartlettXSDKFoundationsExtremescale2017
  abstract: "Extreme-scale computational science increasingly demands multiscale and multiphysics formulations. Combining software developed by independent groups is imperative: no single team has resources for all predictive science and decision support capabilities. Scientific libraries provide high-quality, reusable software components for constructing applications with improved robustness and portability.\_ However, without coordination, many libraries cannot be easily composed.\_ Namespace collisions, inconsistent arguments, lack of third-party software versioning, and additional difficulties make composition costly.The Extreme-scale Scientific Software Development Kit (xSDK) defines community policies to improve code quality and compatibility across independently developed packages (hypre, PETSc, SuperLU, Trilinos, and Alquimia) and provides a foundation for addressing broader issues in software interoperability, performance portability, and sustainability.\_ The xSDK provides turnkey installation of member software and seamless combination of aggregate capabilities, and it marks first steps toward extreme-scale scientific software ecosystems from which future applications can be composed rapidly with assured quality and scalability."
  accessed:
    - year: 2023
      month: 8
      day: 31
  author:
    - family: Bartlett
      given: Roscoe
    - family: Demeshko
      given: Irina
    - family: Gamblin
      given: Todd
    - family: Hammond
      given: Glenn
    - family: Heroux
      given: Michael Allen
    - family: Johnson
      given: Jeffrey
    - family: Klinvex
      given: Alicia
    - family: Li
      given: Xiaoye
    - family: McInnes
      given: Lois Curfman
    - family: Moulton
      given: J. David
    - family: Osei-Kuffuor
      given: Daniel
    - family: Sarich
      given: Jason
    - family: Smith
      given: Barry
    - family: Willenbring
      given: James
    - family: Yang
      given: Ulrike Meier
  citation-key: bartlettXSDKFoundationsExtremescale2017
  container-title: Supercomputing Frontiers and Innovations
  DOI: 10.14529/jsfi170104
  ISSN: 2313-8734
  issue: '1'
  issued:
    - year: 2017
      month: 2
      day: 25
  language: en
  license: Copyright (c)
  number: '1'
  page: 69-82
  source: superfri.org
  title: >-
    xSDK Foundations: Toward an Extreme-scale Scientific Software Development
    Kit
  title-short: xSDK Foundations
  type: article-journal
  URL: https://superfri.org/index.php/superfri/article/view/127
  volume: '4'

- id: batesTrustworthyWholeSystemProvenance2015
  accessed:
    - year: 2023
      month: 8
      day: 25
  author:
    - family: Bates
      given: Adam
    - family: Tian
      given: Dave (Jing)
    - family: Butler
      given: Kevin R. B.
    - family: Moyer
      given: Thomas
  citation-key: batesTrustworthyWholeSystemProvenance2015
  event-title: 24th USENIX Security Symposium (USENIX Security 15)
  ISBN: 978-1-939133-11-3
  issued:
    - year: 2015
  language: en
  page: 319-334
  source: www.usenix.org
  title: Trustworthy {Whole-System} Provenance for the Linux Kernel
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/bates

- id: beaulieu-jonesReproducibilityComputationalWorkflows2017
  abstract: >-
    The application of continuous integration, an approach common in software
    development, enables the automatic reproduction of computational analyses.
  accessed:
    - year: 2023
      month: 2
      day: 20
  author:
    - family: Beaulieu-Jones
      given: Brett K.
    - family: Greene
      given: Casey S.
  citation-key: beaulieu-jonesReproducibilityComputationalWorkflows2017
  container-title: Nature Biotechnology
  container-title-short: Nat Biotechnol
  DOI: 10.1038/nbt.3780
  ISSN: 1546-1696
  issue: '4'
  issued:
    - year: 2017
      month: 4
  language: en
  license: >-
    2017 Nature Publishing Group, a division of Macmillan Publishers Limited.
    All Rights Reserved.
  number: '4'
  page: 342-346
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Reproducibility of computational workflows is automated using continuous
    analysis
  type: article-journal
  URL: https://www.nature.com/articles/nbt.3780
  volume: '35'

- id: bechhoferWhyLinkedData2010
  abstract: >-
    Scientific data stands to represent a significant portion of the linked open
    data cloud and science itself stands to benefit from the data fusion
    capability that this will afford. However, simply publishing linked data
    into the cloud does not necessarily meet the requirements of reuse.
    Publishing has requirements of provenance, quality, credit, attribution,
    methods in order to provide the \emphreproducibility that allows validation
    of results. In this paper we make the case for a scientific data publication
    model on top of linked data and introduce the notion of \emphResearch
    Objects as first class citizens for sharing and publishing.
  author:
    - family: Bechhofer
      given: Sean
    - family: Ainsworth
      given: John
    - family: Bhagat
      given: Jiten
    - family: Buchan
      given: Iain
    - family: Couch
      given: Philip
    - family: Cruickshank
      given: Don
    - family: Roure
      given: David De
    - family: Delderfield
      given: Mark
    - family: Dunlop
      given: Ian
    - family: Gamble
      given: Matthew
    - family: Goble
      given: Carole
    - family: Michaelides
      given: Danius
    - family: Missier
      given: Paolo
    - family: Owen
      given: Stuart
    - family: Newman
      given: David
    - family: Sufi
      given: Shoaib
  citation-key: bechhoferWhyLinkedData2010
  container-title: 2010 IEEE Sixth International Conference on e-Science
  DOI: 10.1109/eScience.2010.21
  event-title: 2010 IEEE Sixth International Conference on e-Science
  issued:
    - year: 2010
      month: 12
  page: 300-307
  source: IEEE Xplore
  title: Why Linked Data is Not Enough for Scientists
  type: paper-conference

- id: begleyDrugDevelopmentRaise2012
  author:
    - family: Begley
      given: C. Glenn
    - family: Ellis
      given: Lee M.
  citation-key: begleyDrugDevelopmentRaise2012
  container-title: Nature
  container-title-short: Nature
  DOI: 10.1038/483531a
  ISSN: 1476-4687
  issue: '7391'
  issued:
    - year: 2012
      month: 3
      day: 28
  language: eng
  note: 'interest: 85'
  page: 531-533
  PMID: '22460880'
  source: PubMed
  title: 'Drug development: Raise standards for preclinical cancer research'
  title-short: Drug development
  type: article-journal
  volume: '483'

- id: behrooziMajorMergersGoing2015
  abstract: >-
    Merging haloes with similar masses (i.e. major mergers) pose significant
    challenges for halo finders. We compare five halo-finding algorithms’ (ahf,
    hbt, rockstar, subfind, and velociraptor) recovery of halo properties for
    both isolated and cosmological major mergers. We find that halo positions
    and velocities are often robust, but mass biases exist for every technique.
    The algorithms also show strong disagreement in the prevalence and duration
    of major mergers, especially at high redshifts (z > 1). This raises
    significant uncertainties for theoretical models that require major mergers
    for, e.g. galaxy morphology changes, size changes, or black hole growth, as
    well as for finding Bullet Cluster analogues. All finders not using temporal
    information also show host halo and subhalo relationship swaps over
    successive timesteps, requiring careful merger tree construction to avoid
    problematic mass accretion histories. We suggest that future algorithms
    should combine phase-space and temporal information to avoid the issues
    presented.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Behroozi
      given: Peter
    - family: Knebe
      given: Alexander
    - family: Pearce
      given: Frazer R.
    - family: Elahi
      given: Pascal
    - family: Han
      given: Jiaxin
    - family: Lux
      given: Hanni
    - family: Mao
      given: Yao-Yuan
    - family: Muldrew
      given: Stuart I.
    - family: Potter
      given: Doug
    - family: Srisawat
      given: Chaichalit
  citation-key: behrooziMajorMergersGoing2015
  container-title: Monthly Notices of the Royal Astronomical Society
  container-title-short: Mon. Not. R. Astron. Soc.
  DOI: 10.1093/mnras/stv2046
  ISSN: 0035-8711, 1365-2966
  issue: '3'
  issued:
    - year: 2015
      month: 12
      day: 11
  language: en
  page: 3020-3029
  source: DOI.org (Crossref)
  title: 'Major mergers going Notts: challenges for modern halo finders'
  title-short: Major mergers going Notts
  type: article-journal
  URL: https://academic.oup.com/mnras/article-lookup/doi/10.1093/mnras/stv2046
  volume: '454'

- id: beingessnerHowSwiftAchieved2019
  accessed:
    - year: 2023
      month: 3
      day: 10
  author:
    - family: Beingessner
      given: Aria
  citation-key: beingessnerHowSwiftAchieved2019
  container-title: Faultlore
  issued:
    - year: 2019
      month: 11
      day: 7
  language: en
  note: 'interest: 75'
  title: How Swift Achieved Dynamic Linking Where Rust Couldn't
  type: post-weblog
  URL: https://gankra.github.io/blah/swift-abi

- id: beingessnerPrePoopingYourPants2015
  abstract: Why Rust is not guarantee when destructors get called
  accessed:
    - year: 2023
      month: 12
      day: 17
  author:
    - family: Beingessner
      given: Alexis
  citation-key: beingessnerPrePoopingYourPants2015
  container-title: The Miscellaneous Screamings of Alexis Beingessner
  issued:
    - year: 2015
      month: 4
      day: 27
  title: Pre-Pooping Your Pants With Rust
  type: post-weblog
  URL: https://cglab.ca/~abeinges/blah/everyone-poops/

- id: belhajjameWf4EverResearchObject2013
  abstract: >-
    The Wf4Ever Research Object Model provides a vocabulary for the description
    of workflow-centric Research Objects: aggregations of resources relating to
    scientific workflows.
  author:
    - family: Belhajjame
      given: Khalid
    - family: Klyne
      given: Graham
    - family: Garijo
      given: Daniel
    - family: Corch
      given: Oscar
    - family: García Cuesta
      given: Esteban
    - family: Palma
      given: Raul
  citation-key: belhajjameWf4EverResearchObject2013
  issued:
    - year: 2013
      month: 11
      day: 30
  title: Wf4Ever Research Object Model 1.0
  type: webpage
  URL: https://wf4ever.github.io/ro/

- id: belhajjameWorkflowPROVcorpusBased2013
  abstract: >-
    We describe a corpus of provenance traces that we have collected by
    executing 120 real world scientific workflows. The workflows are from two
    different workflow systems: Taverna [5] and Wings [3], and 12 different
    application domains (see Figure 1). Table 1 provides a summary of this
    PROV-corpus.
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Belhajjame
      given: Khalid
    - family: Zhao
      given: Jun
    - family: Garijo
      given: Daniel
    - family: Garrido
      given: Aleix
    - family: Soiland-Reyes
      given: Stian
    - family: Alper
      given: Pinar
    - family: Corcho
      given: Oscar
  citation-key: belhajjameWorkflowPROVcorpusBased2013
  container-title: Proceedings of the Joint EDBT/ICDT 2013 Workshops on - EDBT '13
  DOI: 10.1145/2457317.2457376
  event-place: Genoa, Italy
  event-title: the Joint EDBT/ICDT 2013 Workshops
  ISBN: 978-1-4503-1599-9
  issued:
    - year: 2013
  language: en
  page: '331'
  publisher: ACM Press
  publisher-place: Genoa, Italy
  source: DOI.org (Crossref)
  title: A workflow PROV-corpus based on taverna and wings
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?doid=2457317.2457376

- id: berganDeterministicProcessGroups
  abstract: >-
    Current multiprocessor systems execute parallel and concurrent software
    nondeterministically: even when given precisely the same input, two
    executions of the same program may produce different output. This severely
    complicates debugging, testing, and automatic replication for
    fault-tolerance. Previous efforts to address this issue have focused
    primarily on record and replay, but making execution actually deterministic
    would address the problem at the root.
  author:
    - family: Bergan
      given: Tom
    - family: Hunt
      given: Nicholas
    - family: Ceze
      given: Luis
    - family: Gribble
      given: Steven D
  citation-key: berganDeterministicProcessGroups
  language: en
  source: Zotero
  title: Deterministic Process Groups in dOS
  type: article-journal

- id: bergerPerformanceMatters2017
  abstract: >-
    Performance clearly matters to users. The most common software update on the
    AppStore *by far* is "Bug fixes and performance enhancements." Now that
    Moore's Law Free Lunch has ended, programmers have to work hard to get high
    performance for their applications. But why is performance so hard to
    deliver? I will first explain why our current approaches to evaluating and
    optimizing performance don't work, especially on modern hardware and for
    modern applications. I will then present two systems that address these
    challenges. Stabilizer is a tool that enables statistically sound
    performance evaluation, making it possible to understand the impact of
    optimizations and conclude things like the fact that the -O2 and -O3
    optimization levels are indistinguishable from noise (unfortunately true).
    Since compiler optimizations have largely run out of steam, we need better
    profiling support, especially for modern concurrent, multi-threaded
    applications. Coz is a novel "causal profiler" that lets programmers
    optimize for throughput or latency, and which pinpoints and accurately
    predicts the impact of optimizations. Coz's approach unlocks numerous
    previously unknown optimization opportunities. Guided by Coz, we improved
    the performance of Memcached by 9%, SQLite by 25%, and accelerated six
    Parsec applications by as much as 68%; in most cases, these optimizations
    involved modifying under 10 lines of code. This talk is based on work with
    Charlie Curtsinger published at ASPLOS 2013 (Stabilizer) and SOSP 2015
    (Coz), which received a Best Paper Award and was selected as a CACM Research
    Highlight.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Berger
      given: Emery
  citation-key: bergerPerformanceMatters2017
  container-title-short: July 11th, 2017
  event-title: Book of abstracts
  issued:
    - year: 2017
      month: 9
      day: 10
  language: eng
  license: Attribution-NonCommercial-NoDerivs 3.0 Spain
  note: 'Accepted: 2017-10-19T08:36:31Z'
  page: 53-55
  publisher: Barcelona Supercomputing Center
  source: upcommons.upc.edu
  title: Performance matters
  type: paper-conference
  URL: https://upcommons.upc.edu/handle/2117/108842

- id: bergerPerformanceMatters2019
  abstract: >-
    Performance clearly matters to users. For example, the most common software
    update on the AppStore is "Bug fixes and performance enhancements." Now that
    Moore's Law has ended, programmers have to work hard to get high performance
    for their applications. But why is performance hard to deliver?


    I will first explain why current approaches to evaluating and optimizing
    performance don't work, especially on modern hardware and for modern
    applications. I then present two systems that address these challenges.
    Stabilizer is a tool that enables statistically sound performance
    evaluation, making it possible to understand the impact of optimizations and
    conclude things like the fact that the -O2 and -O3 optimization levels are
    indistinguishable from noise (sadly true).


    Since compiler optimizations have run out of steam, we need better profiling
    support, especially for modern concurrent, multi-threaded applications. Coz
    is a new "causal profiler" that lets programmers optimize for throughput or
    latency, and which pinpoints and accurately predicts the impact of
    optimizations. Coz's approach unlocks previously unknown optimization
    opportunities. Guided by Coz, we improved the performance of Memcached (9%),
    SQLite (25%), and accelerated six other applications by as much as 68%; in
    most cases, this involved modifying less than 10 lines of code and took
    under half an hour (without any prior understanding of the programs!). Coz
    now ships as part of standard Linux distros (apt install coz-profiler).
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Berger
      given: Emery
  citation-key: bergerPerformanceMatters2019
  event-title: Strange Loop 2019
  issued:
    - year: 2019
      month: 9
      day: 15
  note: 'score: 80'
  title: Performance Matters
  type: speech
  URL: https://www.youtube.com/watch?v=r-TLSBdHe1A

- id: berners-leeSemanticWeb2001
  accessed:
    - year: 2023
      month: 6
      day: 7
  author:
    - family: Berners-Lee
      given: Tim
    - family: Hendler
      given: James
    - family: Lassila
      given: Ora
  citation-key: berners-leeSemanticWeb2001
  container-title: Scientific American
  ISSN: 0036-8733
  issue: '5'
  issued:
    - year: 2001
  page: 34-43
  publisher: Scientific American, a division of Nature America, Inc.
  source: JSTOR
  title: The Semantic Web
  type: article-journal
  URL: https://www.jstor.org/stable/26059207
  volume: '284'

- id: berradaBaselineUnsupervisedAdvanced2020
  abstract: >-
    Advanced persistent threats (APTs) are stealthy, sophisticated, and
    unpredictable cyberattacks that can steal intellectual property, damage
    critical infrastructure, or cause millions of dollars in damage. Detecting
    APTs by monitoring system-level activity is difficult because manually
    inspecting the high volume of normal system activity is overwhelming for
    security analysts. We evaluate the effectiveness of unsupervised batch and
    streaming anomaly detection algorithms over multiple gigabytes of provenance
    traces recorded on four different operating systems to determine whether
    they can detect realistic APT-like attacks reliably and efficiently. This
    article is the first detailed study of the effectiveness of generic
    unsupervised anomaly detection techniques in this setting.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Berrada
      given: Ghita
    - family: Cheney
      given: James
    - family: Benabderrahmane
      given: Sidahmed
    - family: Maxwell
      given: William
    - family: Mookherjee
      given: Himan
    - family: Theriault
      given: Alec
    - family: Wright
      given: Ryan
  citation-key: berradaBaselineUnsupervisedAdvanced2020
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2020.02.015
  ISSN: 0167-739X
  issued:
    - year: 2020
      month: 7
      day: 1
  page: 401-413
  source: ScienceDirect
  title: >-
    A baseline for unsupervised advanced persistent threat detection in
    system-level provenance
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167739X19320448
  volume: '108'

- id: bertholdKNIMEKonstanzInformation2009
  abstract: >-
    The Konstanz Information Miner is a modular environment, which enables easy
    visual assembly and interactive execution of a data pipeline. It is designed
    as a teaching, research and collaboration platform, which enables simple
    integration of new algorithms and tools as well as data manipulation or
    visualization methods in the form of new modules or nodes. In this paper we
    describe some of the design aspects of the underlying architecture, briey
    sketch how new nodes can be incorporated, and highlight some of the new
    features of version 2.0.
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Berthold
      given: Michael R.
    - family: Cebron
      given: Nicolas
    - family: Dill
      given: Fabian
    - family: Gabriel
      given: Thomas R.
    - family: Kötter
      given: Tobias
    - family: Meinl
      given: Thorsten
    - family: Ohl
      given: Peter
    - family: Thiel
      given: Kilian
    - family: Wiswedel
      given: Bernd
  citation-key: bertholdKNIMEKonstanzInformation2009
  container-title: ACM SIGKDD Explorations Newsletter
  container-title-short: SIGKDD Explor. Newsl.
  DOI: 10.1145/1656274.1656280
  ISSN: 1931-0145, 1931-0153
  issue: '1'
  issued:
    - year: 2009
      month: 11
      day: 16
  language: en
  page: 26-31
  source: DOI.org (Crossref)
  title: 'KNIME - the Konstanz information miner: version 2.0 and beyond'
  title-short: KNIME - the Konstanz information miner
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1656274.1656280
  volume: '11'

- id: besseyFewBillionLines2010
  abstract: >-
    How Coverity built a bug-finding tool, and a business, around the unlimited
    supply of bugs in software systems.
  accessed:
    - year: 2023
      month: 11
      day: 1
  author:
    - family: Bessey
      given: Al
    - family: Block
      given: Ken
    - family: Chelf
      given: Ben
    - family: Chou
      given: Andy
    - family: Fulton
      given: Bryan
    - family: Hallem
      given: Seth
    - family: Henri-Gros
      given: Charles
    - family: Kamsky
      given: Asya
    - family: McPeak
      given: Scott
    - family: Engler
      given: Dawson
  citation-key: besseyFewBillionLines2010
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/1646353.1646374
  ISSN: 0001-0782
  issue: '2'
  issued:
    - year: 2010
      month: 2
      day: 1
  page: 66–75
  source: ACM Digital Library
  title: >-
    A few billion lines of code later: using static analysis to find bugs in the
    real world
  title-short: A few billion lines of code later
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1646353.1646374
  volume: '53'

- id: bestaBuildCloudDistributing2011
  author:
    - family: Besta
      given: Milos
    - family: Miretskiy
      given: Yevgeniy
    - family: Cox
      given: Jeff
  citation-key: bestaBuildCloudDistributing2011
  container-title: Google Engineering Tools
  issued:
    - year: 2011
      month: 10
      day: 27
  title: 'Build in the Cloud: Distributing Build Outputs'
  type: post-weblog
  URL: >-
    https://google-engtools.blogspot.com/2011/10/build-in-cloud-distributing-build.html

- id: beuzenPythonPackages2022
  abstract: >-
    Python packages are a core element of the Python programming language and
    are how you create organized, reusable, and shareable code in Python. Python
    Packages is an open source book that describes modern and efficient
    workflows for creating Python packages.
  author:
    - family: Beuzen
      given: Tomas
    - family: Timbers
      given: Tiffany
  citation-key: beuzenPythonPackages2022
  edition: 1st edition
  event-place: Boca Raton
  ISBN: 978-1-03-202944-3
  issued:
    - year: 2022
      month: 4
      day: 21
  language: English
  note: 'interest: 87'
  number-of-pages: '222'
  publisher: Chapman and Hall/CRC
  publisher-place: Boca Raton
  source: Amazon
  title: Python Packages
  type: book
  URL: https://py-pkgs.org/

- id: beyerApprenticingCustomer1995
  accessed:
    - year: 2022
      month: 6
      day: 7
  author:
    - family: Beyer
      given: Hugh R.
    - family: Holtzblatt
      given: Karen
  citation-key: beyerApprenticingCustomer1995
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/203356.203365
  ISSN: 0001-0782, 1557-7317
  issue: '5'
  issued:
    - year: 1995
      month: 5
  language: en
  page: 45-52
  source: DOI.org (Crossref)
  title: Apprenticing with the customer
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/203356.203365
  volume: '38'

- id: beyerReliableBenchmarkingRequirements2019
  abstract: >-
    Benchmarking is a widely used method in experimental computer science, in
    particular, for the comparative evaluation of tools and algorithms. As a
    consequence, a number of questions need to be answered in order to ensure
    proper benchmarking, resource measurement, and presentation of results, all
    of which is essential for researchers, tool developers, and users, as well
    as for tool competitions. We identify a set of requirements that are
    indispensable for reliable benchmarking and resource measurement of time and
    memory usage of automatic solvers, verifiers, and similar tools, and discuss
    limitations of existing methods and benchmarking tools. Fulfilling these
    requirements in a benchmarking framework can (on Linux systems) currently
    only be done by using the cgroup and namespace features of the kernel. We
    developed BenchExec, a ready-to-use, tool-independent, and open-source
    implementation of a benchmarking framework that fulfills all presented
    requirements, making reliable benchmarking and resource measurement easy.
    Our framework is able to work with a wide range of different tools, has
    proven its reliability and usefulness in the International Competition on
    Software Verification, and is used by several research groups worldwide to
    ensure reliable benchmarking. Finally, we present guidelines on how to
    present measurement results in a scientifically valid and comprehensible
    way.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Beyer
      given: Dirk
    - family: Löwe
      given: Stefan
    - family: Wendler
      given: Philipp
  citation-key: beyerReliableBenchmarkingRequirements2019
  container-title: International Journal on Software Tools for Technology Transfer
  container-title-short: Int J Softw Tools Technol Transfer
  DOI: 10.1007/s10009-017-0469-y
  ISSN: 1433-2779, 1433-2787
  issue: '1'
  issued:
    - year: 2019
      month: 2
      day: 6
  language: en
  note: 'interest: 90'
  page: 1-29
  source: Springer Link
  title: 'Reliable benchmarking: requirements and solutions'
  title-short: Reliable benchmarking
  type: article-journal
  URL: https://link.springer.com/10.1007/s10009-017-0469-y
  volume: '21'

- id: bilderDOIlikeStringsFake2016
  abstract: >-
    TL;DR Crossref discourages our members from using DOI-like strings or fake
    DOIs.

    Details Recently we have seen quite a bit of debate around the use of
    so-called “fake-DOIs.” We have also been quoted as saying that we discourage
    the use of “fake DOIs” or “DOI-like strings”. This post outlines some of the
    cases in which we’ve seen fake DOIs used and why we recommend against doing
    so.

    Using DOI-like strings as internal identifiers Some of our members use
    DOI-like strings as internal identifiers for their manuscript tracking
    systems.
  accessed:
    - year: 2023
      month: 6
      day: 16
  author:
    - family: Bilder
      given: Geoffrey
  citation-key: bilderDOIlikeStringsFake2016
  container-title: Crossref
  genre: website
  issued:
    - year: 2016
      month: 6
      day: 29
  language: en
  license: CC BY 4.0
  title: DOI-like strings and fake DOIs
  type: post-weblog
  URL: https://www.crossref.org/blog/doi-like-strings-and-fake-dois/

- id: bilderStructureORCIDIdentifiers2012
  accessed:
    - year: 2023
      month: 5
      day: 30
  author:
    - family: Bilder
      given: Geoffrey
  citation-key: bilderStructureORCIDIdentifiers2012
  container-title: Google Docs
  issued:
    - year: 2012
      month: 8
      day: 2
  language: en
  title: Structure of ORCID Identifiers V7
  type: webpage
  URL: >-
    https://docs.google.com/document/d/1awd6PPguRAdZsC6CKpFSSSu1dulliT8E3kHwIJ3tD5o/edit?usp=embed_facebook

- id: billahUsingDataGrid2016
  abstract: >-
    Modeling a regional-scale hydrologic system introduces major data challenges
    related to the access and transformation of heterogeneous datasets into the
    information needed to execute a hydrologic model. These data preparation
    activities are difficult to automate, making the reproducibility and
    extensibility of model simulations conducted by others difficult or even
    impossible. This study addresses this challenge by demonstrating how the
    integrated Rule Oriented Data Management System (iRODS) can be used to
    support data processing pipelines needed when using data-intensive models to
    simulate regional-scale hydrologic systems. Focusing on the Variable
    Infiltration Capacity (VIC) model as a case study, data preparation steps
    are sequenced using rules within iRODS. VIC and iRODS are applied to study
    hydrologic conditions in the Carolinas, USA during the period 1998–2007 to
    better understand impacts of drought within the region. The application
    demonstrates how iRODS can support hydrologic modelers to create more
    reproducible and extensible model-based analyses.
  accessed:
    - year: 2024
      month: 2
      day: 5
  author:
    - family: Billah
      given: Mirza M.
    - family: Goodall
      given: Jonathan L.
    - family: Narayan
      given: Ujjwal
    - family: Essawy
      given: Bakinam T.
    - family: Lakshmi
      given: Venkat
    - family: Rajasekar
      given: Arcot
    - family: Moore
      given: Reagan W.
  citation-key: billahUsingDataGrid2016
  container-title: Environmental Modelling & Software
  container-title-short: Environmental Modelling & Software
  DOI: 10.1016/j.envsoft.2015.12.010
  ISSN: 1364-8152
  issued:
    - year: 2016
      month: 4
      day: 1
  page: 31-39
  source: ScienceDirect
  title: >-
    Using a data grid to automate data preparation pipelines required for
    regional-scale hydrologic modeling
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S1364815215301249
  volume: '78'

- id: blosteinIssuesPracticalUse1996
  accessed:
    - year: 2022
      month: 8
      day: 3
  author:
    - family: Blostein
      given: Dorothea
    - family: Fahmy
      given: Hoda
    - family: Grbavec
      given: Ann
  citation-key: blosteinIssuesPracticalUse1996
  collection-editor:
    - family: Goos
      given: Gerhard
    - family: Hartmanis
      given: Juris
    - family: Leeuwen
      given: Jan
      non-dropping-particle: van
  container-title: Graph Grammars and Their Application to Computer Science
  DOI: 10.1007/3-540-61228-9_78
  editor:
    - family: Cuny
      given: Janice
    - family: Ehrig
      given: Hartmut
    - family: Engels
      given: Gregor
    - family: Rozenberg
      given: Grzegorz
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-61228-5 978-3-540-68388-9
  issued:
    - year: 1996
  page: 38-55
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: Issues in the practical use of graph rewriting
  type: chapter
  URL: http://link.springer.com/10.1007/3-540-61228-9_78
  volume: '1073'

- id: bohannonWhoAfraidPeer2013
  abstract: |-
    A spoof paper concocted by 
                  Science 
                  reveals little or no scrutiny at many open-access journals. 
                 
              ,  
                 
                  Dozens of open-access journals targeted in an elaborate 
                  Science 
                  sting accepted a spoof research article, raising questions about peer-review practices in much of the open-access world.
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Bohannon
      given: John
  citation-key: bohannonWhoAfraidPeer2013
  container-title: Science
  container-title-short: Science
  DOI: 10.1126/science.342.6154.60
  ISSN: 0036-8075, 1095-9203
  issue: '6154'
  issued:
    - year: 2013
      month: 10
      day: 4
  language: en
  page: 60-65
  publisher: American Association for the Advancement of Science
  source: science.org (Atypon)
  title: Who's Afraid of Peer Review?
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.342.6154.60
  volume: '342'

- id: bontchevAreGoodComputer1994
  accessed:
    - year: 2023
      month: 10
      day: 12
  author:
    - family: Bontchev
      given: Vesselin
  citation-key: bontchevAreGoodComputer1994
  container-title: Proc. EICAR’94 Conf.
  event-title: EICAR
  issued:
    - year: 1994
  page: 25-47
  title: Are 'Good' Computer Viruses Still a Bad Idea?
  type: paper-conference
  URL: https://cryptohub.nl/zines/vxheavens/lib/avb02.html

- id: borgmanWhoGotData2012
  abstract: >-
    Science and technology always have been interdependent, but never more so
    than with today’s highly instrumented data collection practices. We report
    on a long-term study of collaboration between environmental scientists
    (biology, ecology, marine sciences), computer scientists, and engineering
    research teams as part of a five-university distributed science and
    technology research center devoted to embedded networked sensing. The
    science and technology teams go into the field with mutual interests in
    gathering scientific data. “Data” are constituted very differently between
    the research teams. What are data to the science teams may be context to the
    technology teams, and vice versa. Interdependencies between the teams
    determine the ability to collect, use, and manage data in both the short and
    long terms. Four types of data were identified, which are managed
    separately, limiting both reusability of data and replication of research.
    Decisions on what data to curate, for whom, for what purposes, and for how
    long, should consider the interdependencies between scientific and technical
    processes, the complexities of data collection, and the disposition of the
    resulting data.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Borgman
      given: Christine L.
    - family: Wallis
      given: Jillian C.
    - family: Mayernik
      given: Matthew S.
  citation-key: borgmanWhoGotData2012
  container-title: Computer Supported Cooperative Work (CSCW)
  container-title-short: Comput Supported Coop Work
  DOI: 10.1007/s10606-012-9169-z
  ISSN: 1573-7551
  issue: '6'
  issued:
    - year: 2012
      month: 12
      day: 1
  language: en
  note: 'interest: 80'
  page: 485-523
  source: Springer Link
  title: >-
    Who’s Got the Data? Interdependencies in Science and Technology
    Collaborations
  title-short: Who’s Got the Data?
  type: article-journal
  URL: https://doi.org/10.1007/s10606-012-9169-z
  volume: '21'

- id: bourqueSWEBOKGuideSoftware2014
  author:
    - family: Bourque
      given: Pierre
    - family: Fairley
      given: R. E
    - literal: IEEE Computer Society
  citation-key: bourqueSWEBOKGuideSoftware2014
  ISBN: 978-0-7695-5166-1
  issued:
    - year: 2014
  language: English
  note: 'OCLC: 880350861'
  source: Open WorldCat
  title: 'SWEBOK: guide to the software engineering body of knowledge'
  title-short: SWEBOK
  type: book

- id: BPFDocumentation
  accessed:
    - year: 2023
      month: 8
      day: 24
  citation-key: BPFDocumentation
  container-title: The Linux Kernel documentation
  title: BPF Documentation
  type: webpage
  URL: https://docs.kernel.org/bpf/index.html

- id: braunIssuesAutomaticProvenance2006
  abstract: >-
    Automatic provenance collection describes systems that observe processes and
    data transformations inferring, collecting, and maintaining provenance about
    them. Automatic collection is a powerful tool for analysis of objects and
    processes, providing a level of transparency and pervasiveness not found in
    more conventional provenance systems. Unfortunately, automatic collection is
    also diﬃcult. We discuss the challenges we encountered and the issues we
    exposed as we developed an automatic provenance collector that runs at the
    operating system level.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Braun
      given: Uri
    - family: Garfinkel
      given: Simson
    - family: Holland
      given: David A.
    - family: Muniswamy-Reddy
      given: Kiran-Kumar
    - family: Seltzer
      given: Margo I.
  citation-key: braunIssuesAutomaticProvenance2006
  collection-editor:
    - family: Hutchison
      given: David
    - family: Kanade
      given: Takeo
    - family: Kittler
      given: Josef
    - family: Kleinberg
      given: Jon M.
    - family: Mattern
      given: Friedemann
    - family: Mitchell
      given: John C.
    - family: Naor
      given: Moni
    - family: Nierstrasz
      given: Oscar
    - family: Pandu Rangan
      given: C.
    - family: Steffen
      given: Bernhard
    - family: Sudan
      given: Madhu
    - family: Terzopoulos
      given: Demetri
    - family: Tygar
      given: Dough
    - family: Vardi
      given: Moshe Y.
    - family: Weikum
      given: Gerhard
  container-title: Provenance and Annotation of Data
  DOI: 10.1007/11890850_18
  editor:
    - family: Moreau
      given: Luc
    - family: Foster
      given: Ian
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-46302-3 978-3-540-46303-0
  issued:
    - year: 2006
  language: en
  page: 171-183
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: Issues in Automatic Provenance Collection
  type: chapter
  URL: http://link.springer.com/10.1007/11890850_18
  volume: '4145'

- id: brinckmanComputingEnvironmentsReproducibility2019
  accessed:
    - year: 2024
      month: 9
      day: 4
  author:
    - family: Brinckman
      given: Adam
    - family: Chard
      given: Kyle
    - family: Gaffney
      given: Niall
    - family: Hategan
      given: Mihael
    - family: Jones
      given: Matthew B.
    - family: Kowalik
      given: Kacper
    - family: Kulasekaran
      given: Sivakumar
    - family: Ludäscher
      given: Bertram
    - family: Mecum
      given: Bryce D.
    - family: Nabrzyski
      given: Jarek
    - family: Stodden
      given: Victoria
    - family: Taylor
      given: Ian J.
    - family: Turk
      given: Matthew J.
    - family: Turner
      given: Kandace
  citation-key: brinckmanComputingEnvironmentsReproducibility2019
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.12.029
  ISSN: 0167739X
  issued:
    - year: 2019
      month: 5
  language: en
  page: 854-867
  source: DOI.org (Crossref)
  title: 'Computing environments for reproducibility: Capturing the “Whole Tale”'
  title-short: Computing environments for reproducibility
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0167739X17310695
  volume: '94'

- id: brinckmanComputingEnvironmentsReproducibility2019a
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Brinckman
      given: Adam
    - family: Chard
      given: Kyle
    - family: Gaffney
      given: Niall
    - family: Hategan
      given: Mihael
    - family: Jones
      given: Matthew B.
    - family: Kowalik
      given: Kacper
    - family: Kulasekaran
      given: Sivakumar
    - family: Ludäscher
      given: Bertram
    - family: Mecum
      given: Bryce D.
    - family: Nabrzyski
      given: Jarek
    - family: Stodden
      given: Victoria
    - family: Taylor
      given: Ian J.
    - family: Turk
      given: Matthew J.
    - family: Turner
      given: Kandace
  citation-key: brinckmanComputingEnvironmentsReproducibility2019a
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.12.029
  ISSN: 0167739X
  issued:
    - year: 2019
      month: 5
  language: en
  page: 854-867
  source: DOI.org (Crossref)
  title: 'Computing environments for reproducibility: Capturing the “Whole Tale”'
  title-short: Computing environments for reproducibility
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0167739X17310695
  volume: '94'

- id: bromanDataOrganizationSpreadsheets2018
  abstract: >-
    Spreadsheets are widely used software tools for data entry, storage,
    analysis, and visualization. Focusing on the data entry and storage aspects,
    this article offers practical recommendations for organizing spreadsheet
    data to reduce errors and ease later analyses. The basic principles are: be
    consistent, write dates like YYYY-MM-DD, do not leave any cells empty, put
    just one thing in a cell, organize the data as a single rectangle (with
    subjects as rows and variables as columns, and with a single header row),
    create a data dictionary, do not include calculations in the raw data files,
    do not use font color or highlighting as data, choose good names for things,
    make backups, use data validation to avoid data entry errors, and save the
    data in plain text files.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Broman
      given: Karl W.
    - family: Woo
      given: Kara H.
  citation-key: bromanDataOrganizationSpreadsheets2018
  container-title: The American Statistician
  DOI: 10.1080/00031305.2017.1375989
  ISSN: 0003-1305
  issue: '1'
  issued:
    - year: 2018
      month: 1
      day: 2
  page: 2-10
  publisher: Taylor & Francis
  source: Taylor and Francis+NEJM
  title: Data Organization in Spreadsheets
  type: article-journal
  URL: https://doi.org/10.1080/00031305.2017.1375989
  volume: '72'

- id: brookeSUSQuickDirty1996
  abstract: >-
    Usability is not a quality that exists in any real or absolute sense.
    Perhaps it can be best summed up as being a general quality of the
    appropriateness to a purpose of any particular artefact. This notion is
    neatly summed up by Terry Pratchett in his novel Moving Pictures:


    In just the same way, the usability of any tool or system has to be viewed
    in terms of the context in which it is used, and its appropriateness to that
    context. With particular reference to information systems, this view of
    usability is reflected in the current draft international standard ISO
    9241-11 and in the European Community ESPRIT project MUSiC (Measuring
    Usability of Systems in Context) (e.g. Bevan et al., 1991). In general, it
    is impossible to specify the usability of a system (i.e. its fitness for
    purpose) without first defining who are the intended users of the system,
    the tasks those users will perform with it, and the characteristics of the
    physical, organizational and social environment in which it will be used.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Brooke
      given: John
  citation-key: brookeSUSQuickDirty1996
  container-title: Usability Evaluation In Industry
  DOI: 10.1201/9781498710411-35
  edition: '1'
  editor:
    - family: Jordan
      given: Patrick W.
    - family: Thomas
      given: B.
    - family: McClelland
      given: Ian Lyall
    - family: Weerdmeester
      given: Bernard
  ISBN: 978-0-429-15701-1
  issued:
    - year: 1996
      month: 6
      day: 11
  language: en
  page: 207-212
  publisher: CRC Press
  source: DOI.org (Crossref)
  title: 'SUS: A ''Quick and Dirty'' Usability Scale'
  title-short: SUS
  type: chapter
  URL: >-
    https://www.taylorfrancis.com/books/9781498710411/chapters/10.1201/9781498710411-35

- id: brownCaseStudyUse2007
  abstract: >-
    Modern scientific experiments acquire large amounts of data that must be
    analyzed in subtle and complicated ways to extract the best results. The
    Laser Interferometer Gravitational Wave Observatory (LIGO) is an ambitious
    effort to detect gravitational waves produced by violent events in the
    universe, such as the collision of two black holes or the explosion of
    supernovae [37,258]. The experiment records approximately 1 TB of data per
    day, which is analyzed by scientists in a collaboration that spans four
    continents. LIGO and distributed computing have grown up side by side over
    the past decade, and the analysis strategies adopted by LIGO scientists have
    been strongly influenced by the increasing power of tools to manage
    distributed computing resources and the workflows to run on them. In this
    chapter, we use LIGO as an application case study in workflow design and
    implementation. The software architecture outlined here has been used with
    great efficacy to analyze LIGO data [2–5] using dedicated computing
    facilities operated by the LIGO Scientific Collaboration, the LIGO Data
    Grid. It is just the first step, however. Workflow design and implementation
    lies at the interface between computing and traditional scientific
    activities. In the conclusion, we outline a few directions for future
    development and provide some long-term vision for applications related to
    gravitational wave data analysis.
  accessed:
    - year: 2023
      month: 1
      day: 31
  author:
    - family: Brown
      given: Duncan A.
    - family: Brady
      given: Patrick R.
    - family: Dietz
      given: Alexander
    - family: Cao
      given: Junwei
    - family: Johnson
      given: Ben
    - family: McNabb
      given: John
  citation-key: brownCaseStudyUse2007
  container-title: 'Workflows for e-Science: Scientific Workflows for Grids'
  DOI: 10.1007/978-1-84628-757-2_4
  editor:
    - family: Taylor
      given: Ian J.
    - family: Deelman
      given: Ewa
    - family: Gannon
      given: Dennis B.
    - family: Shields
      given: Matthew
  event-place: London
  ISBN: 978-1-84628-757-2
  issued:
    - year: 2007
  language: en
  page: 39-59
  publisher: Springer
  publisher-place: London
  source: Springer Link
  title: >-
    A Case Study on the Use of Workflow Technologies for Scientific Analysis:
    Gravitational Wave Data Analysis
  title-short: A Case Study on the Use of Workflow Technologies for Scientific Analysis
  type: chapter
  URL: https://doi.org/10.1007/978-1-84628-757-2_4

- id: bryanEnzoAdaptiveMesh2014
  abstract: >-
    This paper describes the open-source code Enzo, which uses block-structured
    adaptive mesh refinement to provide high spatial and temporal resolution for
    modeling astrophysical fluid flows. The code is Cartesian, can be run in
    one, two, and three dimensions, and supports a wide variety of physics
    including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body
    dynamics (and, more broadly, self-gravity of fluids and particles),
    primordial gas chemistry, optically thin radiative cooling of primordial and
    metal-enriched plasmas (as well as some optically-thick cooling models),
    radiation transport, cosmological expansion, and models for star formation
    and feedback in a cosmological context. In addition to explaining the
    algorithms implemented, we present solutions for a wide range of test
    problems, demonstrate the code's parallel performance, and discuss the Enzo
    collaboration's code development methodology.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Bryan
      given: Greg L.
    - family: Norman
      given: Michael L.
    - family: O'Shea
      given: Brian W.
    - family: Abel
      given: Tom
    - family: Wise
      given: John H.
    - family: Turk
      given: Matthew J.
    - family: Reynolds
      given: Daniel R.
    - family: Collins
      given: David C.
    - family: Wang
      given: Peng
    - family: Skillman
      given: Samuel W.
    - family: Smith
      given: Britton
    - family: Harkness
      given: Robert P.
    - family: Bordner
      given: James
    - family: Kim
      given: Ji-hoon
    - family: Kuhlen
      given: Michael
    - family: Xu
      given: Hao
    - family: Goldbaum
      given: Nathan
    - family: Hummels
      given: Cameron
    - family: Kritsuk
      given: Alexei G.
    - family: Tasker
      given: Elizabeth
    - family: Skory
      given: Stephen
    - family: Simpson
      given: Christine M.
    - family: Hahn
      given: Oliver
    - family: Oishi
      given: Jeffrey S.
    - family: So
      given: Geoffrey C.
    - family: Zhao
      given: Fen
    - family: Cen
      given: Renyue
    - family: and
      given: Yuan Li
  citation-key: bryanEnzoAdaptiveMesh2014
  container-title: The Astrophysical Journal Supplement Series
  container-title-short: ApJS
  DOI: 10.1088/0067-0049/211/2/19
  ISSN: 0067-0049
  issue: '2'
  issued:
    - year: 2014
      month: 3
  language: en
  page: '19'
  publisher: American Astronomical Society
  source: Institute of Physics
  title: 'Enzo: An Adaptive Mesh Refinement Code for Astrophysics'
  title-short: ENZO
  type: article-journal
  URL: https://doi.org/10.1088/0067-0049/211/2/19
  volume: '211'

- id: bryanPiecewiseParabolicMethod1995
  abstract: >-
    We describe a hybrid scheme for cosmological simulations that incorporates a
    Lagrangean particle-mesh (PM) algorithm to follow the collisionless matter
    with the higher order accurate piecewise parabolic method (PPM) to solve the
    equations of gas dynamics. Both components interact through the
    gravitational potential, which requires the solution of Poisson's equation,
    here done by Fourier transforms. Due to the vast range of conditions that
    occur in cosmological flows (pressure differences of up to fourteen orders
    of magnitude), a number of additions and modifications to PPM were required
    to produce accurate results. These are described, as are a suite of
    cosmological tests.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Bryan
      given: Greg L.
    - family: Norman
      given: Michael L.
    - family: Stone
      given: James M.
    - family: Cen
      given: Renyue
    - family: Ostriker
      given: Jeremiah P.
  citation-key: bryanPiecewiseParabolicMethod1995
  container-title: Computer Physics Communications
  DOI: 10.1016/0010-4655(94)00191-4
  ISSN: 0010-4655
  issued:
    - year: 1995
      month: 8
      day: 1
  note: 'ADS Bibcode: 1995CoPhC..89..149B'
  page: 149-168
  source: NASA ADS
  title: A piecewise parabolic method for cosmological hydrodynamics
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1995CoPhC..89..149B
  volume: '89'

- id: buckheitWaveLabReproducibleResearch1995
  abstract: >-
    WaveLab is a library of Matlab routines for wavelet analysis, wavelet-packet
    analysis, cosine-packet analysis and matching pursuit. The library is
    available free of charge over the Internet. Versions are provided for
    Macintosh, UNIX and Windows machines.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Buckheit
      given: Jonathan B.
    - family: Donoho
      given: David L.
  citation-key: buckheitWaveLabReproducibleResearch1995
  collection-editor:
    - family: Bickel
      given: P.
    - family: Diggle
      given: P.
    - family: Fienberg
      given: S.
    - family: Krickeberg
      given: K.
    - family: Olkin
      given: I.
    - family: Wermuth
      given: N.
    - family: Zeger
      given: S.
  container-title: Wavelets and Statistics
  DOI: 10.1007/978-1-4612-2544-7_5
  editor:
    - family: Antoniadis
      given: Anestis
    - family: Oppenheim
      given: Georges
  event-place: New York, NY
  ISBN: 978-0-387-94564-4 978-1-4612-2544-7
  issued:
    - year: 1995
  language: en
  page: 55-81
  publisher: Springer New York
  publisher-place: New York, NY
  source: DOI.org (Crossref)
  title: WaveLab and Reproducible Research
  type: chapter
  URL: http://link.springer.com/10.1007/978-1-4612-2544-7_5
  volume: '103'

- id: bugayenkoAcademicWritingLATEX
  abstract: >-
    This is a humble attempt to summarize most typical mistakes we make while
    writing academic papers

    in LATEX and most important recommendations. Each suggestion or a mistake
    takes a short paragraph of

    description right here and also may suggest looking into a more detailed
    explanation in some other online

    resource. We recommend, before submitting your paper to a conference or a
    journal, go through this list of

    mistakes and make sure none of them are present in your paper
  author:
    - family: Bugayenko
      given: Yegor
  citation-key: bugayenkoAcademicWritingLATEX
  language: en
  title: 'Academic Writing in LATEX: Best and Worst Practices'
  type: post-weblog

- id: buranyiHitechWarScience2017
  abstract: >-
    The Long Read: The problem of fake data may go far deeper than scientists
    admit. Now a team of researchers has a controversial plan to root out the
    perpetrators
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Buranyi
      given: Stephen
  citation-key: buranyiHitechWarScience2017
  container-title: The Guardian
  ISSN: 0261-3077
  issued:
    - year: 2017
      month: 2
      day: 1
  language: en-GB
  section: Science
  source: The Guardian
  title: The hi-tech war on science fraud
  type: article-newspaper
  URL: https://www.theguardian.com/science/2017/feb/01/high-tech-war-on-science

- id: buranyiStaggeringlyProfitableBusiness2017
  abstract: >-
    The long read: It is an industry like no other, with profit margins to rival
    Google – and it was created by one of Britain’s most notorious tycoons:
    Robert Maxwell
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Buranyi
      given: Stephen
  citation-key: buranyiStaggeringlyProfitableBusiness2017
  container-title: The Guardian
  ISSN: 0261-3077
  issued:
    - year: 2017
      month: 6
      day: 27
  language: en-GB
  section: Science
  source: The Guardian
  title: >-
    Is the staggeringly profitable business of scientific publishing bad for
    science?
  type: article-newspaper
  URL: >-
    https://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science

- id: burkatServerlessContainersRising2021
  abstract: >-
    The increasing popularity of the serverless computing approach has led to
    the emergence of new cloud infrastructures working in Container-as-a-Service
    (CaaS) model like AWS Fargate, Google Cloud Run, or Azure Container
    Instances. New infrastructures facilitate an innovative approach to running
    cloud containers where developers are freed from managing underlying
    resources. In this paper, we focus on evaluating the capabilities of elastic
    containers and their usefulness for scientific computing in the scientific
    workflow paradigm using AWS Fargate and Google Cloud Run infrastructures.
    For the experimental evaluation of our approach, we extended the HyperFlow
    engine to support these CaaS platforms, together with adapting four
    scientific workflows composed of several dozen to hundreds of tasks
    organized into a dependency graph. Studied applications are used to create
    cost-performance benchmarks and flow execution plots, delay, elasticity, and
    scalability measurements. Results show that serverless containers can be
    successfully utilized for running scientific workflows. Moreover, the
    results allow for gaining insight into the specific advantages and limits of
    the studied platforms.
  author:
    - family: Burkat
      given: Krzysztof
    - family: Pawlik
      given: Maciej
    - family: Balis
      given: Bartosz
    - family: Malawski
      given: Maciej
    - family: Vahi
      given: Karan
    - family: Rynge
      given: Mats
    - family: Ferreira da Silva
      given: Rafael
    - family: Deelman
      given: Ewa
  citation-key: burkatServerlessContainersRising2021
  container-title: 2021 IEEE 17th International Conference on eScience (eScience)
  DOI: 10.1109/eScience51609.2021.00014
  event-title: 2021 IEEE 17th International Conference on eScience (eScience)
  issued:
    - year: 2021
      month: 9
  note: 'interest: 90'
  page: 40-49
  source: IEEE Xplore
  title: Serverless Containers – Rising Viable Approach to Scientific Workflows
  type: paper-conference

- id: BURRITOWrappingYour2012
  accessed:
    - year: 2023
      month: 7
      day: 7
  citation-key: BURRITOWrappingYour2012
  event-title: 4th USENIX Workshop on the Theory and Practice of Provenance (TaPP 12)
  issued:
    - year: 2012
  language: en
  note: 'interest: 98'
  source: www.usenix.org
  title: '{BURRITO}: Wrapping Your Lab Notebook in Computational Infrastructure'
  title-short: '{BURRITO}'
  type: paper-conference
  URL: https://www.usenix.org/conference/tapp12/workshop-program/presentation/guo

- id: burtonWorkloadCharacterizationUsing1998
  abstract: >-
    This paper shows how system call traces can be obtained with minimal
    interference to the system being characterized, and used as realistic,
    repeatable workloads for experiments to evaluate operating system and file
    system designs and configuration alternatives. Our system call trace
    mechanism, called ULTra, captures a complete trace of each UNIX process's
    calls to the operating system. The performance impact is normally small, and
    it runs in user mode without special privileges. We show how the resulting
    traces can be used to drive full, repeatable reexecution of the captured
    behaviour, and present a case study which shows the usefulness and accuracy
    of the tool for predicting the impact of file system caching on a WWW
    server's performance.
  accessed:
    - year: 2024
      month: 2
      day: 13
  author:
    - family: Burton
      given: A.N.
    - family: Kelly
      given: P.H.J.
  citation-key: burtonWorkloadCharacterizationUsing1998
  container-title: >-
    1998 IEEE International Performance, Computing and Communications
    Conference. Proceedings (Cat. No.98CH36191)
  DOI: 10.1109/PCCC.1998.659975
  event-title: >-
    1998 IEEE International Performance, Computing and Communications
    Conference. Proceedings (Cat. No.98CH36191)
  ISSN: 1097-2641
  issued:
    - year: 1998
      month: 2
  page: 260-266
  source: IEEE Xplore
  title: >-
    Workload characterization using lightweight system call tracing and
    reexecution
  type: paper-conference
  URL: >-
    https://ieeexplore.ieee.org/abstract/document/659975?casa_token=HKVbz10sLZIAAAAA:smrnozHPF8pQo71mmNw8svhVLFy0aaKiIpNKcdiy9BFIju5IIw0KjE0be84S9TrdIgB9BdK8

- id: butlerInvestigatingJournalsDark2013
  abstract: >-
    The explosion in open-access publishing has fuelled the rise of questionable
    operators.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Butler
      given: Declan
  citation-key: butlerInvestigatingJournalsDark2013
  container-title: Nature
  DOI: 10.1038/495433a
  ISSN: 1476-4687
  issue: '7442'
  issued:
    - year: 2013
      month: 3
      day: 1
  language: en
  license: 2013 Nature Publishing Group
  number: '7442'
  page: 433-435
  publisher: Nature Publishing Group
  source: www.nature.com
  title: 'Investigating journals: The dark side of publishing'
  title-short: Investigating journals
  type: article-journal
  URL: https://www.nature.com/articles/495433a
  volume: '495'

- id: buttProvONEProvenanceModel2020
  abstract: >-
    The provenance of workflows is essential, both for the data they derive and
    for their specification, to allow for the reproducibility, sharing and reuse
    of information in the scientific community. Although the formal modelling of
    scientific workflow provenance was of interest and studied, in many fields
    like semantic web, yet no provenance model has existed, we are aware of, to
    model control-flow driven scientific workflows. The provenance models
    proposed by the semantic web community for data-driven scientific workflows
    may capture the provenance of control-flow driven workflows execution traces
    (i.e., retrospective provenance) but underspecify the workflow structure
    (i.e., workflow provenance). An underspecified or incomplete structure of a
    workflow results in the misinterpretation of a scientific experiment and
    precludes conformance checking of the workflow, thereby restricting the
    gains of provenance. To overcome the limitation, we present a formal,
    lightweight and general-purpose specification model for the control-flows
    involved scientific workflows. The proposed model can be combined with the
    existing provenance models and easy to extend to specify the common
    control-flow patterns. In this article, we inspire the need for control-flow
    driven scientific workflow provenance model, provide an overview of its key
    classes and properties, and briefly discuss its integration with the ProvONE
    provenance model as well as its compatibility to PROV-DM. We will also focus
    on the sample modelling using the proposed model and present a comprehensive
    implementation scenario from the agricultural domain for validating the
    model.
  author:
    - family: Butt
      given: Anila Sahar
    - family: Fitch
      given: Peter
  citation-key: buttProvONEProvenanceModel2020
  collection-title: Web Information Systems Engineering – WISE 2020
  container-title: Web Information Systems Engineering – WISE 2020
  DOI: 10.1007/978-3-030-62008-0_30
  editor:
    - family: Huang
      given: Zhisheng
    - family: Beek
      given: Wouter
    - family: Wang
      given: Hua
    - family: Zhou
      given: Rui
    - family: Zhang
      given: Yanchun
  event-place: Cham
  ISBN: 978-3-030-62008-0
  issued:
    - year: 2020
  language: en
  page: 431-444
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: 'ProvONE+: A Provenance Model for Scientific Workflows'
  title-short: ProvONE+
  type: paper-conference

- id: bzeznikNixHPCPackage2017
  abstract: >-
    Modern High Performance Computing systems are becoming larger and more
    heterogeneous. The proper management of software for the users of such
    systems poses a significant challenge. These users run very diverse
    applications that may be compiled with proprietary tools for specialized
    hardware. Moreover, the application life-cycle of these software may exceed
    the lifetime of the HPC systems themselves. These difficulties motivate the
    use of specialized package management systems. In this paper, we outline an
    approach to HPC package development, deployment, management, sharing, and
    reuse based on the Nix functional package manager. We report our experience
    with this approach inside the GRICAD HPC center[GRICAD 2017a] in Grenoble
    over a 12 month period and compare it to other existing approaches.
  accessed:
    - year: 2023
      month: 5
      day: 5
  author:
    - family: Bzeznik
      given: Bruno
    - family: Henriot
      given: Oliver
    - family: Reis
      given: Valentin
    - family: Richard
      given: Olivier
    - family: Tavard
      given: Laure
  citation-key: bzeznikNixHPCPackage2017
  collection-title: HUST'17
  container-title: Proceedings of the Fourth International Workshop on HPC User Support Tools
  DOI: 10.1145/3152493.3152556
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5130-0
  issued:
    - year: 2017
      month: 11
      day: 12
  note: 'interest: 99'
  page: 1–6
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Nix as HPC package management system
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3152493.3152556

- id: caralliIntroducingOCTAVEAllegro2007
  abstract: >-
    This technical report introduces the next generation of the Operationally
    Critical Threat, Asset, and Vulnerability Evaluation OCTAVE methodology,
    OCTAVE Allegro. OCTAVE Allegro is a methodology to streamline and optimize
    the process of assessing information security risks so that an organization
    can obtain sufficient results with a small investment in time, people, and
    other limited resources. It leads the organization to consider people,
    technology, and facilities in the context of their relationship to
    information and the business processes and services they support. This
    report highlights the design considerations and requirements for OCTAVE
    Allegro based on field experience with existing OCTAVE methods and provides
    guidance, worksheets, and examples that an organization can use to begin
    performing OCTAVE Allegro-based risk assessments.
  author:
    - family: Caralli
      given: Richard A.
    - family: Stevens
      given: James F.
    - family: Young
      given: Lisa R.
    - family: Wilson
      given: William R.
  citation-key: caralliIntroducingOCTAVEAllegro2007
  genre: Technical Report
  issued:
    - year: 2007
      month: 5
      day: 1
  number: CMU/SEI-2007-TR-012
  page: '154'
  publisher: Carnegie Mellon University
  title: >-
    Introducing OCTAVE Allegro: Improving the Information Security Risk
    Assessment Process
  type: report
  URL: https://apps.dtic.mil/sti/citations/ADA470450

- id: carataPrimerProvenance2014
  abstract: Better understanding data requires tracking its history and context.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Carata
      given: Lucian
    - family: Akoush
      given: Sherif
    - family: Balakrishnan
      given: Nikilesh
    - family: Bytheway
      given: Thomas
    - family: Sohan
      given: Ripduman
    - family: Seltzer
      given: Margo
    - family: Hopper
      given: Andy
  citation-key: carataPrimerProvenance2014
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/2596628
  ISSN: 0001-0782
  issue: '5'
  issued:
    - year: 2014
      month: 5
      day: 1
  page: 52–60
  source: ACM Digital Library
  title: A primer on provenance
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2596628
  volume: '57'

- id: carataPrimerProvenanceBetter2014
  abstract: >-
    Assessing the quality or validity of a piece of data is not usually done in
    isolation. You typically examine the context in which the data appears and
    try to determine its original sources or review the process through which it
    was created. This is not so straightforward when dealing with digital data,
    however: the result of a computation might have been derived from numerous
    sources and by applying complex successive transformations, possibly over
    long periods of time.
  accessed:
    - year: 2023
      month: 7
      day: 18
  author:
    - family: Carata
      given: Lucian
    - family: Akoush
      given: Sherif
    - family: Balakrishnan
      given: Nikilesh
    - family: Bytheway
      given: Thomas
    - family: Sohan
      given: Ripduman
    - family: Seltzer
      given: Margo
    - family: Hopper
      given: Andy
  citation-key: carataPrimerProvenanceBetter2014
  container-title: Queue
  container-title-short: Queue
  DOI: 10.1145/2602649.2602651
  ISSN: 1542-7730
  issue: '3'
  issued:
    - year: 2014
      month: 3
      day: 1
  page: 10–23
  source: ACM Digital Library
  title: >-
    A Primer on Provenance: Better understanding of data requires tracking its
    history and context.
  title-short: A Primer on Provenance
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2602649.2602651
  volume: '12'

- id: carloUnderstandLegacyCode
  abstract: >-
    We all have to deal with Legacy Code. But it's damn hard to!


    Here you'll find answers to your questions. I'm sharing useful tips and
    concrete advice that will help you tame the legacy codebase you've
    inherited.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Carlo
      given: Nicolas
  citation-key: carloUnderstandLegacyCode
  container-title: Understanding Legacy Code
  language: en
  note: 'interest: 81'
  title: Understand Legacy Code
  type: webpage
  URL: https://understandlegacycode.com

- id: carruthGoogleHereBe2011
  author:
    - family: Carruth
      given: Chandler
  citation-key: carruthGoogleHereBe2011
  container-title: Google Engineering Tools
  issued:
    - year: 2011
      month: 5
      day: 23
  title: 'C++ at Google: Here Be Dragons'
  type: post-weblog
  URL: http://google-engtools.blogspot.com/2011/05/c-at-google-here-be-dragons.html

- id: cartaxoRapidReviewsSoftware2020
  abstract: >-
    Integrating research evidence into practice is one of the main goals of
    evidence-based software engineering (EBSE). Secondary studies, one of the
    main EBSE products, are intended to summarize the “best” research evidence
    and make them easily consumable by practitioners. However, recent studies
    show that some secondary studies lack connections with software engineering
    practice. In this chapter, we present the concept of Rapid Reviews, which
    are lightweight secondary studies focused on delivering evidence to
    practitioners in a timely manner. Rapid reviews support practitioners in
    their decision-making, and should be conducted bounded to a practical
    problem, inserted into a practical context. Thus, Rapid Reviews can be
    easily integrated in a knowledge/technology transfer initiative. After
    describing the basic concepts, we present the results and experiences of
    conducting two Rapid Reviews. We also provide guidelines to help researchers
    and practitioners who want to conduct Rapid Reviews, and we finally discuss
    topics that may concern the research community about the feasibility of
    Rapid Reviews as an evidence-based method. In conclusion, we believe Rapid
    Reviews might be of interest to researchers and practitioners working on the
    intersection of software engineering research and practice.
  accessed:
    - year: 2023
      month: 10
      day: 27
  author:
    - family: Cartaxo
      given: Bruno
    - family: Pinto
      given: Gustavo
    - family: Soares
      given: Sergio
  citation-key: cartaxoRapidReviewsSoftware2020
  container-title: Contemporary Empirical Methods in Software Engineering
  DOI: 10.1007/978-3-030-32489-6_13
  editor:
    - family: Felderer
      given: Michael
    - family: Travassos
      given: Guilherme Horta
  event-place: Cham
  ISBN: 978-3-030-32489-6
  issued:
    - year: 2020
  language: en
  page: 357-384
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: Rapid Reviews in Software Engineering
  type: chapter
  URL: https://doi.org/10.1007/978-3-030-32489-6_13

- id: cartaxoRoleRapidReviews2018
  abstract: >-
    Context: Recent work on Evidence Based Software Engineering (EBSE) suggests
    that systematic reviews lack connection with Software Engineering (SE)
    practice. In Evidence Based Medicine there is a growing initiative to
    address this kind of problem, in particular through what has been named as
    Rapid Reviews (RRs). They are adaptations of regular systematic reviews made
    to fit practitioners constraints. Goal: Evaluate the perceptions from SE
    practitioners on the use of Rapid Reviews to support decision-making in SE
    practice. Method: We conducted an Action Research to evaluate RRs insertion
    in a real-world software development project. Results: Our results show that
    practitioners are rater positive about Rapid Reviews. They reported to have
    learned new concepts, reduced time and cost of decision-making, improved
    their understanding about the problem their facing, among other benefits.
    Additionally, two months after the introduction of the Rapid Review, in a
    follow up visit, we perceived that the practitioners have indeed adopted the
    evidence provided. Conclusions: Based on the positive results we obtained
    with this study, and the experiences reported in medicine, we believe RRs
    could play an important role towards knowledge transfer and decision-making
    support in SE practice.
  accessed:
    - year: 2023
      month: 10
      day: 26
  author:
    - family: Cartaxo
      given: Bruno
    - family: Pinto
      given: Gustavo
    - family: Soares
      given: Sergio
  citation-key: cartaxoRoleRapidReviews2018
  collection-title: EASE '18
  container-title: >-
    Proceedings of the 22nd International Conference on Evaluation and
    Assessment in Software Engineering 2018
  DOI: 10.1145/3210459.3210462
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6403-4
  issued:
    - year: 2018
      month: 6
      day: 28
  page: 24–34
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    The Role of Rapid Reviews in Supporting Decision-Making in Software
    Engineering Practice
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3210459.3210462

- id: carverSurveyStatePractice2022
  abstract: >-
    Research software is a critical component of contemporary scholarship. Yet,
    most research software is developed and managed in ways that are at odds
    with its long-term sustainability. This paper presents findings from a
    survey of 1,149 researchers, primarily from the United States, about
    sustainability challenges they face in developing and using research
    software. Some of our key findings include a repeated need for more
    opportunities and time for developers of research software to receive
    training. These training needs cross the software lifecycle and various
    types of tools. We also identified the recurring need for better models of
    funding research software and for providing credit to those who develop the
    software so they can advance in their careers. The results of this survey
    will help inform future infrastructure and service support for software
    developers and users, as well as national research policy aimed at
    increasing the sustainability of research software.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Carver
      given: Jeffrey C.
    - family: Weber
      given: Nic
    - family: Ram
      given: Karthik
    - family: Gesing
      given: Sandra
    - family: Katz
      given: Daniel S.
  citation-key: carverSurveyStatePractice2022
  container-title: PeerJ Computer Science
  container-title-short: PeerJ Comput. Sci.
  DOI: 10.7717/peerj-cs.963
  ISSN: 2376-5992
  issued:
    - year: 2022
      month: 5
      day: 5
  language: en
  page: e963
  publisher: PeerJ Inc.
  source: peerj.com
  title: >-
    A survey of the state of the practice for research software in the United
    States
  type: article-journal
  URL: https://peerj.com/articles/cs-963
  volume: '8'

- id: casasPSODSSchedulingEngine2017
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Casas
      given: Israel
    - family: Taheri
      given: Javid
    - family: Ranjan
      given: Rajiv
    - family: Zomaya
      given: Albert Y.
  citation-key: casasPSODSSchedulingEngine2017
  container-title: The Journal of Supercomputing
  container-title-short: J Supercomput
  DOI: 10.1007/s11227-017-1992-z
  ISSN: 0920-8542, 1573-0484
  issue: '9'
  issued:
    - year: 2017
      month: 9
  language: en
  page: 3924-3947
  source: DOI.org (Crossref)
  title: 'PSO-DS: a scheduling engine for scientific workflow managers'
  title-short: PSO-DS
  type: article-journal
  URL: http://link.springer.com/10.1007/s11227-017-1992-z
  volume: '73'

- id: cashenImpactEconomicSanctions
  abstract: >-
    Since the early 1990s, economic sanctions have emerged as a favoured foreign
    policy tool. With the US ramping up measures against North Korea and Russia,
    it seems sanctions are here to stay – despite their many flaws
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Cashen
      given: Emily
  citation-key: cashenImpactEconomicSanctions
  language: en-US
  note: 'interest: 62'
  title: The impact of economic sanctions
  type: article-newspaper
  URL: >-
    https://www.worldfinance.com/special-reports/the-impact-of-economic-sanctions

- id: castagnaGradualTypingNew2019
  abstract: >-
    We define a new, more semantic interpretation of gradual types and use it to
    ``gradualize'' two forms of polymorphism: subtyping polymorphism and
    implicit parametric polymorphism. In particular, we use the new
    interpretation to define three gradual type systems ---Hindley-Milner, with
    subtyping, and with union and intersection types--- in terms of two
    preorders, subtyping and materialization. We define these systems both
    declaratively ---by adding two subsumption-like rules--- which yields
    clearer, more intelligible, and streamlined definitions, and algorithmically
    by reusing existing techniques such as unification and tallying.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Castagna
      given: Giuseppe
    - family: Lanvin
      given: Victor
    - family: Petrucciani
      given: Tommaso
    - family: Siek
      given: Jeremy G.
  citation-key: castagnaGradualTypingNew2019
  container-title: Proceedings of the ACM on Programming Languages
  container-title-short: Proc. ACM Program. Lang.
  DOI: 10.1145/3290329
  issue: POPL
  issued:
    - year: 2019
      month: 1
      day: 2
  note: 'interest: 84'
  page: 16:1–16:32
  source: January 2019
  title: 'Gradual typing: a new perspective'
  title-short: Gradual typing
  type: article-journal
  URL: https://doi.org/10.1145/3290329
  volume: '3'

- id: cespedesLtrace
  abstract: >-
    ltrace intercepts and records dynamic library calls which are called by an
    executed process and the signals received by that process. It can also
    intercept and print the system calls executed by the program.
  author:
    - family: Cespedes
      given: Juan
  citation-key: cespedesLtrace
  title: ltrace
  type: software
  URL: http://ltrace.org/

- id: chalmersAdventuresTechnophilosophyReality2022
  abstract: >-
    When I was ten years old, I discovered computers. My first machine was a
    PDP-​10 mainframe system at the medical center where my father worked. I
    taught myself to write simple programs in the BASIC…
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Chalmers
      given: David J.
  citation-key: chalmersAdventuresTechnophilosophyReality2022
  container-title: Literary  Hub
  issued:
    - year: 2022
      month: 1
      day: 28
  language: en-US
  note: 'interest: 71'
  title: 'Adventures in Technophilosophy: On the Reality of Virtual Worlds'
  title-short: Adventures in Technophilosophy
  type: post-weblog
  URL: >-
    https://lithub.com/adventures-in-technophilosophy-on-the-reality-of-virtual-worlds/

- id: chanExpressivenessBenchmarkingSystemLevel2017
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Chan
      given: Sheung Chi
    - family: Gehani
      given: Ashish
    - family: Cheney
      given: James
    - family: Sohan
      given: Ripduman
    - family: Irshad
      given: Hassaan
  citation-key: chanExpressivenessBenchmarkingSystemLevel2017
  event-title: 9th USENIX Workshop on the Theory and Practice of Provenance (TaPP 2017)
  issued:
    - year: 2017
  language: en
  source: www.usenix.org
  title: Expressiveness Benchmarking for {System-Level} Provenance
  type: paper-conference
  URL: https://www.usenix.org/conference/tapp17/workshop-program/presentation/chan

- id: changRetraction2006
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Chang
      given: Geoffrey
    - family: Roth
      given: Christopher B.
    - family: Reyes
      given: Christopher L.
    - family: Pornillos
      given: Owen
    - family: Chen
      given: Yen-Ju
    - family: Chen
      given: Andy P.
  citation-key: changRetraction2006
  container-title: Science
  container-title-short: Science
  DOI: 10.1126/science.314.5807.1875b
  ISSN: 0036-8075, 1095-9203
  issue: '5807'
  issued:
    - year: 2006
      month: 12
      day: 22
  language: en
  page: 1875-1875
  source: DOI.org (Crossref)
  title: Retraction
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.314.5807.1875b
  volume: '314'

- id: chanProvMarkProvenanceExpressiveness2019a
  abstract: >-
    System level provenance is of widespread interest for applications such as
    security enforcement and information protection. However, testing the
    correctness or completeness of provenance capture tools is challenging and
    currently done manually. In some cases there is not even a clear consensus
    about what behavior is correct. We present an automated tool, ProvMark, that
    uses an existing provenance system as a black box and reliably identifies
    the provenance graph structure recorded for a given activity, by a reduction
    to subgraph isomorphism problems handled by an external solver. ProvMark is
    a beginning step in the much needed area of testing and comparing the
    expressiveness of provenance systems. We demonstrate ProvMark's usefuless in
    comparing three capture systems with different architectures and distinct
    design philosophies.
  accessed:
    - year: 2023
      month: 8
      day: 21
  author:
    - family: Chan
      given: Sheung Chi
    - family: Cheney
      given: James
    - family: Bhatotia
      given: Pramod
    - family: Pasquier
      given: Thomas
    - family: Gehani
      given: Ashish
    - family: Irshad
      given: Hassaan
    - family: Carata
      given: Lucian
    - family: Seltzer
      given: Margo
  citation-key: chanProvMarkProvenanceExpressiveness2019a
  collection-title: Middleware '19
  container-title: Proceedings of the 20th International Middleware Conference
  DOI: 10.1145/3361525.3361552
  event-place: New York, NY, USA
  ISBN: 978-1-4503-7009-7
  issued:
    - year: 2019
      month: 12
      day: 9
  page: 268–279
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'ProvMark: A Provenance Expressiveness Benchmarking System'
  title-short: ProvMark
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3361525.3361552

- id: chanslerArchitectureOpenSource
  author:
    - family: Chansler
      given: Robert
    - family: Bryant
      given: Russell
    - family: Bryant
      given: Roy
    - family: Canino-Koening
      given: Rosangela
    - family: Cesarini
      given: Francesco
    - family: Allman
      given: Eric
    - family: Bostic
      given: Keith
    - family: Brown
      given: Titus
  citation-key: chanslerArchitectureOpenSource
  editor:
    - family: Brown
      given: Amy
    - family: Wilson
      given: Greg
  language: English
  number-of-pages: '434'
  source: Amazon
  title: The Architecture of Open Source Applications
  type: book
  URL: http://aosabook.org/en/index.html

- id: chardApplicationBagItSerializedResearch2019
  abstract: >-
    In this paper we describe our experience adopting the Research Object Bundle
    (RO-Bundle) format with BagIt serialization (BagIt-RO) for the design and
    implementation of "tales" in the Whole Tale platform. A tale is an
    executable research object intended for the dissemination of computational
    scientific findings that captures information needed to facilitate
    understanding, transparency, and re-execution for review and computational
    reproducibility at the time of publication. We describe the Whole Tale
    platform and requirements that led to our adoption of BagIt-RO, specifics of
    our implementation, and discuss migrating to the emerging Research Object
    Crate (RO-Crate) standard.
  author:
    - family: Chard
      given: Kyle
    - family: Gaffney
      given: Niall
    - family: Jones
      given: Matthew B.
    - family: Kowalik
      given: Kacper
    - family: Ludäscher
      given: Bertram
    - family: McPhillips
      given: Timothy
    - family: Nabrzyski
      given: Jarek
    - family: Stodden
      given: Victoria
    - family: Taylor
      given: Ian
    - family: Thelen
      given: Thomas
    - family: Turk
      given: Matthew J.
    - family: Willis
      given: Craig
  citation-key: chardApplicationBagItSerializedResearch2019
  container-title: 2019 15th International Conference on eScience (eScience)
  DOI: 10.1109/eScience.2019.00068
  event-title: 2019 15th International Conference on eScience (eScience)
  issued:
    - year: 2019
      month: 9
  note: 'interest: 90'
  page: 514-521
  source: IEEE Xplore
  title: >-
    Application of BagIt-Serialized Research Object Bundles for Packaging and
    Re-Execution of Computational Analyses
  type: paper-conference

- id: chardFuncXFederatedFunction2020
  abstract: >-
    Exploding data volumes and velocities, new computational methods and
    platforms, and ubiquitous connectivity demand new approaches to computation
    in the sciences. These new approaches must enable computation to be mobile,
    so that, for example, it can occur near data, be triggered by events (e.g.,
    arrival of new data), be offloaded to specialized accelerators, or run
    remotely where resources are available. They also require new design
    approaches in which monolithic applications can be decomposed into smaller
    components, that may in turn be executed separately and on the most suitable
    resources. To address these needs we present funcX---a distributed function
    as a service (FaaS) platform that enables flexible, scalable, and high
    performance remote function execution. funcX's endpoint software can
    transform existing clouds, clusters, and supercomputers into function
    serving systems, while funcX's cloud-hosted service provides transparent,
    secure, and reliable function execution across a federated ecosystem of
    endpoints. We motivate the need for funcX with several scientific case
    studies, present our prototype design and implementation, show optimizations
    that deliver throughput in excess of 1 million functions per second, and
    demonstrate, via experiments on two supercomputers, that funcX can scale to
    more than more than 130 000 concurrent workers.
  accessed:
    - year: 2022
      month: 11
      day: 14
  author:
    - family: Chard
      given: Ryan
    - family: Babuji
      given: Yadu
    - family: Li
      given: Zhuozhao
    - family: Skluzacek
      given: Tyler
    - family: Woodard
      given: Anna
    - family: Blaiszik
      given: Ben
    - family: Foster
      given: Ian
    - family: Chard
      given: Kyle
  citation-key: chardFuncXFederatedFunction2020
  collection-title: HPDC '20
  container-title: >-
    Proceedings of the 29th International Symposium on High-Performance Parallel
    and Distributed Computing
  DOI: 10.1145/3369583.3392683
  event-place: New York, NY, USA
  ISBN: 978-1-4503-7052-3
  issued:
    - year: 2020
      month: 6
      day: 23
  note: 'interest: 95'
  page: 65–76
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'funcX: A Federated Function Serving Fabric for Science'
  title-short: funcX
  type: paper-conference
  URL: https://doi.org/10.1145/3369583.3392683

- id: chazetteExplainabilityNonfunctionalRequirement2020
  abstract: >-
    Software systems are becoming increasingly complex. Their ubiquitous
    presence makes users more dependent on their correctness in many aspects of
    daily life. As a result, there is a growing need to make software systems
    and their decisions more comprehensible, with more transparency in
    software-based decision making. Transparency is therefore becoming
    increasingly important as a non-functional requirement. However, the
    abstract quality aspect of transparency needs to be better understood and
    related to mechanisms that can foster it. The integration of explanations
    into software has often been discussed as a solution to mitigate system
    opacity. Yet, an important first step is to understand user requirements in
    terms of explainable software behavior: Are users really interested in
    software transparency and are explanations considered an appropriate way to
    achieve it? We conducted a survey with 107 end users to assess their opinion
    on the current level of transparency in software systems and what they
    consider to be the main advantages and disadvantages of embedded
    explanations. We assess the relationship between explanations and
    transparency and analyze its potential impact on software quality. As
    explainability has become an important issue, researchers and professionals
    have been discussing how to deal with it in practice. While there are
    differences of opinion on the need for built-in explanations, understanding
    this concept and its impact on software is a key step for requirements
    engineering. Based on our research results and on the study of existing
    literature, we offer recommendations for the elicitation and analysis of
    explainability and discuss strategies for the practice.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Chazette
      given: Larissa
    - family: Schneider
      given: Kurt
  citation-key: chazetteExplainabilityNonfunctionalRequirement2020
  container-title: Requirements Engineering
  container-title-short: Requirements Eng
  DOI: 10.1007/s00766-020-00333-1
  ISSN: 0947-3602, 1432-010X
  issue: '4'
  issued:
    - year: 2020
      month: 12
  language: en
  page: 493-514
  source: DOI.org (Crossref)
  title: >-
    Explainability as a non-functional requirement: challenges and
    recommendations
  title-short: Explainability as a non-functional requirement
  type: article-journal
  URL: https://link.springer.com/10.1007/s00766-020-00333-1
  volume: '25'

- id: chenDynamicSlicingPython2014
  abstract: >-
    Python is widely used for web programming and GUI development. Due to the
    dynamic features of Python, Python programs may contain various unlimited
    errors. Dynamic slicing extracts those statements from a program which
    affect the variables in a slicing criterion with a particular input. Dynamic
    slicing of Python programs is essential for program debugging and fault
    location. In this paper, we propose an approach of dynamic slicing for
    Python programs which combines static analysis and dynamic tracing of the
    Python byte code. It precisely handles the dynamic features of Python, such
    as dynamic typing of variables, heavy usage of first-class objects, and
    dynamic modifications of classes and instances. Finally, we evaluate our
    approach on several Python programs. Experimental results show that the
    whole dynamic slicing for each subject program spends at most about 13
    seconds on the average and costs at most 7.58 mb memory space overhead.
    Furthermore, the average slice ratio of Python source code ranges from 9.26%
    to 59.42%. According to it, our dynamic slicing approach can be effectively
    and efficiently performed. To the best of our knowledge, it is the first one
    of dynamic slicing for Python programs.
  author:
    - family: Chen
      given: Zhifei
    - family: Chen
      given: Lin
    - family: Zhou
      given: Yuming
    - family: Xu
      given: Zhaogui
    - family: Chu
      given: William C.
    - family: Xu
      given: Baowen
  citation-key: chenDynamicSlicingPython2014
  container-title: 2014 IEEE 38th Annual Computer Software and Applications Conference
  DOI: 10.1109/COMPSAC.2014.30
  event-title: 2014 IEEE 38th Annual Computer Software and Applications Conference
  ISSN: 0730-3157
  issued:
    - year: 2014
      month: 7
  note: 'interest: 85'
  page: 219-228
  source: IEEE Xplore
  title: Dynamic Slicing of Python Programs
  type: paper-conference

- id: chengCompressionLowRank2005
  abstract: >-
    Randomized sampling has recently been proven a highly efficient technique
    for computing approximate factorizations of matrices that have low numerical
    rank. This paper describes an extension of such techniques to a wider class
    of matrices that are not themselves rank-deficient but have off-diagonal
    blocks that are; specifically, the class of so-called hierarchically
    semiseparable (HSS) matrices. HSS matrices arise frequently in numerical
    analysis and signal processing, particularly in the construction of fast
    methods for solving differential and integral equations numerically. The HSS
    structure admits algebraic operations (matrix-vector multiplications, matrix
    factorizations, matrix inversion, etc.) to be performed very rapidly, but
    only once the HSS representation of the matrix has been constructed. How to
    rapidly compute this representation in the first place is much less well
    understood. The present paper demonstrates that if an $N\times N$ matrix can
    be applied to a vector in $O(N)$ time, and if individual entries of the
    matrix can be computed rapidly, then provided that an HSS representation of
    the matrix exists, it can be constructed in $O(N\,k^{2})$ operations, where
    k is an upper bound for the numerical rank of the off-diagonal blocks. The
    point is that when legacy codes (based on, e.g., the fast multipole method)
    can be used for the fast matrix-vector multiply, the proposed algorithm can
    be used to obtain the HSS representation of the matrix, and then
    well-established techniques for HSS matrices can be used to invert or factor
    the matrix.
  accessed:
    - year: 2024
      month: 2
      day: 5
  author:
    - family: Cheng
      given: H.
    - family: Gimbutas
      given: Z.
    - family: Martinsson
      given: P. G.
    - family: Rokhlin
      given: V.
  citation-key: chengCompressionLowRank2005
  container-title: SIAM Journal on Scientific Computing
  container-title-short: SIAM J. Sci. Comput.
  DOI: 10.1137/030602678
  ISSN: 1064-8275
  issue: '4'
  issued:
    - year: 2005
      month: 1
  page: 1389-1404
  publisher: Society for Industrial and Applied Mathematics
  source: epubs.siam.org (Atypon)
  title: On the Compression of Low Rank Matrices
  type: article-journal
  URL: https://epubs.siam.org/doi/10.1137/030602678
  volume: '26'

- id: chengWhatImprovesDeveloper2022
  abstract: >-
    Understanding what affects software developer productivity can help
    organizations choose wise investments in their technical and social
    environment. But the research literature either focuses on what correlates
    with developer productivity in ecologically valid settings or focuses on
    what causes developer productivity in highly constrained settings. In this
    paper, we bridge the gap by studying software developers at Google through
    two analyses. In the first analysis, we use panel data with 39 productivity
    factors, finding that code quality, technical debt, infrastructure tools and
    support, team communication, goals and priorities, and organizational change
    and process are all causally linked to self-reported developer productivity.
    In the second analysis, we use a lagged panel analysis to strengthen our
    causal claims. We find that increases in perceived code quality tend to be
    followed by increased perceived developer productivity, but not vice versa,
    providing the strongest evidence to date that code quality affects
    individual developer productivity.
  accessed:
    - year: 2022
      month: 11
      day: 14
  author:
    - family: Cheng
      given: Lan
    - family: Murphy-Hill
      given: Emerson
    - family: Canning
      given: Mark
    - family: Jaspan
      given: Ciera
    - family: Green
      given: Collin
    - family: Knight
      given: Andrea
    - family: Zhang
      given: Nan
    - family: Kammer
      given: Elizabeth
  citation-key: chengWhatImprovesDeveloper2022
  collection-title: ESEC/FSE 2022
  container-title: >-
    Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3540250.3558940
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9413-0
  issued:
    - year: 2022
      month: 11
      day: 9
  note: 'interest: 70'
  page: 1302–1313
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: What improves developer productivity at google? code quality
  title-short: What improves developer productivity at google?
  type: paper-conference
  URL: https://doi.org/10.1145/3540250.3558940

- id: chenHybridInformationFlow2014
  abstract: >-
    Python is widely used to create and manage complex, database-driven
    websites. However, due to dynamic features such as dynamic typing of
    variables, Python programs pose a serious security risk to web applications.
    Most security vulnerabilities result from the fact that unsafe data input
    reaches security-sensitive operations. To address this problem, information
    flow analysis for Python programs is proposed to enforce this property.
    Information flow can capture the fact that a particular value affects
    another value in the program. In this paper, we present a novel approach for
    analyzing information flow in Python byte code which is a low-level language
    and is more widely broadcast. Our approach performs a hybrid of static and
    dynamic control/data flow analysis. Static analysis is used to study
    implicit flow, while dynamic analysis efficiently tracks execution
    information and determines definition-use pair. To the best of our
    knowledge, it is the first one for Python byte code.
  author:
    - family: Chen
      given: Zhifei
    - family: Chen
      given: Lin
    - family: Xu
      given: Baowen
  citation-key: chenHybridInformationFlow2014
  container-title: 2014 11th Web Information System and Application Conference
  DOI: 10.1109/WISA.2014.26
  event-title: 2014 11th Web Information System and Application Conference
  issued:
    - year: 2014
      month: 9
  note: 'interest: 85'
  page: 95-100
  source: IEEE Xplore
  title: Hybrid Information Flow Analysis for Python Bytecode
  type: paper-conference

- id: chenImplementationEvaluationProtocol2008
  abstract: >-
    The provenance of a particular data item is the process that led to that
    piece of data. Previous work has enabled the creation of detailed
    representation of past executions for determining provenance, termed process
    documentation. However, current solutions to recording process documentation
    assume a failure free environment. Failures result in process documentation
    not being recorded, thereby causing the loss of evidence that a process
    occurred. We have designed F-PReP, a protocol to guarantee the recording of
    process documentation in the presence of failures. This paper discusses its
    implementation and evaluates its performance. The result reveals that it
    introduces acceptable overhead.
  author:
    - family: Chen
      given: Zheng
    - family: Moreau
      given: Luc
  citation-key: chenImplementationEvaluationProtocol2008
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-540-89965-5_11
  editor:
    - family: Freire
      given: Juliana
    - family: Koop
      given: David
    - family: Moreau
      given: Luc
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-89965-5
  issued:
    - year: 2008
  language: en
  note: 'interest: 95'
  page: 92-105
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: >-
    Implementation and Evaluation of a Protocol for Recording Process
    Documentation in the Presence of Failures
  type: paper-conference

- id: chenMetamorphicTestingNew2020
  abstract: >-
    In software testing, a set of test cases is constructed according to some
    predefined selection criteria. The software is then examined against these
    test cases. Three interesting observations have been made on the current
    artifacts of software testing. Firstly, an error-revealing test case is
    considered useful while a successful test case which does not reveal
    software errors is usually not further investigated. Whether these
    successful test cases still contain useful information for revealing
    software errors has not been properly studied. Secondly, no matter how
    extensive the testing has been conducted in the development phase, errors
    may still exist in the software [5]. These errors, if left undetected, may
    eventually cause damage to the production system. The study of techniques
    for uncovering software errors in the production phase is seldom addressed
    in the literature. Thirdly, as indicated by Weyuker in [6], the availability
    of test oracles is pragmatically unattainable in most situations. However,
    the availability of test oracles is generally assumed in conventional
    software testing techniques. In this paper, we propose a novel test case
    selection technique that derives new test cases from the successful ones.
    The selection aims at revealing software errors that are possibly left
    undetected in successful test cases which may be generated using some
    existing strategies. As such, the proposed technique augments the
    effectiveness of existing test selection strategies. The technique also
    helps uncover software errors in the production phase and can be used in the
    absence of test oracles.
  accessed:
    - year: 2022
      month: 10
      day: 11
  author:
    - family: Chen
      given: T. Y.
    - family: Cheung
      given: S. C.
    - family: Yiu
      given: S. M.
  citation-key: chenMetamorphicTestingNew2020
  DOI: 10.48550/arXiv.2002.12543
  issued:
    - year: 2020
      month: 2
      day: 27
  note: 'interest: 85'
  number: arXiv:2002.12543
  publisher: arXiv
  source: arXiv.org
  title: 'Metamorphic Testing: A New Approach for Generating Next Test Cases'
  title-short: Metamorphic Testing
  type: article
  URL: http://arxiv.org/abs/2002.12543

- id: chenOpenNotEnough2019
  abstract: >-
    The solutions adopted by the high-energy physics community to foster
    reproducible research are examples of best practices that could be embraced
    more widely. This first experience suggests that reproducibility requires
    going beyond openness.
  accessed:
    - year: 2022
      month: 9
      day: 27
  author:
    - family: Chen
      given: Xiaoli
    - family: Dallmeier-Tiessen
      given: Sünje
    - family: Dasler
      given: Robin
    - family: Feger
      given: Sebastian
    - family: Fokianos
      given: Pamfilos
    - family: Gonzalez
      given: Jose Benito
    - family: Hirvonsalo
      given: Harri
    - family: Kousidis
      given: Dinos
    - family: Lavasa
      given: Artemis
    - family: Mele
      given: Salvatore
    - family: Rodriguez
      given: Diego Rodriguez
    - family: Šimko
      given: Tibor
    - family: Smith
      given: Tim
    - family: Trisovic
      given: Ana
    - family: Trzcinska
      given: Anna
    - family: Tsanaktsidis
      given: Ioannis
    - family: Zimmermann
      given: Markus
    - family: Cranmer
      given: Kyle
    - family: Heinrich
      given: Lukas
    - family: Watts
      given: Gordon
    - family: Hildreth
      given: Michael
    - family: Lloret Iglesias
      given: Lara
    - family: Lassila-Perini
      given: Kati
    - family: Neubert
      given: Sebastian
  citation-key: chenOpenNotEnough2019
  container-title: Nature Physics
  container-title-short: Nature Phys
  DOI: 10.1038/s41567-018-0342-2
  ISSN: 1745-2481
  issue: '2'
  issued:
    - year: 2019
      month: 2
  language: en
  license: 2018 The Author(s)
  note: 'interest: 84'
  number: '2'
  page: 113-119
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Open is not enough
  type: article-journal
  URL: https://www.nature.com/articles/s41567-018-0342-2
  volume: '15'

- id: chenRobustBenchmarkingNoisy2016
  abstract: >-
    We propose a benchmarking strategy that is robust in the presence of timer
    error, OS jitter and other environmental fluctuations, and is insensitive to
    the highly nonideal statistics produced by timing measurements. We construct
    a model that explains how these strongly nonideal statistics can arise from
    environmental fluctuations, and also justifies our proposed strategy. We
    implement this strategy in the BenchmarkTools Julia package, where it is
    used in production continuous integration (CI) pipelines for developing the
    Julia language and its ecosystem.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Chen
      given: Jiahao
    - family: Revels
      given: Jarrett
  citation-key: chenRobustBenchmarkingNoisy2016
  container-title: arXiv:1608.04295 [cs]
  issued:
    - year: 2016
      month: 8
      day: 15
  note: 'interest: 90'
  source: arXiv.org
  title: Robust benchmarking in noisy environments
  type: article-journal
  URL: http://arxiv.org/abs/1608.04295

- id: chirigatiReproZipComputationalReproducibility2016
  abstract: >-
    We present ReproZip, the recommended packaging tool for the SIGMOD
    Reproducibility Review. ReproZip was designed to simplify the process of
    making an existing computational experiment reproducible across platforms,
    even when the experiment was put together without reproducibility in mind.
    The tool creates a self-contained package for an experiment by automatically
    tracking and identifying all its required dependencies. The researcher can
    share the package with others, who can then use ReproZip to unpack the
    experiment, reproduce the findings on their favorite operating system, as
    well as modify the original experiment for reuse in new research, all with
    little effort. The demo will consist of examples of non-trivial experiments,
    showing how these can be packed in a Linux machine and reproduced on
    different machines and operating systems. Demo visitors will also be able to
    pack and reproduce their own experiments.
  accessed:
    - year: 2023
      month: 10
      day: 16
  author:
    - family: Chirigati
      given: Fernando
    - family: Rampin
      given: Rémi
    - family: Shasha
      given: Dennis
    - family: Freire
      given: Juliana
  citation-key: chirigatiReproZipComputationalReproducibility2016
  collection-title: SIGMOD '16
  container-title: Proceedings of the 2016 International Conference on Management of Data
  DOI: 10.1145/2882903.2899401
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3531-7
  issued:
    - year: 2016
      month: 6
      day: 26
  page: 2085–2088
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'ReproZip: Computational Reproducibility With Ease'
  title-short: ReproZip
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2882903.2899401

- id: christensenEraseYourDarlings
  accessed:
    - year: 2022
      month: 8
      day: 22
  author:
    - family: Christensen
      given: Graham
  citation-key: christensenEraseYourDarlings
  title: >-
    Erase your darlings: immutable infrastructure for mutable systems - Graham
    Christensen
  type: post-weblog
  URL: https://grahamc.com/blog/erase-your-darlings

- id: christensenZFSDatasetsNixOS
  accessed:
    - year: 2022
      month: 8
      day: 22
  author:
    - family: Christensen
      given: Graham
  citation-key: christensenZFSDatasetsNixOS
  title: ZFS Datasets for NixOS - Graham Christensen
  type: post-weblog
  URL: https://grahamc.com/blog/nixos-on-zfs

- id: churchyardSingularTheirJane
  accessed:
    - year: 2024
      month: 3
      day: 30
  author:
    - family: Churchyard
      given: Harry
  citation-key: churchyardSingularTheirJane
  title: 'Singular "their" in Jane Austen and elsewhere: Anti-pedantry page'
  type: webpage
  URL: https://www.pemberley.com/janeinfo/austheir.html#X1aii

- id: ciancariniEvaluatingCitationFunctions2014
  abstract: >-
    Networks of citations are a key tool for referencing, disseminating and
    evaluating research results. The task of characterising the functional role
    of citations in scientific literature is very difficult, not only for
    software agents but for humans, too. The main problem is that the mental
    models of different annotators hardly ever converge to a single shared
    opinion. The goal of this paper is to investigate how an existing reference
    model for classifying citations, namely CiTO (Citation Typing Ontology), is
    interpreted and used by annotators of scientific literature. We present an
    experiment capturing the cognitive processes behind subjects’ decisions in
    annotating papers with CiTO, and we provide initial ideas to refine future
    releases of CiTO.
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Ciancarini
      given: Paolo
    - family: Di Iorio
      given: Angelo
    - family: Nuzzolese
      given: Andrea Giovanni
    - family: Peroni
      given: Silvio
    - family: Vitali
      given: Fabio
  citation-key: ciancariniEvaluatingCitationFunctions2014
  collection-editor:
    - family: Hutchison
      given: David
    - family: Kanade
      given: Takeo
    - family: Kittler
      given: Josef
    - family: Kleinberg
      given: Jon M.
    - family: Kobsa
      given: Alfred
    - family: Mattern
      given: Friedemann
    - family: Mitchell
      given: John C.
    - family: Naor
      given: Moni
    - family: Nierstrasz
      given: Oscar
    - family: Pandu Rangan
      given: C.
    - family: Steffen
      given: Bernhard
    - family: Terzopoulos
      given: Demetri
    - family: Tygar
      given: Doug
    - family: Weikum
      given: Gerhard
  container-title: 'The Semantic Web: Trends and Challenges'
  DOI: 10.1007/978-3-319-07443-6_39
  editor:
    - family: Presutti
      given: Valentina
    - family: Amato
      given: Claudia
      non-dropping-particle: d’
    - family: Gandon
      given: Fabien
    - family: Aquin
      given: Mathieu
      non-dropping-particle: d’
    - family: Staab
      given: Steffen
    - family: Tordai
      given: Anna
  event-place: Cham
  ISBN: 978-3-319-07442-9 978-3-319-07443-6
  issued:
    - year: 2014
  language: en
  page: 580-594
  publisher: Springer International Publishing
  publisher-place: Cham
  source: DOI.org (Crossref)
  title: 'Evaluating Citation Functions in CiTO: Cognitive Issues'
  title-short: Evaluating Citation Functions in CiTO
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-319-07443-6_39
  volume: '8465'

- id: ciechanowskiCamerasLensesBartosz
  abstract: Interactive article explaining how cameras and lenses work.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Ciechanowski
      given: Bartosz
  citation-key: ciechanowskiCamerasLensesBartosz
  language: en
  note: 'interest: 95'
  title: Cameras and Lenses – Bartosz Ciechanowski
  type: post-weblog
  URL: https://ciechanow.ski/cameras-and-lenses/

- id: ciepielaGridSpace2VirtualLaboratory2012
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Ciepiela
      given: Eryk
    - family: Zaraska
      given: Leszek
    - family: Sulka
      given: Grzegorz D.
  citation-key: ciepielaGridSpace2VirtualLaboratory2012
  container-title: Building a National Distributed e-Infrastructure–PL-Grid
  DOI: 10.1007/978-3-642-28267-6_19
  editor:
    - family: Bubak
      given: Marian
    - family: Szepieniec
      given: Tomasz
    - family: Wiatr
      given: Kazimierz
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-28266-9 978-3-642-28267-6
  issued:
    - year: 2012
  page: 240-251
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: >-
    GridSpace2 Virtual Laboratory Case Study: Implementation of Algorithms for
    Quantitative Analysis of Grain Morphology in Self-assembled Hexagonal
    Lattices According to the Hillebrand Method
  title-short: GridSpace2 Virtual Laboratory Case Study
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-642-28267-6_19
  volume: '7136'

- id: cimpanuSupercomputersHackedEurope2020
  abstract: >-
    Confirmed infections have been reported in the UK, Germany, and Switzerland.
    Another suspected infection was reported in Spain.
  accessed:
    - year: 2022
      month: 5
      day: 23
  author:
    - family: Cimpanu
      given: Catalin
  citation-key: cimpanuSupercomputersHackedEurope2020
  container-title: ZDNet
  issued:
    - year: 2020
      month: 5
      day: 16
  language: en
  title: Supercomputers hacked across Europe to mine cryptocurrency
  type: article-newspaper
  URL: >-
    https://www.zdnet.com/article/supercomputers-hacked-across-europe-to-mine-cryptocurrency/

- id: cirilloDeclineViolentConflicts2016
  abstract: >-
    We propose a methodology to look at violence in particular, and other
    aspects of quantitative historiography in general, in a way compatible with
    statistical inference, which needs to accommodate the fat-tailedness of the
    data and the unreliability of the reports of conflicts. We investigate the
    theses of “long peace” and drop in violence and find that these are
    statistically invalid and resulting from flawed and naive methodologies,
    incompatible with fat tails and non-robust to minor changes in data
    formatting and methodologies. There is no statistical basis to claim that
    “times are different” owing to the long inter-arrival times between
    conflicts; there is no basis to discuss any “trend”, and no scientific basis
    for narratives about change in risk. We describe naive empiricism under fat
    tails. We also establish that violence has a “true mean” that is
    underestimated in the track record. This is a historiographical adaptation
    of the results in Cirillo and Taleb (2016).
  accessed:
    - year: 2022
      month: 9
      day: 9
  author:
    - family: Cirillo
      given: Pasquale
    - family: Taleb
      given: Nassim Nicholas
  citation-key: cirilloDeclineViolentConflicts2016
  DOI: 10.2139/ssrn.2876315
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2016
      month: 11
      day: 27
  language: en
  number: '2876315'
  publisher-place: Rochester, NY
  source: Social Science Research Network
  title: 'The Decline of Violent Conflicts: What Do the Data Really Say?'
  title-short: The Decline of Violent Conflicts
  type: article
  URL: https://papers.ssrn.com/abstract=2876315

- id: citoUsingDockerContainers2016
  abstract: >-
    The ability to replicate and reproduce scientific results has become an
    increasingly important topic for many academic disciplines. In computer
    science and, more specifically, software and web engineering, contributions
    of scientific work rely on developed algorithms, tools and prototypes,
    quantitative evaluations, and other computational analyses. Published code
    and data come with many undocumented assumptions, dependencies, and
    configurations that are internal knowledge and make reproducibility hard to
    achieve. This tutorial presents how Docker containers can overcome these
    issues and aid the reproducibility of research artifacts in software and web
    engineering and discusses their applications in the field.
  author:
    - family: Cito
      given: Jürgen
    - family: Ferme
      given: Vincenzo
    - family: Gall
      given: Harald C.
  citation-key: citoUsingDockerContainers2016
  collection-title: Lecture Notes in Computer Science
  container-title: Web Engineering
  DOI: 10.1007/978-3-319-38791-8_58
  editor:
    - family: Bozzon
      given: Alessandro
    - family: Cudre-Maroux
      given: Philippe
    - family: Pautasso
      given: Cesare
  event-place: Cham
  ISBN: 978-3-319-38791-8
  issued:
    - year: 2016
  language: en
  note: 'interest: 94'
  page: 609-612
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: >-
    Using Docker Containers to Improve Reproducibility in Software and Web
    Engineering Research
  type: paper-conference

- id: claerboutElectronicDocumentsGive1992
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Claerbout
      given: Jon F.
    - family: Karrenbach
      given: Martin
  citation-key: claerboutElectronicDocumentsGive1992
  container-title: SEG Technical Program Expanded Abstracts 1992
  DOI: 10.1190/1.1822162
  event-title: SEG Technical Program Expanded Abstracts 1992
  issued:
    - year: 1992
      month: 1
  language: en
  page: 601-604
  publisher: Society of Exploration Geophysicists
  source: DOI.org (Crossref)
  title: Electronic documents give reproducible research a new meaning
  type: paper-conference
  URL: http://library.seg.org/doi/abs/10.1190/1.1822162

- id: clarkAnalyzingAccessibilityWikipedia2017
  abstract: >-
    This study, conducted by the Internet Monitor project at the Berkman Klein
    Center for Internet & Society, analyzes the scope of government-sponsored
    censorship of Wikimedia sites around the world. The study finds that, as of
    June 2016, China was likely censoring the Chinese language Wikipedia
    project, and Thailand and Uzbekistan were likely interfering intermittently
    with specific language projects of Wikipedia as well. However, considering
    the widespread use of filtering technologies and the vast coverage of
    Wikipedia, our study finds that, as of June 2016, there was relatively
    little censorship of Wikipedia globally. In fact, our study finds there was
    less censorship in June 2016 than before Wikipedia’s transition to
    HTTPS-only content delivery in June 2015. HTTPS prevents censors from seeing
    which page a user is viewing, which means censors must choose between
    blocking the entire site and allowing access to all articles. This finding
    suggests that the shift to HTTPS has been a good one in terms of ensuring
    accessibility to knowledge. The study identifies and documents the blocking
    of Wikipedia content using two complementary data collection and analysis
    strategies: a client-side system that collects data from the perspective of
    users around the globe and a server-side tool to analyze traffic coming in
    to Wikipedia servers. Both client- and server-side methods detected events
    that we consider likely related to censorship, in addition to a large number
    of suspicious events that remain unexplained. The report features results of
    our data analysis and insights into the state of access to Wikipedia content
    in 15 select countries.
  accessed:
    - year: 2022
      month: 4
      day: 13
  author:
    - family: Clark
      given: Justin
    - family: Faris
      given: Robert
    - family: Heacock Jones
      given: Rebekah
  citation-key: clarkAnalyzingAccessibilityWikipedia2017
  DOI: 10.2139/ssrn.2951312
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2017
      month: 5
      day: 1
  language: en
  number: '2951312'
  publisher: Social Science Research Network
  publisher-place: Rochester, NY
  source: papers.ssrn.com
  title: Analyzing Accessibility of Wikipedia Projects Around the World
  type: report
  URL: https://papers.ssrn.com/abstract=2951312

- id: clarkMicropublicationsSemanticModel2014
  abstract: >-
    Scientific publications are documentary representations of defeasible
    arguments, supported by data and repeatable methods. They are the essential
    mediating artifacts in the ecosystem of scientific communications. The
    institutional “goal” of science is publishing results. The linear document
    publication format, dating from 1665, has survived transition to the Web.
  accessed:
    - year: 2023
      month: 11
      day: 9
  author:
    - family: Clark
      given: Tim
    - family: Ciccarese
      given: Paolo N.
    - family: Goble
      given: Carole A.
  citation-key: clarkMicropublicationsSemanticModel2014
  container-title: Journal of Biomedical Semantics
  container-title-short: Journal of Biomedical Semantics
  DOI: 10.1186/2041-1480-5-28
  ISSN: 2041-1480
  issue: '1'
  issued:
    - year: 2014
      month: 7
      day: 4
  page: '28'
  source: BioMed Central
  title: >-
    Micropublications: a semantic model for claims, evidence, arguments and
    annotations in biomedical communications
  title-short: Micropublications
  type: article-journal
  URL: https://doi.org/10.1186/2041-1480-5-28
  volume: '5'

- id: CLI11CommandLine2022
  abstract: >-
    CLI11 is a command line parser for C++11 and beyond that provides a rich
    feature set with a simple and intuitive interface.
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: CLI11CommandLine2022
  genre: C++
  issued:
    - year: 2022
      month: 9
      day: 6
  original-date:
    - year: 2017
      month: 1
      day: 25
  publisher: CLIUtils
  source: GitHub
  title: 'CLI11: Command line parser for C++11'
  title-short: CLI11
  type: software
  URL: >-
    https://github.com/CLIUtils/CLI11/blob/1a26afab049bb75c0523a754d62b961439248d44/README.md

- id: cohenReproducibilityNaturalLanguage2016
  abstract: >-
    There is currently a crisis in science related to highly publicized failures
    to reproduce large numbers of published studies. The current work proposes,
    by way of case studies, a methodology for moving the study of
    reproducibility in computational work to a full stage beyond that of earlier
    work. Specifically, it presents a case study in attempting to reproduce the
    reports of two R libraries for doing text mining of the PubMed/MEDLINE
    repository of scientific publications. The main findings are that a rational
    paradigm for reproduction of natural language processing papers can be
    established; the advertised functionality was difficult, but not impossible,
    to reproduce; and reproducibility studies can produce additional insights
    into the functioning of the published system. Additionally, the work on
    reproducibility lead to the production of novel user-centered documentation
    that has been accessed 260 times since its publication—an average of once a
    day per library.
  accessed:
    - year: 2022
      month: 9
      day: 20
  author:
    - family: Cohen
      given: K. Bretonnel
    - family: Xia
      given: Jingbo
    - family: Roeder
      given: Christophe
    - family: Hunter
      given: Lawrence E.
  citation-key: cohenReproducibilityNaturalLanguage2016
  container-title: >-
    LREC ... International Conference on Language Resources & Evaluation :
    [proceedings]. International Conference on Language Resources and Evaluation
  container-title-short: LREC Int Conf Lang Resour Eval
  issue: W23
  issued:
    - year: 2016
      month: 5
  note: 'interest: 87'
  page: 6-12
  PMCID: PMC5860830
  PMID: '29568821'
  source: PubMed Central
  title: >-
    Reproducibility in Natural Language Processing: A Case Study of Two R
    Libraries for Mining PubMed/MEDLINE
  title-short: Reproducibility in Natural Language Processing
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5860830/
  volume: '2016'

- id: colellaPiecewiseParabolicMethod1984
  abstract: >-
    We present the piecewise parabolic method, a higher-order extension of
    Godunov's method. There are several new features of this method which
    distinguish it from other higher-order Godunov-type methods. We use a
    higher-order spatial interpolation than previously used, which allows for a
    steeper representation of discontinuities, particularly contact
    discontinuities. We introduce a simpler and more robust algorithm for
    calculating the nonlinear wave interactions used to compute fluxes. Finally,
    we recognize the need for additional dissipation in any higher-order Godunov
    method of this type, and introduce it in such a way so as not to degrade the
    quality of the results.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Colella
      given: P.
    - family: Woodward
      given: Paul R.
  citation-key: colellaPiecewiseParabolicMethod1984
  container-title: Journal of Computational Physics
  DOI: 10.1016/0021-9991(84)90143-8
  ISSN: 0021-9991
  issued:
    - year: 1984
      month: 9
      day: 1
  note: 'ADS Bibcode: 1984JCoPh..54..174C'
  page: 174-201
  source: NASA ADS
  title: The Piecewise Parabolic Method (PPM) for Gas-Dynamical Simulations
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1984JCoPh..54..174C
  volume: '54'

- id: colemanWfCommonsFrameworkEnabling2022
  abstract: >-
    Scientific workflows are a cornerstone of modern scientific computing. They
    are used to describe complex computational applications that require
    efficient and robust management of large volumes of data, which are
    typically stored/processed on heterogeneous, distributed resources. The
    workflow research and development community has employed a number of methods
    for the quantitative evaluation of existing and novel workflow algorithms
    and systems. In particular, a common approach is to simulate workflow
    executions. In previous works, we have presented a collection of tools that
    have been adopted by the community for conducting workflow research. Despite
    their popularity, they suffer from several shortcomings that prevent easy
    adoption, maintenance, and consistency with the evolving structures and
    computational requirements of production workflows. In this work, we present
    WfCommons , a framework that provides a collection of tools for analyzing
    workflow executions, for producing generators of synthetic workflows, and
    for simulating workflow executions. We demonstrate the realism of the
    generated synthetic workflows by comparing their simulated executions to
    real workflow executions. We also contrast these results with results
    obtained when using the previously available collection of tools. We find
    that the workflow generators that are automatically constructed by our
    framework not only generate representative same-scale workflows (i.e., with
    structures and task characteristics distributions that resemble those
    observed in real-world workflows), but also do so at scales larger than that
    of available real-world workflows. Finally, we conduct a case study to
    demonstrate the usefulness of our framework for estimating the energy
    consumption of large-scale workflow executions.
  accessed:
    - year: 2022
      month: 10
      day: 31
  author:
    - family: Coleman
      given: Tainã
    - family: Casanova
      given: Henri
    - family: Pottier
      given: Loïc
    - family: Kaushik
      given: Manav
    - family: Deelman
      given: Ewa
    - family: Ferreira da Silva
      given: Rafael
  citation-key: colemanWfCommonsFrameworkEnabling2022
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2021.09.043
  ISSN: 0167-739X
  issued:
    - year: 2022
      month: 3
      day: 1
  language: en
  page: 16-27
  source: ScienceDirect
  title: >-
    WfCommons: A framework for enabling scientific workflow research and
    development
  title-short: WfCommons
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167739X21003897
  volume: '128'

- id: collbergRepeatabilityBenefactionComputer2015
  abstract: >-
    We describe a study into the extent to which Computer Systems researchers
    share their code and data and the extent to which such code builds. Starting
    with 601 papers from ACM conferences and journals, we examine 402 papers
    whose results were backed by code. For 32.3% of these papers we were able to
    obtain the code and build it within 30 minutes; for 48.3% of the

    papers we managed to build the code, but it may have required extra effort;
    for 54.0% of the papers either we managed to build the code or the authors
    stated the code would build with reasonable effort. We also propose a novel
    sharing specification scheme that requires researchers to specify the level
    of sharing that reviewers and readers can assume from a paper.
  author:
    - family: Collberg
      given: Christian
    - family: Proebsting
      given: Todd
    - family: Warren
      given: Alex M
  citation-key: collbergRepeatabilityBenefactionComputer2015
  issued:
    - year: 2015
      month: 2
      day: 27
  number: 14-04
  publisher: University of Arizona
  title: >-
    Repeatability and Benefaction in Computer Systems Research—A Study and a
    Modest Proposal
  type: report
  URL: http://repeatability.cs.arizona.edu/v2/RepeatabilityTR.pdf

- id: collbergRepeatabilityComputerSystems2016
  abstract: >-
    To encourage repeatable research, fund repeatability engineering and reward
    commitments to sharing research artifacts.
  accessed:
    - year: 2022
      month: 5
      day: 27
  author:
    - family: Collberg
      given: Christian
    - family: Proebsting
      given: Todd A.
  citation-key: collbergRepeatabilityComputerSystems2016
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/2812803
  ISSN: 0001-0782, 1557-7317
  issue: '3'
  issued:
    - year: 2016
      month: 2
      day: 25
  language: en
  page: 62-69
  source: DOI.org (Crossref)
  title: Repeatability in computer systems research
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2812803
  volume: '59'

- id: collbergSharingSpecificationsRepeatability2016
  abstract: >-
    We describe a study into the extent to which Computer Systems researchers
    share their code and data. Starting with 601 papers from ACM conferences and
    journals, we examine the papers whose results were backed by code to see for
    what fraction of these we would be able to obtain and build the code. Based
    on the results of this study, we propose a novel sharing specification
    scheme that requires researchers to specify the level of sharing that
    reviewers and readers can assume from a paper.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Collberg
      given: Christian S.
    - family: Proebsting
      given: Todd A.
  citation-key: collbergSharingSpecificationsRepeatability2016
  event-place: University of Arizona
  event-title: 'Data Reproducibility: Integrity and Transparency'
  issued:
    - year: 2016
      month: 10
      day: 27
  language: en_US
  publisher-place: University of Arizona
  title: Sharing Specifications or Repeatability in Computer Systems Research
  type: speech
  URL: https://repository.arizona.edu/handle/10150/621552

- id: collinsCosmologicalAdaptiveMesh2010
  abstract: >-
    In this work, we present EnzoMHD, the extension of the cosmological code
    Enzo to include the effects of magnetic fields through the ideal
    magnetohydrodynamics approximation. We use a higher order Godunov method for
    the computation of interface fluxes. We use two constrained transport
    methods to compute the electric field from those interface fluxes, which
    simultaneously advances the induction equation and maintains the divergence
    of the magnetic field. A second-order divergence-free reconstruction
    technique is used to interpolate the magnetic fields in the block-structured
    adaptive mesh refinement framework already extant in Enzo. This
    reconstruction also preserves the divergence of the magnetic field to
    machine precision. We use operator splitting to include gravity and
    cosmological expansion. We then present a series of cosmological and
    non-cosmological test problems to demonstrate the quality of solution
    resulting from this combination of solvers.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Collins
      given: David C.
    - family: Xu
      given: Hao
    - family: Norman
      given: Michael L.
    - family: Li
      given: Hui
    - family: Li
      given: Shengtai
  citation-key: collinsCosmologicalAdaptiveMesh2010
  container-title: The Astrophysical Journal Supplement Series
  DOI: 10.1088/0067-0049/186/2/308
  ISSN: 0067-0049
  issued:
    - year: 2010
      month: 2
      day: 1
  note: 'ADS Bibcode: 2010ApJS..186..308C'
  page: 308-333
  source: NASA ADS
  title: Cosmological Adaptive Mesh Refinement Magnetohydrodynamics with Enzo
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/2010ApJS..186..308C
  volume: '186'

- id: >-
    committeeonreproducibilityandreplicabilityinscienceReproducibilityReplicabilityScience2019
  accessed:
    - year: 2023
      month: 1
      day: 19
  citation-key: >-
    committeeonreproducibilityandreplicabilityinscienceReproducibilityReplicabilityScience2019
  contributor:
    - literal: Committee on Reproducibility and Replicability in Science
    - literal: Board on Behavioral, Cognitive, and Sensory Sciences
    - literal: Committee on National Statistics
    - literal: Division of Behavioral and Social Sciences and Education
    - literal: Nuclear and Radiation Studies Board
    - literal: Division on Earth and Life Studies
    - literal: Board on Mathematical Sciences and Analytics
    - literal: Committee on Applied and Theoretical Statistics
    - literal: Division on Engineering and Physical Sciences
    - literal: Board on Research Data and Information
    - literal: Committee on Science, Engineering, Medicine, and Public Policy
    - literal: Policy and Global Affairs
    - literal: National Academies of Sciences, Engineering, and Medicine
  DOI: 10.17226/25303
  event-place: Washington, D.C.
  ISBN: 978-0-309-48616-3
  issued:
    - year: 2019
      month: 9
      day: 20
  note: 'interest: 90'
  publisher: National Academies Press
  publisher-place: Washington, D.C.
  source: National Academies Press
  title: Reproducibility and Replicability in Science
  type: book
  URL: https://www.nap.edu/catalog/25303

- id: ConfirmationDepthMeasure2014
  abstract: >-
    What does it mean to reproduce a scientific study?  Confirmation Depth
    provides a guiding principle.
  accessed:
    - year: 2023
      month: 2
      day: 23
  citation-key: ConfirmationDepthMeasure2014
  container-title: David Soergel
  issued:
    - year: 2014
      month: 10
      day: 21
  language: en
  title: Confirmation Depth as a measure of reproducible scientific research.
  type: webpage
  URL: >-
    http://davidsoergel.com/posts/confirmation-depth-as-a-measure-of-reproducible-scientific-research

- id: constantinDocumentComponentsOntology2016
  abstract: >-
    The availability in machine-readable form of descriptions of the structure
    of documents, as well as of the document discourse (e.g. the scientific
    discourse within scholarly articles), is crucial for facilitating semantic
    publishing and the overall c
  accessed:
    - year: 2023
      month: 5
      day: 25
  author:
    - family: Constantin
      given: Alexandru
    - family: Peroni
      given: Silvio
    - family: Pettifer
      given: Steve
    - family: Shotton
      given: David
    - family: Vitali
      given: Fabio
  citation-key: constantinDocumentComponentsOntology2016
  container-title: Semantic Web
  DOI: 10.3233/SW-150177
  ISSN: 1570-0844
  issue: '2'
  issued:
    - year: 2016
      month: 1
      day: 1
  language: en
  page: 167-181
  publisher: IOS Press
  source: content.iospress.com
  title: The Document Components Ontology (DoCO)
  type: article-journal
  URL: https://content.iospress.com/articles/semantic-web/sw177
  volume: '7'

- id: copeStrongSecurityStarts2020
  abstract: >-
    While there is – rightly – a big focus on securing software that is already
    deployed, the reality is that many future vulnerabilities stem from the
    creation of that software. Insecure applications give hackers a back door.
    For instance, buffer overflows and code injection attacks can lead to
    compromised confidentiality of data, loss of service, damage to the systems
    of thousands of users, even – in the case of products containing embedded
    software, such as medical equipment or vehicles – risk to life. 

    While we focus on securing software that is already deployed, the reality is
    that many future vulnerabilities stem from the creation of that software. 

    Securing development is a tough challenge due to the increasing complexity
    of software, the volume of code, multiple contributors, distributed teams
    and the pressure to deliver to tight deadlines. Plus, developers
    traditionally have not been focused on security. That is changing with the
    emergence of DevSecOps, which focuses on implementing software security
    practices and tools at every stage of the lifecycle, explains Rod Cope of
    Perforce Software.
  accessed:
    - year: 2022
      month: 5
      day: 23
  author:
    - family: Cope
      given: Rod
  citation-key: copeStrongSecurityStarts2020
  container-title: Network Security
  container-title-short: Network Security
  DOI: 10.1016/S1353-4858(20)30078-7
  ISSN: 1353-4858, 1872-9371
  issue: '7'
  issued:
    - year: 2020
      month: 7
  language: en
  page: 6-9
  source: DOI.org (Crossref)
  title: Strong security starts with software development
  type: article-journal
  URL: http://www.magonlinelibrary.com/doi/10.1016/S1353-4858%2820%2930078-7
  volume: '2020'

- id: copikSeBSServerlessBenchmark2021
  abstract: >-
    Function-as-a-Service (FaaS) is one of the most promising directions for the
    future of cloud services, and serverless functions have immediately become a
    new middleware for building scalable and cost-efficient microservices and
    appli cations. However, the quickly moving technology hinders
    reproducibility, and the lack of a standardized benchmarking suite leads to
    ad-hoc solutions and microbenchmarks being used in serverless research,
    further complicating meta-analysis and comparison of research solutions. To
    address this challenge, we propose the Serverless Benchmark Suite: the first
    benchmark for FaaS computing that systematically covers a wide spectrum of
    cloud resources and applications. Our benchmark consists of the
    specification of representative workloads, the accompanying implementation
    and evaluation infrastructure, and the evaluation methodology that
    facilitates reproducibility and enables interpretability. We demonstrate
    that the abstract model of a FaaS execution environment ensures the
    applicability of our benchmark to multiple commercial providers such as AWS,
    Azure, and Google Cloud. Our work facilities experimental evaluation of
    serverless systems, and delivers a standardized, reliable and evolving
    evaluation methodology of performance, efficiency, scalability and
    reliability of middleware FaaS platforms.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Copik
      given: Marcin
    - family: Kwasniewski
      given: Grzegorz
    - family: Besta
      given: Maciej
    - family: Podstawski
      given: Michal
    - family: Hoefler
      given: Torsten
  citation-key: copikSeBSServerlessBenchmark2021
  collection-title: Middleware '21
  container-title: Proceedings of the 22nd International Middleware Conference
  DOI: 10.1145/3464298.3476133
  event-place: New York, NY, USA
  ISBN: 978-1-4503-8534-3
  issued:
    - year: 2021
      month: 10
      day: 2
  note: 'interest: 94'
  page: 64–78
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'SeBS: a serverless benchmark suite for function-as-a-service computing'
  title-short: SeBS
  type: paper-conference
  URL: https://doi.org/10.1145/3464298.3476133

- id: coplienWhyMostUnit
  author:
    - family: Coplien
      given: James O.
  citation-key: coplienWhyMostUnit
  title: Why Most Unit Testing is Waste
  type: manuscript

- id: corchoWorkflowcentricResearchObjects2012
  abstract: >-
    A workflow-centric research object bundles a workflow, the provenance of the
    results obtained by its enactment, other digital objects that are relevant
    for the experiment (papers, datasets, etc.), and annotations that
    semantically describe all these objects. In this paper, we propose a model
    to specify workflow-centric research objects, and show how the model can be
    grounded using semantic technologies and existing vocabularies, in
    particular the Object Reuse and Exchange (ORE) model and the Annotation
    Ontology (AO).We describe the life-cycle of a research object, which
    resembles the life-cycle of a scienti?c experiment.
  accessed:
    - year: 2022
      month: 7
      day: 25
  author:
    - family: Corcho
      given: Oscar
    - family: Garijo Verdejo
      given: Daniel
    - family: Belhajjame
      given: K.
    - family: Zhao
      given: Jun
    - family: Missier
      given: P.
    - family: Newman
      given: David
    - family: Palma
      given: R.
    - family: Bechhofer
      given: S.
    - family: García Cuesta
      given: Esteban
    - family: Gómez-Pérez
      given: José Manuel
    - family: Klyne
      given: Graham
    - family: Roos
      given: Marco
    - family: Ruiz
      given: José Enrique
    - family: Soiland-Reyes
      given: Stian
    - family: Verdes-Montenegro
      given: Lourdes
    - family: De Roure
      given: D.
    - family: Goble
      given: C.
  citation-key: corchoWorkflowcentricResearchObjects2012
  container-title: >-
    Proceedings of Workshop on the Semantic Publishing | 9 th Extended Semantic
    Web Conference Hersonissos | 28/05/2012 - 28/05/2012 | Hersonissos, Creta
    (Grecia)
  event-place: Hersonissos, Creta (Grecia)
  event-title: 9 th Extended Semantic Web Conference Hersonissos
  issued:
    - year: 2012
  language: eng
  license: https://creativecommons.org/licenses/by-nc-nd/3.0/es/
  page: 1-12
  publisher: Facultad de Informática (UPM)
  publisher-place: Hersonissos, Creta (Grecia)
  source: oa.upm.es
  title: >-
    Workflow-centric research objects: First class citizens in scholarly
    discourse.
  title-short: Workflow-centric research objects
  type: paper-conference
  URL: http://sepublica.mywikipaper.org/sepublica2012.pdf

- id: cosmoIdentifiersDigitalObjects2018
  abstract: >-
    In the very broad scope addressed by digital preservation initiatives, a
    special place belongs to the scientific and technical artifacts that we need
    to properly archive to enable scientific reproducibility. For these
    artifacts we need identifiers that are not only unique and persistent, but
    also support integrity in an intrinsic way. They must provide strong
    guarantees that the object denoted by a given identifier will always be the
    same, without relying on third parties and external administrative
    processes.

    In this article, we report on our quest for this identifiers for digital
    objects (IDOs), whose properties are different from, and complementary to,
    those of the various digital identifiers of objects (DIOs) that are in
    widespread use today. We argue that both kinds of identifiers are needed and
    present the framework for intrinsic persistent identifiers that we have
    adopted in Software Heritage for preserving billions of software artifacts. 
        Hosted on the Open Science Framework
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Cosmo
      given: Roberto Di
    - family: Gruenpeter
      given: Morane
    - family: Zacchiroli
      given: Stefano
  citation-key: cosmoIdentifiersDigitalObjects2018
  DOI: 10.17605/OSF.IO/KDE56
  issued:
    - year: 2018
      month: 9
      day: 21
  language: en
  note: 'interest: 86'
  publisher: OSF
  source: osf.io
  title: >-
    Identifiers for Digital Objects: The case of software source code
    preservation.
  title-short: 204.4 Identifiers for Digital Objects
  type: article-journal
  URL: https://osf.io/kde56/

- id: costaCapturingQueryingWorkflow2013
  abstract: >-
    Scientific workflows are commonly used to model and execute large-scale
    scientific experiments. They represent key resources for scientists and are
    enacted and managed by Scientific Workflow Management Systems (SWfMS). Each
    SWfMS has its particular approach to execute workflows and to capture and
    manage their provenance data. Due to the large scale of experiments, it may
    be unviable to analyze provenance data only after the end of the execution.
    A single experiment may demand weeks to run, even in high performance
    computing environments. Thus scientists need to monitor the experiment
    during its execution, and this can be done through provenance data. Runtime
    provenance analysis allows for scientists to monitor workflow execution and
    to take actions before the end of it (i.e. workflow steering). This
    provenance data can also be used to fine-tune the parallel execution of the
    workflow dynamically. We use the PROV data model as a basic framework for
    modeling and providing runtime provenance as a database that can be queried
    even during the execution. This database is agnostic of SWfMS and workflow
    engine. We show the benefits of representing and sharing runtime provenance
    data for improving the experiment management as well as the analysis of the
    scientific data.
  accessed:
    - year: 2022
      month: 7
      day: 26
  author:
    - family: Costa
      given: Flavio
    - family: Silva
      given: Vítor
    - family: Oliveira
      given: Daniel
      non-dropping-particle: de
    - family: Ocaña
      given: Kary
    - family: Ogasawara
      given: Eduardo
    - family: Dias
      given: Jonas
    - family: Mattoso
      given: Marta
  citation-key: costaCapturingQueryingWorkflow2013
  container-title: Proceedings of the Joint EDBT/ICDT 2013 Workshops on - EDBT '13
  DOI: 10.1145/2457317.2457365
  event-place: Genoa, Italy
  event-title: the Joint EDBT/ICDT 2013 Workshops
  ISBN: 978-1-4503-1599-9
  issued:
    - year: 2013
  language: en
  page: '282'
  publisher: ACM Press
  publisher-place: Genoa, Italy
  source: DOI.org (Crossref)
  title: >-
    Capturing and querying workflow runtime provenance with PROV: a practical
    approach
  title-short: Capturing and querying workflow runtime provenance with PROV
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?doid=2457317.2457365

- id: coulourisBlastBenchmark2016
  accessed:
    - year: 2023
      month: 12
      day: 4
  author:
    - family: Coulouris
      given: George
    - family: NIH Staff
      given: ''
  citation-key: coulourisBlastBenchmark2016
  container-title: Fiehn Lab
  issued:
    - year: 2016
  title: Blast Benchmark
  type: webpage
  URL: https://fiehnlab.ucdavis.edu/staff/kind/Collector/Benchmark/blast-benchmark

- id: courtesFunctionalPackageManagement2013
  abstract: >-
    We describe the design and implementation of GNU Guix, a purely functional
    package manager designed to support a complete GNU/Linux distribution. Guix
    supports transactional upgrades and roll-backs, unprivileged package
    management, per-user profiles, and garbage collection. It builds upon the
    low-level build and deployment layer of the Nix package manager. Guix uses
    Scheme as its programming interface. In particular, we devise an embedded
    domain-specific language (EDSL) to describe and compose packages. We
    demonstrate how it allows us to benefit from the host general-purpose
    programming language while not compromising on expressiveness. Second, we
    show the use of Scheme to write build programs, leading to "two-tier''
    programming system.
  accessed:
    - year: 2023
      month: 12
      day: 19
  author:
    - family: Courtès
      given: Ludovic
  citation-key: courtesFunctionalPackageManagement2013
  event-title: European Lisp Symposium
  issued:
    - year: 2013
      month: 6
      day: 3
  language: en
  source: inria.hal.science
  title: Functional Package Management with Guix
  type: paper-conference
  URL: https://inria.hal.science/hal-00824004

- id: courtesReproducibleUserControlledSoftware2015
  abstract: "Support teams of high-performance computing (HPC) systems often find themselves between a rock and a hard place: on one hand, they understandably administrate these large systems in a conservative way, but on the other hand, they try to satisfy their users by deploying up-to-date tool chains as well as libraries and scientific software. HPC system users often have no guarantee that they will be able to reproduce results at a later point in time, even on the same system—software may have been upgraded, removed, or recompiled under their feet, and they have little hope of being able to reproduce the same software environment elsewhere. We present GNU\_Guix and the functional package management paradigm and show how it can improve reproducibility and sharing among researchers with representative use cases."
  author:
    - family: Courtès
      given: Ludovic
    - family: Wurmus
      given: Ricardo
  citation-key: courtesReproducibleUserControlledSoftware2015
  collection-title: Lecture Notes in Computer Science
  container-title: 'Euro-Par 2015: Parallel Processing Workshops'
  DOI: 10.1007/978-3-319-27308-2_47
  editor:
    - family: Hunold
      given: Sascha
    - family: Costan
      given: Alexandru
    - family: Giménez
      given: Domingo
    - family: Iosup
      given: Alexandru
    - family: Ricci
      given: Laura
    - family: Gómez Requena
      given: María Engracia
    - family: Scarano
      given: Vittorio
    - family: Varbanescu
      given: Ana Lucia
    - family: Scott
      given: Stephen L.
    - family: Lankes
      given: Stefan
    - family: Weidendorfer
      given: Josef
    - family: Alexander
      given: Michael
  event-place: Cham
  ISBN: 978-3-319-27308-2
  issued:
    - year: 2015
  language: en
  note: 'interest: 99'
  page: 579-591
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: Reproducible and User-Controlled Software Environments in HPC with Guix
  type: paper-conference

- id: courtesTamingStatStorm2021
  accessed:
    - year: 2024
      month: 1
      day: 19
  author:
    - family: Courtès
      given: Ludovic
  citation-key: courtesTamingStatStorm2021
  container-title: GNU Guix Blog
  issued:
    - year: 2021
      month: 8
      day: 2
  title: Taming the ‘stat’ storm with a loader cache
  type: post-weblog
  URL: https://guix.gnu.org/en/blog/2021/taming-the-stat-storm-with-a-loader-cache/

- id: coxGenericDilemma2009
  abstract: >-
    Generic data structures (vectors, queues, maps, trees, and so on) seem to be
    the hot topic if you are evaluating a new language. One of the most frequent
    questions we've had about Go is where the generics are. It seems like there
    are three basic approaches to generics:
        (The C approach.) Leave them out.
        (The C++ approach.) Compile-time specialization or macro expansion.
        (The Java approach.) Box everything implicitly.
  author:
    - family: Cox
      given: Russ
  citation-key: coxGenericDilemma2009
  container-title: Thoughts and links about programming
  issued:
    - year: 2009
      month: 12
      day: 3
  title: The Generic Dilemma
  type: post-weblog
  URL: https://research.swtch.com/generic

- id: cpythondevelopersStatusPythonVersions
  abstract: >-
    The main branch is currently the future Python 3.13, and is the only branch
    that accepts new features. The latest release for each Python version can be
    found on the download page. Python Release C...
  accessed:
    - year: 2023
      month: 7
      day: 17
  author:
    - family: CPython developers
      given: ''
  citation-key: cpythondevelopersStatusPythonVersions
  container-title: Python Developer's Guide
  language: en
  title: Status of Python Versions
  type: webpage
  URL: https://devguide.python.org/versions/

- id: crawlProvenanceBasedFaultTolerance2008
  abstract: >-
    Capturing provenance information in scientific workflows is not only useful
    for determining data-dependencies, but also for a wide range of queries
    including fault tolerance and usage statistics. As collaborative scientific
    workflow environments provide users with reusable shared workflows,
    collection and usage of provenance data in a generic way that could serve
    multiple data and computational models become vital. This paper presents a
    method for capturing data value- and control- dependencies for provenance
    information collection in the Kepler scientific workflow system. It also
    describes how the collected information based on these dependencies could be
    used for a fault tolerance framework in different models of computation.
  author:
    - family: Crawl
      given: Daniel
    - family: Altintas
      given: Ilkay
  citation-key: crawlProvenanceBasedFaultTolerance2008
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-540-89965-5_17
  editor:
    - family: Freire
      given: Juliana
    - family: Koop
      given: David
    - family: Moreau
      given: Luc
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-89965-5
  issued:
    - year: 2008
  language: en
  note: 'interest: 96'
  page: 152-159
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: A Provenance-Based Fault Tolerance Mechanism for Scientific Workflows
  type: paper-conference

- id: cristeaValuesDisplayItems2018
  abstract: >-
    P values represent a widely used, but pervasively misunderstood and fiercely
    contested method of scientific inference. Display items, such as figures and
    tables, often containing the main results, are an important source of P
    values. We conducted a survey comparing the overall use of P values and the
    occurrence of significant P values in display items of a sample of articles
    in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017
    and, respectively, in 1997. We also examined the reporting of multiplicity
    corrections and its potential influence on the proportion of statistically
    significant P values. Our findings demonstrated substantial and growing
    reliance on P values in display items, with increases of 2.5 to 14.5 times
    in 2017 compared to 1997. The overwhelming majority of P values (94%, 95%
    confidence interval [CI] 92% to 96%) were statistically significant. Methods
    to adjust for multiplicity were almost non-existent in 1997, but reported in
    many articles relying on P values in 2017 (Nature 68%, Science 48%, PNAS
    38%). In their absence, almost all reported P values were statistically
    significant (98%, 95% CI 96% to 99%). Conversely, when any multiplicity
    corrections were described, 88% (95% CI 82% to 93%) of reported P values
    were statistically significant. Use of Bayesian methods was scant (2.5%) and
    rarely (0.7%) articles relied exclusively on Bayesian statistics. Overall,
    wider appreciation of the need for multiplicity corrections is a welcome
    evolution, but the rapid growth of reliance on P values and implausibly high
    rates of reported statistical significance are worrisome.
  accessed:
    - year: 2022
      month: 11
      day: 15
  author:
    - family: Cristea
      given: Ioana Alina
    - family: Ioannidis
      given: John P. A.
  citation-key: cristeaValuesDisplayItems2018
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10.1371/journal.pone.0197440
  ISSN: 1932-6203
  issue: '5'
  issued:
    - year: 2018
      month: 5
      day: 15
  language: en
  note: 'interest: 83'
  page: e0197440
  publisher: Public Library of Science
  source: PLoS Journals
  title: >-
    P values in display items are ubiquitous and almost invariably significant:
    A survey of top science journals
  title-short: P values in display items are ubiquitous and almost invariably significant
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197440
  volume: '13'

- id: cuevas-vicenttinScientificWorkflowsProvenance2012
  abstract: >-
    Scientific workflows are becoming increasingly popular for compute-intensive
    and data-intensive scientific applications. The vision and promise of
    scientific workflows includes rapid, easy workflow design, reuse, scalable
    execution, and other advantages, e.g., to facilitate “reproducible science”
    through provenance (e.g., data lineage) support. However, as described in
    the paper, important research challenges remain. While the database
    community has studied (business) workflow technologies extensively in the
    past, most current work in scientific workflows seems to be done outside of
    the database community, e.g., by practitioners and researchers in the
    computational sciences and eScience. We provide a brief introduction to
    scientific workflows and provenance, and identify areas and problems that
    suggest new opportunities for database research.
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Cuevas-Vicenttín
      given: Víctor
    - family: Dey
      given: Saumen
    - family: Köhler
      given: Sven
    - family: Riddle
      given: Sean
    - family: Ludäscher
      given: Bertram
  citation-key: cuevas-vicenttinScientificWorkflowsProvenance2012
  container-title: Datenbank-Spektrum
  container-title-short: Datenbank Spektrum
  DOI: 10.1007/s13222-012-0100-z
  ISSN: 1618-2162, 1610-1995
  issue: '3'
  issued:
    - year: 2012
      month: 11
  language: en
  page: 193-203
  source: DOI.org (Crossref)
  title: 'Scientific Workflows and Provenance: Introduction and Research Opportunities'
  title-short: Scientific Workflows and Provenance
  type: article-journal
  URL: http://link.springer.com/10.1007/s13222-012-0100-z
  volume: '12'

- id: curtsingerSTABILIZERStatisticallySound2013
  abstract: >-
    Researchers and software developers require effective performance
    evaluation. Researchers must evaluate optimizations or measure overhead.
    Software developers use automatic performance regression tests to discover
    when changes improve or degrade performance. The standard methodology is to
    compare execution times before and after applying changes. Unfortunately,
    modern architectural features make this approach unsound. Statistically
    sound evaluation requires multiple samples to test whether one can or cannot
    (with high confidence) reject the null hypothesis that results are the same
    before and after. However, caches and branch predictors make performance
    dependent on machine-specific parameters and the exact layout of code, stack
    frames, and heap objects. A single binary constitutes just one sample from
    the space of program layouts, regardless of the number of runs. Since
    compiler optimizations and code changes also alter layout, it is currently
    impossible to distinguish the impact of an optimization from that of its
    layout effects. This paper presents Stabilizer, a system that enables the
    use of the powerful statistical techniques required for sound performance
    evaluation on modern architectures. Stabilizer forces executions to sample
    the space of memory configurations by repeatedly re-randomizing layouts of
    code, stack, and heap objects at runtime. Stabilizer thus makes it possible
    to control for layout effects. Re-randomization also ensures that layout
    effects follow a Gaussian distribution, enabling the use of statistical
    tests like ANOVA. We demonstrate Stabilizer's efficiency (<7% median
    overhead) and its effectiveness by evaluating the impact of LLVM's
    optimizations on the SPEC CPU2006 benchmark suite. We find that, while -O2
    has a significant impact relative to -O1, the performance impact of -O3 over
    -O2 optimizations is indistinguishable from random noise.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Curtsinger
      given: Charlie
    - family: Berger
      given: Emery D.
  citation-key: curtsingerSTABILIZERStatisticallySound2013
  container-title: ACM SIGARCH Computer Architecture News
  container-title-short: SIGARCH Comput. Archit. News
  DOI: 10.1145/2490301.2451141
  ISSN: 0163-5964
  issue: '1'
  issued:
    - year: 2013
      month: 3
      day: 16
  note: 'interest: 60'
  page: 219–228
  source: March 2013
  title: 'STABILIZER: statistically sound performance evaluation'
  title-short: STABILIZER
  type: article-journal
  URL: https://doi.org/10.1145/2490301.2451141
  volume: '41'

- id: cvedatabaseCVECVE2020143862020
  accessed:
    - year: 2023
      month: 2
      day: 18
  author:
    - family: CVE database
      given: ''
  citation-key: cvedatabaseCVECVE2020143862020
  issued:
    - year: 2020
      month: 6
      day: 17
  title: CVE - CVE-2020-14386
  type: webpage
  URL: https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-14386

- id: cyranoskiEducationPhDFactory2011
  abstract: The world is producing more PhDs than ever before. Is it time to stop?
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Cyranoski
      given: David
    - family: Gilbert
      given: Natasha
    - family: Ledford
      given: Heidi
    - family: Nayar
      given: Anjali
    - family: Yahia
      given: Mohammed
  citation-key: cyranoskiEducationPhDFactory2011
  container-title: Nature
  DOI: 10.1038/472276a
  ISSN: 1476-4687
  issue: '7343'
  issued:
    - year: 2011
      month: 4
      day: 1
  language: en
  license: 2011 Nature Publishing Group
  number: '7343'
  page: 276-279
  publisher: Nature Publishing Group
  source: www.nature.com
  title: 'Education: The PhD factory'
  title-short: Education
  type: article-journal
  URL: https://www.nature.com/articles/472276a
  volume: '472'

- id: daiLightweightProvenanceService2017
  abstract: >-
    Provenance describes detailed information about the history of a piece of
    data, containing the relationships among elements such as users, processes,
    jobs, and workflows that contribute to the existence of data. Provenance is
    key to supporting many data management functionalities that are increasingly
    important in operations such as identifying data sources, parameters, or
    assumptions behind a given result; auditing data usage; or understanding
    details about how inputs are transformed into outputs. Despite its
    importance, however, provenance support is largely underdeveloped in highly
    parallel architectures and systems. One major challenge is the demanding
    requirements of providing provenance service in situ. The need to remain
    lightweight and to be always on often conflicts with the need to be
    transparent and offer an accurate catalog of details regarding the
    applications and systems. To tackle this challenge, we introduce a
    lightweight provenance service, called LPS, for high-performance computing
    (HPC) systems. LPS leverages a kernel instrument mechanism to achieve
    transparency and introduces representative execution and flexible
    granularity to capture comprehensive provenance with controllable overhead.
    Extensive evaluations and use cases have confirmed its efficiency and
    usability. We believe that LPS can be integrated into current and future HPC
    systems to support a variety of data management needs.
  accessed:
    - year: 2024
      month: 2
      day: 14
  author:
    - family: Dai
      given: Dong
    - family: Chen
      given: Yong
    - family: Carns
      given: Philip
    - family: Jenkins
      given: John
    - family: Ross
      given: Robert
  citation-key: daiLightweightProvenanceService2017
  container-title: >-
    2017 26th International Conference on Parallel Architectures and Compilation
    Techniques (PACT)
  DOI: 10.1109/PACT.2017.14
  event-title: >-
    2017 26th International Conference on Parallel Architectures and Compilation
    Techniques (PACT)
  issued:
    - year: 2017
      month: 9
  page: 117-129
  source: IEEE Xplore
  title: Lightweight Provenance Service for High-Performance Computing
  type: paper-conference
  URL: https://ieeexplore.ieee.org/abstract/document/8091224

- id: dancoCanTwitterScience2020
  abstract: >-
    Before I found my way to the tech world, I was a grad student in the
    neuroscience department at McGill University. I never took the opportunity
    to get my PhD, and left science to do a startup instead. But I still think
    about it sometimes.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Danco
      given: Alex
  citation-key: dancoCanTwitterScience2020
  container-title: Welcome to Dancoland
  issued:
    - year: 2020
      month: 2
      day: 15
  language: en
  title: Can Twitter Save Science?
  type: post-weblog
  URL: https://alexdanco.com/2020/02/15/can-twitter-save-science/

- id: dashnowTenSimpleRules2014
  accessed:
    - year: 2024
      month: 4
      day: 21
  author:
    - family: Dashnow
      given: Harriet
    - family: Lonsdale
      given: Andrew
    - family: Bourne
      given: Philip E.
  citation-key: dashnowTenSimpleRules2014
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1003858
  ISSN: 1553-7358
  issue: '10'
  issued:
    - year: 2014
      month: 10
      day: 23
  language: en
  page: e1003858
  publisher: Public Library of Science
  source: PLoS Journals
  title: Ten Simple Rules for Writing a PLOS Ten Simple Rules Article
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003858
  volume: '10'

- id: dasilvaReplicationEmpiricalStudies2014
  abstract: "In this article, we present a systematic mapping study of replications in software engineering. The goal is to plot the landscape of current published replications of empirical studies in software engineering research. We applied the systematic review method to search and select published articles, and to extract and synthesize data from the selected articles that reported replications. Our search retrieved more than 16,000 articles, from which we selected 96 articles, reporting 133 replications performed between 1994 and 2010, of 72 original studies. Nearly 70\_% of the replications were published after 2004 and 70\_% of these studies were internal replications. The topics of software requirements, software construction, and software quality concentrated over 55\_% of the replications, while software design, configuration management, and software tools and methods were the topics with the smallest number of replications. We conclude that the number of replications has grown in the last few years, but the absolute number of replications is still small, in particular considering the breadth of topics in software engineering. We still need incentives to perform external replications, better standards to report empirical studies and their replications, and collaborative research agendas that could speed up development and publication of replications."
  accessed:
    - year: 2022
      month: 9
      day: 7
  author:
    - family: Silva
      given: Fabio Q. B.
      non-dropping-particle: da
    - family: Suassuna
      given: Marcos
    - family: França
      given: A. César C.
    - family: Grubb
      given: Alicia M.
    - family: Gouveia
      given: Tatiana B.
    - family: Monteiro
      given: Cleviton V. F.
    - family: Santos
      given: Igor Ebrahim
      non-dropping-particle: dos
  citation-key: dasilvaReplicationEmpiricalStudies2014
  container-title: Empirical Software Engineering
  container-title-short: Empir Software Eng
  DOI: 10.1007/s10664-012-9227-7
  ISSN: 1573-7616
  issue: '3'
  issued:
    - year: 2014
      month: 6
      day: 1
  language: en
  note: 'interest: 66'
  page: 501-557
  source: Springer Link
  title: >-
    Replication of empirical studies in software engineering research: a
    systematic mapping study
  title-short: Replication of empirical studies in software engineering research
  type: article-journal
  URL: https://doi.org/10.1007/s10664-012-9227-7
  volume: '19'

- id: dataversecommunitydevelopersDataverse2023
  abstract: >-
    Dataverse is an open source software platform for sharing, finding, citing,
    and preserving research data (developed by the Data Science and Products
    team at the Institute for Quantitative Social Science and the Dataverse
    community).


    dataverse.org is our home on the web and shows a map of Dataverse
    installations around the world, a list of features, integrations that have
    been made possible through REST APIs, our development roadmap, and more.


    We maintain a demo site at demo.dataverse.org which you are welcome to use
    for testing and evaluating Dataverse.


    To install Dataverse, please see our Installation Guide which will prompt
    you to download our latest release.


    To discuss Dataverse with the community, please join our mailing list,
    participate in a community call, chat with us at chat.dataverse.org, or
    attend our annual Dataverse Community Meeting.


    We love contributors! Please see our Contributing Guide for ways you can
    help.


    Dataverse is a trademark of President and Fellows of Harvard College and is
    registered in the United States.
  accessed:
    - year: 2023
      month: 4
      day: 21
  author:
    - family: Dataverse community developers
      given: ''
    - family: >-
        Data Science and Products team at the Institute for Quantitative Social
        Science
      given: ''
  citation-key: dataversecommunitydevelopersDataverse2023
  genre: Java
  issued:
    - year: 2023
      month: 4
      day: 21
  original-date:
    - year: 2013
      month: 11
      day: 1
  publisher: Institute for Quantitative Social Science
  source: GitHub
  title: Dataverse®
  type: software
  URL: https://github.com/IQSS/dataverse

- id: DataVersionControl
  abstract: >-
    Open-source version control system for Data Science and Machine Learning
    projects. Git-like experience to organize your data, models, and
    experiments.
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: DataVersionControl
  container-title: Data Version Control · DVC
  language: en
  note: 'interest: 68'
  title: Data Version Control · DVC
  type: webpage
  URL: https://dvc.org/

- id: dattaPerformanceEnhancementCustomer2020
  abstract: >-
    Over the years, there has been a huge popularity of the recommender systems
    worldwide. Recommender systems have been implemented over several domains
    ranging from recommendations for videos and movies to that for products and
    applications, and many more. The algorithms, which are used for recommender
    systems, implement segmentation of the customer based on several attributes.
    These algorithms are time-consuming and require comparatively high
    computation power. This work deals with the parallelization of different
    algorithms for simple customer segmentation in the Python environment using
    the framework, Ray. The dataset for this work includes a huge list of
    purchases that are carried out by 4000 customers, over a year. The
    parallelization is carried out throughout the multicores of CPU and the
    cores of GPU. Additionally, the work also shows the speedup that is obtained
    after parallelization, for analyzing the overall increase in performance.
  accessed:
    - year: 2022
      month: 11
      day: 15
  author:
    - family: Datta
      given: Debajit
    - family: Agarwal
      given: Rishav
    - family: David
      given: Preetha Evangeline
  citation-key: dattaPerformanceEnhancementCustomer2020
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2020
      month: 11
      day: 20
  language: en
  note: 'interest: 75'
  number: '3733832'
  publisher-place: Rochester, NY
  source: Social Science Research Network
  title: >-
    Performance Enhancement of Customer Segmentation Using a Distributed Python
    Framework, Ray
  type: article
  URL: https://papers.ssrn.com/abstract=3733832

- id: davidsonProvenanceScientificWorkflows2008
  abstract: >-
    Provenance in the context of workflows, both for the data they derive and
    for their specification, is an essential component to allow for result
    reproducibility, sharing, and knowledge re-use in the scientific community.
    Several workshops have been held on the topic, and it has been the focus of
    many research projects and prototype systems. This tutorial provides an
    overview of research issues in provenance for scientific workflows, with a
    focus on recent literature and technology in this area. It is aimed at a
    general database research audience and at people who work with scientific
    data and workflows. We will (1) provide a general overview of scientific
    workflows, (2) describe research on provenance for scientific workflows and
    show in detail how provenance is supported in existing systems; (3) discuss
    emerging applications that are enabled by provenance; and (4) outline open
    problems and new directions for database-related research.
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Davidson
      given: Susan B.
    - family: Freire
      given: Juliana
  citation-key: davidsonProvenanceScientificWorkflows2008
  container-title: >-
    Proceedings of the 2008 ACM SIGMOD international conference on Management of
    data  - SIGMOD '08
  DOI: 10.1145/1376616.1376772
  event-place: Vancouver, Canada
  event-title: the 2008 ACM SIGMOD international conference
  ISBN: 978-1-60558-102-6
  issued:
    - year: 2008
  language: en
  page: '1345'
  publisher: ACM Press
  publisher-place: Vancouver, Canada
  source: DOI.org (Crossref)
  title: 'Provenance and scientific workflows: challenges and opportunities'
  title-short: Provenance and scientific workflows
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=1376616.1376772

- id: davisonAutomatedCaptureExperiment2012
  abstract: >-
    Published scientific research that relies on numerical computations is too
    often not reproducible. For computational research to become consistently
    and reliably reproducible, the process must become easier to achieve, as
    part of day-to-day research. A combination of best practices and automated
    tools can make it easier to create reproducible research.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Davison
      given: Andrew
  citation-key: davisonAutomatedCaptureExperiment2012
  container-title: Computing in Science & Engineering
  container-title-short: Comput. Sci. Eng.
  DOI: 10.1109/MCSE.2012.41
  ISSN: 1521-9615
  issue: '4'
  issued:
    - year: 2012
      month: 7
  page: 48-56
  source: DOI.org (Crossref)
  title: >-
    Automated Capture of Experiment Context for Easier Reproducibility in
    Computational Research
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/6180156/
  volume: '14'

- id: davisPerceivedUsefulnessPerceived1989
  abstract: >-
    Valid measurement scales for predicting user acceptance of computers are in
    short supply. Most subjective measures used in practice are unvalidated, and
    their relationship to system usage is unknown. The present research develops
    and validates new scales for two specific variables, perceived usefulness
    and perceived ease of use, which are hypothesized to be fundamental
    determinants of user acceptance. Definitions for these two variables were
    used to develop scale items that were pretested for content validity and
    then tested for reliability and construct validity in two studies involving
    a total of 152 users and four application programs. The measures were
    refined and stream-lined, resulting in two six-item scales with
    reliabilities of.98 for usefulness and.94 for ease of use. The scales
    exhibited high convergent, discriminant, and factorial validity. Perceived
    usefulness was significantly correlated with both self-reported current
    usage (r=.63, Study 1) and self-predicted future usage (r=.85, Study 2).
    Perceived ease of use was also significantly correlated with current usage
    (r=.45, Study 1) and future usage (r=.59, Study 2). In both studies,
    usefulness had a significantly greater correlation with usage behavior than
    did ease of use. Regression analyses suggest that perceived ease of use may
    actually be a causal antecedent to perceived usefulness, as opposed to a
    parallel, direct determinant of system usage. Implications are drawn for
    future research on user acceptance.
  accessed:
    - year: 2022
      month: 5
      day: 27
  author:
    - family: Davis
      given: Fred D.
  citation-key: davisPerceivedUsefulnessPerceived1989
  container-title: MIS Quarterly
  container-title-short: MIS Quarterly
  DOI: 10.2307/249008
  ISSN: '02767783'
  issue: '3'
  issued:
    - year: 1989
      month: 9
  page: '319'
  source: DOI.org (Crossref)
  title: >-
    Perceived Usefulness, Perceived Ease of Use, and User Acceptance of
    Information Technology
  type: article-journal
  URL: https://www.jstor.org/stable/249008?origin=crossref
  volume: '13'

- id: davisTechnologyAcceptanceModel1985
  abstract: >-
    The goal of this research is to develop and test a theoretical model of the
    effect of system characteristics on user acceptance of computer-based
    information systems. The model, referred to as the technology acceptance
    model (TAM), is being developed with two major objectives in mind. First, it
    should improve our understanding of user acceptance processes, providing new
    theoretical insights into the successful design and implementation of
    information systems. Second, TAM should provide the theoretical basis for a
    practical "user acceptance testing" methodology that would enable system
    designers and implementors to evaluate proposed new systems prior to their
    implementation. Applying the proposed model in user acceptance testing would
    involve demonstrating system prototypes to potential users and measuring
    their motivation to use the alternative systems. Such user acceptance
    testing could provide useful information about the relative likelihood of
    success of proposed systems early in their development, where such
    information has

    greatest value. Based on these objectives, key questions guiding this
    research include:

    1. What are the major motivational variables that mediate between system
    characteristics and actual use of computer-based systems by end-users in
    organiza ional settings?

    2. How are these variables causally related to one another, to system
    characteristics, and to user behavior?

    3. How can user motivation be measured prior to organizational
    implementation in order to evaluate the rebtive likelihood of user
    acceptance for proposed new systems?

    For user acceptance testing to be viable, the associated model of user
    motivation must be valid. The present research takes several steps toward
    establishing a valid motivational model of the user, and aims to provide the
    foundation for future research that will tend to lead toward this end.
    Research steps taken in the present thesis include:

    1. a fairly general, well-established theoretical model of human behavior
    from psychology was chosen as a paradigm within which to formulate the
    proposed technology acceptance model;

    2. several adaptations to this paradigm were introduced in order to render
    it applicable to the present context;

    3. published literature in the Management Information Systems and Human
    Factors fields was reviewed to demonstrate that empirical support exists for
    various elements of the proposed model, while at the same time the model
    goes beyond existing theoretical specifications, building upon and
    integrating previous research in a cumulative manner;

    4. measures for the model's psychological variables were developed and
    pre-tested;

    5. a field survey of 100 organizational users was conducted in order to
    validate the measures of the model's variables, and to test the model's
    structure, and

    6. a laboratory user acceptance experiment of two business graphics systems
    involving 40 MBA student subjects was performed to further test the proposed
    model's structure, to test the ability to substitute videotape presentation
    for hands-on interaction in user acceptance tests, to evaluate the specific
    graphics systems being tested, and to test several theoretical extensions
    and refinements to the propose
  accessed:
    - year: 2022
      month: 6
      day: 3
  author:
    - family: Davis
      given: Fred D.
  citation-key: davisTechnologyAcceptanceModel1985
  genre: Thesis
  issued:
    - year: 1985
  language: eng
  license: >-
    M.I.T. theses are protected by copyright. They may be viewed from this
    source for any purpose, but reproduction or distribution in any format is
    prohibited without written permission. See provided URL for inquiries about
    permission.
  publisher: Massachusetts Institute of Technology
  source: dspace.mit.edu
  title: >-
    A technology acceptance model for empirically testing new end-user
    information systems: theory and results
  title-short: >-
    A technology acceptance model for empirically testing new end-user
    information systems
  type: thesis
  URL: https://dspace.mit.edu/handle/1721.1/15192

- id: deelmanFutureScientificWorkflows2018
  abstract: >-
    Today’s computational, experimental, and observational sciences rely on
    computations that involve many related tasks. The success of a scientific
    mission often hinges on the computer automation of these workflows. In April
    2015, the US Department of Energy (DOE) invited a diverse group of domain
    and computer scientists from national laboratories supported by the Office
    of Science, the National Nuclear Security Administration, from industry, and
    from academia to review the workflow requirements of DOE’s science and
    national security missions, to assess the current state of the art in
    science workflows, to understand the impact of emerging extreme-scale
    computing systems on those workflows, and to develop requirements for
    automated workflow management in future and existing environments. This
    article is a summary of the opinions of over 50 leading researchers
    attending this workshop. We highlight use cases, computing systems, workflow
    needs and conclude by summarizing the remaining challenges this community
    sees that inhibit large-scale scientific workflows from becoming a
    mainstream tool for extreme-scale science.
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Deelman
      given: Ewa
    - family: Peterka
      given: Tom
    - family: Altintas
      given: Ilkay
    - family: Carothers
      given: Christopher D
    - family: Dam
      given: Kerstin Kleese
      non-dropping-particle: van
    - family: Moreland
      given: Kenneth
    - family: Parashar
      given: Manish
    - family: Ramakrishnan
      given: Lavanya
    - family: Taufer
      given: Michela
    - family: Vetter
      given: Jeffrey
  citation-key: deelmanFutureScientificWorkflows2018
  container-title: The International Journal of High Performance Computing Applications
  container-title-short: The International Journal of High Performance Computing Applications
  DOI: 10.1177/1094342017704893
  ISSN: 1094-3420, 1741-2846
  issue: '1'
  issued:
    - year: 2018
      month: 1
      day: 1
  language: en
  page: 159-175
  publisher: SAGE Publications Ltd STM
  source: SAGE Journals
  title: The future of scientific workflows
  type: article-journal
  URL: http://journals.sagepub.com/doi/10.1177/1094342017704893
  volume: '32'

- id: DefmacroNatureLisp
  accessed:
    - year: 2023
      month: 9
      day: 11
  citation-key: DefmacroNatureLisp
  title: defmacro - The Nature of Lisp
  type: webpage
  URL: https://www.defmacro.org/ramblings/lisp.html

- id: demorlhonDockerHubImage2020
  accessed:
    - year: 2024
      month: 9
      day: 4
  author:
    - family: Morlhon
      given: Jean-Laurent
      non-dropping-particle: de
  citation-key: demorlhonDockerHubImage2020
  container-title: Docker Blog
  issued:
    - year: 2020
      month: 10
      day: 22
  title: Docker Hub Image Retention Policy Delayed, Subscription Updates
  type: post-weblog
  URL: >-
    https://www.docker.com/blog/docker-hub-image-retention-policy-delayed-and-subscription-updates/

- id: demorlhonScalingDockersBusiness2020
  accessed:
    - year: 2024
      month: 9
      day: 4
  author:
    - family: Morlhon
      given: Jean-Laurent
      non-dropping-particle: de
  citation-key: demorlhonScalingDockersBusiness2020
  container-title: Docker Blog
  issued:
    - year: 2020
      month: 8
      day: 24
  title: 'Scaling Docker’s Business to Serve Millions More Developers: Storage'
  type: post-weblog
  URL: >-
    https://www.docker.com/blog/scaling-dockers-business-to-serve-millions-more-developers-storage/

- id: derossoPurposesConceptsMisfits2016
  abstract: >-
    Git is a widely used version control system that is powerful but
    complicated. Its complexity may not be an inevitable consequence of its
    power but rather evidence of flaws in its design. To explore this
    hypothesis, we analyzed the design of Git using a theory that identifies
    concepts, purposes, and misfits. Some well-known difficulties with Git are
    described, and explained as misfits in which underlying concepts fail to
    meet their intended purpose. Based on this analysis, we designed a reworking
    of Git (called Gitless) that attempts to remedy these flaws. To correlate
    misfits with issues reported by users, we conducted a study of Stack
    Overflow questions. And to determine whether users experienced fewer
    complications using Gitless in place of Git, we conducted a small user
    study. Results suggest our approach can be profitable in identifying,
    analyzing, and fixing design problems.
  accessed:
    - year: 2023
      month: 7
      day: 7
  author:
    - family: De Rosso
      given: Santiago Perez
    - family: Jackson
      given: Daniel
  citation-key: derossoPurposesConceptsMisfits2016
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/3022671.2984018
  ISSN: 0362-1340
  issue: '10'
  issued:
    - year: 2016
      month: 10
      day: 19
  page: 292–310
  source: ACM Digital Library
  title: Purposes, concepts, misfits, and a redesign of git
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/3022671.2984018
  volume: '51'

- id: deroureComputationalResearchObjects2013
  abstract: >-
    Research Objects are bundles of the digital bits and pieces that make up the
    reusable record of a piece of research; they are identifiable, citable and
    sharable. The evolution of this idea within digital research practice has
    led to the development of workflow-centric Research Objects with executable
    components. To address the evolving requirements of research we propose a
    further step, towards objects that are composable and executable by machine:
    Computational Research Objects -- a vision in which the content of our
    digital libraries is autonomously conducting research.
  accessed:
    - year: 2023
      month: 6
      day: 28
  author:
    - family: De Roure
      given: David
  citation-key: deroureComputationalResearchObjects2013
  collection-title: DPRMA '13
  container-title: >-
    Proceedings of the 1st International Workshop on Digital Preservation of
    Research Methods and Artefacts
  DOI: 10.1145/2499583.2499590
  event-place: New York, NY, USA
  ISBN: 978-1-4503-2185-3
  issued:
    - year: 2013
      month: 7
      day: 25
  page: 16–19
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Towards computational research objects
  type: paper-conference
  URL: https://doi.org/10.1145/2499583.2499590

- id: desnoyersUsingLinuxKernel
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Desnoyers
      given: Matthieu
  citation-key: desnoyersUsingLinuxKernel
  container-title: The Linux Kernel documentation
  title: Using the Linux Kernel Tracepoints
  type: webpage
  URL: https://www.kernel.org/doc/html/latest/trace/tracepoints.html

- id: dicosmoCuratedArchivingResearch2020
  abstract: >-
    Software has become an indissociable support of technical and scientific
    knowledge. The preservation of this universal body of knowledge is as
    essential as preserving research articles and data sets. In the quest to
    make scientific results reproducible, and pass knowledge to future
    generations, we must preserve these three main pillars: research articles
    that describe the results, the data sets used or produced, and the software
    that embodies the logic of the data transformation. The collaboration
    between Software Heritage (SWH), the Center for Direct Scientific
    Communication (CCSD) and the scientific and technical information services
    (IES) of The French Institute for Research in Computer Science and
    Automation (Inria)  has resulted in a specified moderation and curation
    workflow for research software artifacts deposited in the HAL  open access
    repository. The curation workflow was developed to help digital librarians
    and archivists handle this new and peculiar artifact-software source code.
    While implementing the workflow, a set of guidelines has emerged from the
    challenges and the solutions put in place to help all actors involved in the
    process.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Di Cosmo
      given: Roberto
    - family: Gruenpeter
      given: Morane
    - family: Marmol
      given: Bruno P
    - family: Monteil
      given: Alain
    - family: Romary
      given: Laurent
    - family: Sadowska
      given: Jozefina
  citation-key: dicosmoCuratedArchivingResearch2020
  container-title: IDCC 2020 - International Digital Curation Conference
  DOI: 10.2218/ijdc.v15i1.698
  event-place: Dublin, Ireland
  issued:
    - year: 2020
      month: 2
  note: 'interest: 84'
  publisher-place: Dublin, Ireland
  source: HAL Archives Ouvertes
  title: >-
    Curated Archiving of Research Software Artifacts : lessons learned from the
    French open archive (HAL)
  title-short: Curated Archiving of Research Software Artifacts
  type: paper-conference
  URL: https://hal.inria.fr/hal-02475835

- id: diFastErrorBoundedLossy2016
  abstract: >-
    Today's HPC applications are producing extremely large amounts of data, thus
    it is necessary to use an efficient compression before storing them to
    parallel file systems. In this paper, we optimize the error-bounded HPC data
    compression, by proposing a novel HPC data compression method that works
    very effectively on compressing large-scale HPC data sets. The compression
    method starts by linearizing multi-dimensional snapshot data. The key idea
    is to fit/predict the successive data points with the bestfit selection of
    curve fitting models. The data that can be predicted precisely will be
    replaced by the code of the corresponding curve-fitting model. As for the
    unpredictable data that cannot be approximated by curve-fitting models, we
    perform an optimized lossy compression via a binary representation analysis.
    We evaluate our proposed solution using 13 real-world HPC applications
    across different scientific domains, and compare it to many other
    state-of-the-art compression methods (including Gzip, FPC, ISABELA, NUMARCK,
    ZFP, FPZIP, etc.). Experiments show that the compression ratio of our
    compressor ranges in 3.3/1 - 436/1, which is higher than the second-best
    solution ZFP by as little as 2x and as much as an order of magnitude for
    most cases. The compression time of SZ is comparable to other solutions',
    while its decompression time is less than the second best one by 50%-90%. On
    an extreme-scale use case, experiments show that the compression ratio of SZ
    exceeds that of ZFP by 80%.
  author:
    - family: Di
      given: Sheng
    - family: Cappello
      given: Franck
  citation-key: diFastErrorBoundedLossy2016
  container-title: >-
    2016 IEEE International Parallel and Distributed Processing Symposium
    (IPDPS)
  DOI: 10.1109/IPDPS.2016.11
  event-title: >-
    2016 IEEE International Parallel and Distributed Processing Symposium
    (IPDPS)
  ISSN: 1530-2075
  issued:
    - year: 2016
      month: 5
  note: 'interest: 70'
  page: 730-739
  source: IEEE Xplore
  title: Fast Error-Bounded Lossy HPC Data Compression with SZ
  type: paper-conference

- id: dingSameAsNetworksAnalyzing2010
  abstract: >-
    Millions of owl:sameAs statements have been published on the Web of Data.
    Due to its unique role and heavy usage in Linked Data integration,
    owl:sameAs has become a topic of increasing interest and debate. This paper
    provides a quantitative analysis of owl:sameAs deployment status and uses
    these statistics to focus discussion around its usage in Linked Data.
  author:
    - family: Ding
      given: Li
    - family: Shinavier
      given: Joshua
    - family: Shangguan
      given: Zhenning
    - family: McGuinness
      given: Deborah L.
  citation-key: dingSameAsNetworksAnalyzing2010
  collection-title: Lecture Notes in Computer Science
  container-title: The Semantic Web – ISWC 2010
  DOI: 10.1007/978-3-642-17746-0_10
  editor:
    - family: Patel-Schneider
      given: Peter F.
    - family: Pan
      given: Yue
    - family: Hitzler
      given: Pascal
    - family: Mika
      given: Peter
    - family: Zhang
      given: Lei
    - family: Pan
      given: Jeff Z.
    - family: Horrocks
      given: Ian
    - family: Glimm
      given: Birte
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-17746-0
  issued:
    - year: 2010
  language: en
  page: 145-160
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: >-
    SameAs Networks and Beyond: Analyzing Deployment Status and Implications of
    owl:sameAs in Linked Data
  title-short: SameAs Networks and Beyond
  type: paper-conference

- id: dinhStatisticalAssertionMore2014
  abstract: >-
    Traditional debuggers are of limited value for modern scientific codes that
    manipulate large complex data structures. Current parallel machines make
    this even more complicated, because the data structure may be distributed
    across processors, making it difficult to view/interpret and validate its
    contents. Therefore, many applications’ developers resort to placing
    validation code directly in the source program. This paper discusses a novel
    debug-time assertion, called a “Statistical Assertion”, that allows using
    extracted statistics instead of raw data to reason about large data
    structures, therefore help locating coding defects. In this paper, we
    present the design and implementation of an ‘extendable’
    statistical-framework which executes the assertion in parallel by exploiting
    the underlying parallel system. We illustrate the debugging technique with a
    molecular dynamics simulation. The performance is evaluated on a 20,000
    processor Cray XE6 to show that it is useful for real-time debugging.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Dinh
      given: Minh Ngoc
    - family: Abramson
      given: David
    - family: Jin
      given: Chao
  citation-key: dinhStatisticalAssertionMore2014
  collection-title: Empowering Science through Computing + BioInspired Computing
  container-title: Journal of Computational Science
  container-title-short: Journal of Computational Science
  DOI: 10.1016/j.jocs.2013.12.002
  ISSN: 1877-7503
  issue: '2'
  issued:
    - year: 2014
      month: 3
      day: 1
  language: en
  note: 'interest: 98'
  page: 126-134
  source: ScienceDirect
  title: >-
    Statistical assertion: A more powerful method for debugging scientific
    applications
  title-short: Statistical assertion
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S1877750313001415
  volume: '5'

- id: ditommasoNextflowEnablesReproducible2017
  accessed:
    - year: 2023
      month: 1
      day: 30
  author:
    - family: Di Tommaso
      given: Paolo
    - family: Chatzou
      given: Maria
    - family: Floden
      given: Evan W.
    - family: Barja
      given: Pablo Prieto
    - family: Palumbo
      given: Emilio
    - family: Notredame
      given: Cedric
  citation-key: ditommasoNextflowEnablesReproducible2017
  container-title: Nature Biotechnology
  container-title-short: Nat Biotechnol
  DOI: 10.1038/nbt.3820
  ISSN: 1546-1696
  issue: '4'
  issued:
    - year: 2017
      month: 4
  language: en
  license: >-
    2017 Nature Publishing Group, a division of Macmillan Publishers Limited.
    All Rights Reserved.
  number: '4'
  page: 316-319
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Nextflow enables reproducible computational workflows
  type: article-journal
  URL: https://www.nature.com/articles/nbt.3820%7B
  volume: '35'

- id: doctorowMetacrap2001
  abstract: >-
    A world of exhaustive, reliable metadata would be a utopia. It's also a
    pipe-dream, founded on self-delusion, nerd hubris and hysterically inflated
    market opportunities.
  accessed:
    - year: 2023
      month: 6
      day: 9
  author:
    - family: Doctorow
      given: Cory
  citation-key: doctorowMetacrap2001
  issued:
    - year: 2001
      month: 8
      day: 26
  title: Metacrap
  type: webpage
  URL: https://people.well.com/user/doctorow/metacrap.htm

- id: doctorowPluralisticIfBuying
  accessed:
    - year: 2023
      month: 12
      day: 13
  author:
    - family: Doctorow
      given: Corey
  citation-key: doctorowPluralisticIfBuying
  title: >-
    Pluralistic: “If buying isn’t owning, piracy isn’t stealing” (08 Dec 2023) –
    Pluralistic: Daily links from Cory Doctorow
  type: webpage
  URL: https://pluralistic.net/2023/12/08/playstationed/#tyler-james-hill

- id: dolstraPurelyFunctionalSoftware2006
  abstract: >-
    Software deployment is the set of activities related to getting software
    components to work on the machines of end users. It includes activities such
    as installation, upgrading, uninstallation, and so on. Many tools have been
    developed to support deployment, but they all have serious limitations with
    respect to correctness. For instance, the installation of a component can
    lead to the failure of previously installed components; a component might
    require other components that are not present; and it is generally difficult
    to undo deployment actions. The fundamental causes of these problems are a
    lack of isolation between components, the difficulty in identifying the
    dependencies between components, and incompatibilities between versions and
    variants of components. This thesis describes a better approach based on a
    purely functional deployment model, implemented in a deployment system
    called Nix. Components are stored in isolation from each other in a Nix
    store. Each component has a name that contains a cryptographic hash of all
    inputs that contributed to its build process, and the content of a component
    never changes after it has been built. Hence the model is purely functional.
    This storage scheme provides several important advantages. First, it ensures
    isolation between components: if two components differ in any way, they will
    be stored in different locations and will not overwrite each other. Second,
    it allows us to identify component dependencies. Undeclared build time
    dependencies are prevented due to the absence of "global" component
    directories used in other deployment systems. Runtime dependencies can be
    found by scanning for cryptographic hashes in the binary contents of
    components, a technique analogous to conservative garbage collection in
    programming language implementation. Since dependency information is
    complete, complete deployment can be performed by copying closures of
    components under the dependency relation. Developers and users are not
    confronted with components' cryptographic hashes directly. Components are
    built automatically from Nix expressions, which describe how to build and
    compose arbitrary software components; hashes are computed as part of this
    process. Components are automatically made available to users through "user
    environments", which are synthesised sets of activated components. User
    environments enable atomic upgrades and rollbacks, as well as different sets
    of activated components for different users. Nix expressions provide a
    source-based deployment model. However, source-based deployment can be
    transparently optimised into binary deployment by making pre-built binaries
    (keyed on their cryptographic hashes) available in a shared location such as
    a network server. This is referred to as transparent source/binary
    deployment. The purely functional deployment model has been validated by
    applying it to the deployment of more than 278 existing Unix packages. In
    addition, this thesis shows that the model can be applied naturally to the
    related activities of continuous integration using build farms, service
    deployment and build management.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Dolstra
      given: Eelco
  citation-key: dolstraPurelyFunctionalSoftware2006
  genre: PhD Thesis
  issued:
    - year: 2006
      month: 1
      day: 18
  language: en
  publisher: Utrecht University
  title: The Purely Functional Software Deployment Model
  type: thesis
  URL: https://dspace.library.uu.nl/handle/1874/7540

- id: dolstraSecureSharingUntrusted2005
  abstract: >-
    The Nix software deployment system is based on the paradigm of transparent
    source/binary deployment: distributors deploy descriptors that build
    components from source, while client machines can transparently optimise
    such source builds by downloading pre-built binaries from remote
    repositories. This model combines the simplicity and flexibility of source
    deployment with the efficiency of binary deployment. A desirable property is
    sharing of components: if multiple users install from the same source
    descriptors, ideally only one remotely built binary should be installed. The
    problem is that users must trust that remotely downloaded binaries were
    built from the sources they are claimed to have been built from, while users
    in general do not have a trust relation with each other or with the same
    remote repositories.This paper presents three models that enable sharing:
    the extensional model that requires that all users on a system have the same
    remote trust relations, the intensional model that does not have this
    requirement but may be suboptimal in terms of space use, and the mixed model
    that merges the best properties of both. The latter two models are achieved
    through a novel technique of hash rewriting in content-addressable component
    stores, and were implemented in the context of the Nix system.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Dolstra
      given: Eelco
  citation-key: dolstraSecureSharingUntrusted2005
  collection-title: ASE '05
  container-title: >-
    Proceedings of the 20th IEEE/ACM International Conference on Automated
    Software Engineering
  DOI: 10.1145/1101908.1101933
  event-place: New York, NY, USA
  ISBN: 978-1-58113-993-8
  issued:
    - year: 2005
      month: 11
      day: 7
  note: 'interest: 99'
  page: 154–163
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Secure sharing between untrusted users in a transparent source/binary
    deployment model
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/1101908.1101933

- id: dongSRRTARegressionTesting2020
  abstract: >-
    Regression testing is widely recognized as an important but time-consuming
    process. To alleviate this cost issue, test selection, reduction, and
    prioritization have been widely studied, and they share the commonality that
    they improve regression testing by optimizing the execution of the whole
    test suite. In this paper, we attempt to accelerate regression testing from
    a totally new perspective, i.e., skipping some execution of a new program by
    reusing program states of an old program. Following this intuition, we
    propose a state-reuse based acceleration approach SRRTA, consisting of two
    components: state storage and loading. With the former, SRRTA collects some
    program states during the execution of an old version through three
    heuristic-based storage strategies; with the latter, SRRTA loads the stored
    program states with efficiency optimization strategies. Through the
    preliminary study on commons-math, SRRTA reduces 82.7% of the regression
    testing time.
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Dong
      given: Jinhao
    - family: Lou
      given: Yiling
    - family: Hao
      given: Dan
  citation-key: dongSRRTARegressionTesting2020
  collection-title: ASE '20
  container-title: >-
    Proceedings of the 35th IEEE/ACM International Conference on Automated
    Software Engineering
  DOI: 10.1145/3324884.3418928
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6768-4
  issued:
    - year: 2020
      month: 12
      day: 21
  note: |-
    score: 60
    interest: 50
  page: 1244–1248
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'SRRTA: regression testing acceleration via state reuse'
  title-short: SRRTA
  type: paper-conference
  URL: https://doi.org/10.1145/3324884.3418928

- id: donohoReproducibleResearchComputational2009
  abstract: >-
    Scientific computation is emerging as absolutely central to the scientific
    method. Unfortunately, it's error-prone and currently immature—traditional
    scientific publication is incapable of finding and rooting out errors in
    scientific computation—which must be recognized as a crisis. An important
    recent development and a necessary response to the crisis is reproducible
    computational research in which researchers publish the article along with
    the full computational environment that produces the results. The authors
    have practiced reproducible computational research for 15 years and have
    integrated it with their scientific research and with doctoral and
    postdoctoral education. In this article, they review their approach and how
    it has evolved over time, discussing the arguments for and against working
    reproducibly.
  author:
    - family: Donoho
      given: David L.
    - family: Maleki
      given: Arian
    - family: Rahman
      given: Inam Ur
    - family: Shahram
      given: Morteza
    - family: Stodden
      given: Victoria
  citation-key: donohoReproducibleResearchComputational2009
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2009.15
  ISSN: 1558-366X
  issue: '1'
  issued:
    - year: 2009
      month: 1
  note: 'interest: 93'
  page: 8-18
  source: IEEE Xplore
  title: Reproducible Research in Computational Harmonic Analysis
  type: article-journal
  volume: '11'

- id: douglasthainTechniquesPreservingScientific2015
  accessed:
    - year: 2024
      month: 10
      day: 5
  author:
    - family: Douglas Thain
      given: Peter Ivie
  citation-key: douglasthainTechniquesPreservingScientific2015
  DOI: 10.7274/R0CZ353M
  issued:
    - year: 2015
  publisher: University of Notre Dame
  source: DOI.org (Datacite)
  title: >-
    Techniques for Preserving Scientific Software Executions: Preserve the Mess
    or Encourage Cleanliness?
  title-short: Techniques for Preserving Scientific Software Executions
  type: article
  URL: https://curate.nd.edu/show/n009w091d9n

- id: drepperHowWriteShared2011
  abstract: >-
    Today, shared libraries are ubiquitous. Developers use them for multiple
    reasons and create them just as they would create application code. This is
    a problem, though, since on many platforms some additional techniques must
    be applied even to generate decent code. Even more knowledge is needed to
    generate optimized code. This paper introduces the required rules and
    techniques. In addition, it introduces the concept of ABI (Application
    Binary Interface) stability and shows how to manage it.
  author:
    - family: Drepper
      given: Ulrich
  citation-key: drepperHowWriteShared2011
  issued:
    - year: 2011
      month: 12
      day: 10
  language: en
  title: How To Write Shared Libraries
  type: post-weblog
  URL: >-
    https://cs.dartmouth.edu/~sergey/cs258/ABI/UlrichDrepper-How-To-Write-Shared-Libraries.pdf

- id: druskatSoftwarePublicationsRich2022
  abstract: >-
    To satisfy the principles of FAIR software, software sustainability and
    software citation, research software must be formally published. Publication
    repositories make this possible and provide published software versions with
    unique and persistent identifiers. However, software publication is still a
    tedious, mostly manual process. To streamline software publication, HERMES,
    a project funded by the Helmholtz Metadata Collaboration, develops automated
    workflows to publish research software with rich metadata. The tooling
    developed by the project utilizes continuous integration solutions to
    retrieve, collate, and process existing metadata in source repositories, and
    publish them on publication repositories, including checks against existing
    metadata requirements. To accompany the tooling and enable researchers to
    easily reuse it, the project also provides comprehensive documentation and
    templates for widely used CI solutions. In this paper, we outline the
    concept for these workflows, and describe how our solution advance the state
    of the art in research software publication.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Druskat
      given: Stephan
    - family: Bertuch
      given: Oliver
    - family: Juckeland
      given: Guido
    - family: Knodel
      given: Oliver
    - family: Schlauch
      given: Tobias
  citation-key: druskatSoftwarePublicationsRich2022
  DOI: 10.48550/arXiv.2201.09015
  issued:
    - year: 2022
      month: 1
      day: 22
  note: 'interest: 74'
  number: arXiv:2201.09015
  publisher: arXiv
  source: arXiv.org
  title: >-
    Software publications with rich metadata: state of the art, automated
    workflows and HERMES concept
  title-short: Software publications with rich metadata
  type: article
  URL: http://arxiv.org/abs/2201.09015

- id: DTrace
  accessed:
    - year: 2023
      month: 8
      day: 23
  citation-key: DTrace
  language: en
  title: About DTrace
  type: post-weblog
  URL: http://dtrace.org/blogs/about/

- id: duffRcPlanShell
  abstract: |-
    Rc is a command interpreter for Plan 9 that provides similar facilities
    to UNIX's Bourne shell, with some small additions and less idiosyncratic
    syntax. This paper uses numerous examples to describe rc's features,
    and contrasts rc with the Bourne shell, a model that many readers will be
    familiar wit
  author:
    - family: Duff
      given: Tom
  citation-key: duffRcPlanShell
  title: Rc -- The Plan 9 Shell
  type: manuscript

- id: duttaFLEXFixingFlaky2021
  abstract: >-
    Many machine learning (ML) algorithms are inherently random – multiple
    executions using the same inputs may produce slightly different results each
    time. Randomness impacts how developers write tests that check for
    end-to-end quality of their implementations of these ML algorithms. In
    particular, selecting the proper thresholds for comparing obtained quality
    metrics with the reference results is a non-intuitive task, which may lead
    to flaky test executions. We present FLEX, the first tool for automatically
    fixing flaky tests due to algorithmic randomness in ML algorithms. FLEX
    fixes tests that use approximate assertions to compare actual and expected
    values that represent the quality of the outputs of ML algorithms. We
    present a technique for systematically identifying the acceptable bound
    between the actual and expected output quality that also minimizes
    flakiness. Our technique is based on the Peak Over Threshold method from
    statistical Extreme Value Theory, which estimates the tail distribution of
    the output values observed from several runs. Based on the tail
    distribution, FLEX updates the bound used in the test, or selects the number
    of test re-runs, based on a desired confidence level. We evaluate FLEX on a
    corpus of 35 tests collected from the latest versions of 21 ML projects.
    Overall, FLEX identifies and proposes a fix for 28 tests. We sent 19 pull
    requests, each fixing one test, to the developers. So far, 9 have been
    accepted by the developers.
  accessed:
    - year: 2022
      month: 10
      day: 13
  author:
    - family: Dutta
      given: Saikat
    - family: Shi
      given: August
    - family: Misailovic
      given: Sasa
  citation-key: duttaFLEXFixingFlaky2021
  collection-title: ESEC/FSE 2021
  container-title: >-
    Proceedings of the 29th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3468264.3468615
  event-place: New York, NY, USA
  ISBN: 978-1-4503-8562-6
  issued:
    - year: 2021
      month: 8
      day: 20
  note: 'interest: 97'
  page: 603–614
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    FLEX: fixing flaky tests in machine learning projects by updating assertion
    bounds
  title-short: FLEX
  type: paper-conference
  URL: https://doi.org/10.1145/3468264.3468615

- id: duttaSeedNotSeed2022
  abstract: >-
    Many Machine Learning (ML) algorithms are in-herently random in nature -
    executing them using the same inputs may lead to slightly different results
    across different runs. Such randomness makes it challenging for developers
    to write tests for their implementations of ML algorithms. A natural
    consequence of randomness is test flakiness - tests both pass and fail
    non-deterministically for same version of code. Developers often choose to
    alleviate test flakiness in ML projects by setting seeds in the random
    number generators used by the code under test. However, this approach
    commonly serves as a “workaround” rather than an actual solution. Instead,
    it may be possible to mitigate flakiness and alleviate the negative effects
    of setting seeds using alternative approaches. To understand the role of
    seeds and the feasibility of alternative solutions, we conduct the first
    large-scale empirical study of the usage of seeds and its implications on
    testing on a corpus of 114 Machine Learning projects. We identify 461 tests
    in these projects that fail without seeds and study their nature and root
    causes. We try to minimize the flakiness of a subset of 42 identified tests
    using alternative strategies such as tuning algorithm hyper-parameters and
    adjusting assertion bounds and send them to developers. So far, developers
    have accepted our fixes for 26 tests. We further manually analyze a subset
    of 56 tests and study various characteristics such as the nature of test
    oracles and how the seed settings evolve over time. Finally, we provide a
    general set of recommendations for both researchers and developers in the
    context of setting seeds in tests.
  author:
    - family: Dutta
      given: Saikat
    - family: Arunachalam
      given: Anshul
    - family: Misailovic
      given: Sasa
  citation-key: duttaSeedNotSeed2022
  container-title: 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
  DOI: 10.1109/ICST53961.2022.00026
  event-title: 2022 IEEE Conference on Software Testing, Verification and Validation (ICST)
  ISSN: 2159-4848
  issued:
    - year: 2022
      month: 4
  note: 'interest: 95'
  page: 151-161
  source: IEEE Xplore
  title: >-
    To Seed or Not to Seed? An Empirical Analysis of Usage of Seeds for Testing
    in Machine Learning Projects
  title-short: To Seed or Not to Seed?
  type: paper-conference

- id: dykstraApptainerSetuid2022
  abstract: >-
    Apptainer (formerly known as Singularity) since its beginning implemented
    many of its container features with the assistance of a setuid-root program.
    It still supports that mode, but as of version 1.1.0 it no longer uses
    setuid by default. This is feasible because it now can mount squash
    filesystems, mount ext2/3/4 filesystems, and use overlayfs using
    unprivileged user namespaces and FUSE. It also now enables unprivileged
    users to build containers, even without requiring system administrators to
    configure /etc/subuid and /etc/subgid unlike other "rootless" container
    systems. As a result, all the unprivileged functions can be used nested
    inside of another container, even if the container runtime prevents any
    elevated privileges.
  accessed:
    - year: 2023
      month: 2
      day: 18
  author:
    - family: Dykstra
      given: Dave
  citation-key: dykstraApptainerSetuid2022
  DOI: 10.48550/arXiv.2208.12106
  issued:
    - year: 2022
      month: 8
      day: 25
  number: arXiv:2208.12106
  publisher: arXiv
  source: arXiv.org
  title: Apptainer Without Setuid
  type: article
  URL: http://arxiv.org/abs/2208.12106

- id: eilersNaginiStaticVerifier2018
  abstract: >-
    We present Nagini, an automated, modular verifier for statically-typed,
    concurrent Python 3 programs, built on the Viper verification
    infrastructure. Combining established concepts with new ideas, Nagini can
    verify memory safety, functional properties, termination, deadlock freedom,
    and input/output behavior. Our experiments show that Nagini is able to
    verify non-trivial properties of real-world Python code.
  author:
    - family: Eilers
      given: Marco
    - family: Müller
      given: Peter
  citation-key: eilersNaginiStaticVerifier2018
  collection-title: Lecture Notes in Computer Science
  container-title: Computer Aided Verification
  DOI: 10.1007/978-3-319-96145-3_33
  editor:
    - family: Chockler
      given: Hana
    - family: Weissenbacher
      given: Georg
  event-place: Cham
  ISBN: 978-3-319-96145-3
  issued:
    - year: 2018
  language: en
  note: 'interest: 92'
  page: 596-603
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: 'Nagini: A Static Verifier for Python'
  title-short: Nagini
  type: paper-conference

- id: eistyQualityAssuranceResearch
  abstract: >-
    Breakthroughs in research increasingly depend on complex software libraries,
    tools, and applications aimed at supporting specific science, engineering,
    business, or humanities disciplines. Collectively, we call these software,
    libraries, tools, and applications as research software. Research software
    plays an important role in solving real-life problems, scientific
    innovations, and handling emergency situations. So the correctness and
    trustworthiness of research software are of absolute importance. The
    complexity and criticality of this software motivate the need for proper
    software quality assurance through different software engineering practices.
    Software metrics, software development process, peer code review, and
    software testing are four key tools for assessing, measuring, and ensuring
    software quality and reliability.

    The goal of this dissertation is to better understand how research software
    developers use traditional software engineering concepts of software quality
    to support and evaluate both the software and the software development
    process. One key aspect of this goal is to identify how the four quality
    practices relevant to research software corresponds to the practices
    commonly used in traditional software engineering. I used empirical software
    engineering research methods to study the human aspects related to using
    software quality practices for the development of research software. I
    collected information related to the four software activities through
    surveys, interviews, and directly working with research software developers.
    Research software developers appear to be interested and see value in
    software quality practices, but maybe encountering roadblocks when trying to
    use them. Through this dissertation, beside current practices, I identified
    challenges to use those quality practices and provided guidelines to
    overcome the challenges and to improve the current practices.
  accessed:
    - year: 2022
      month: 10
      day: 11
  author:
    - family: Eisty
      given: Nasir U.
  citation-key: eistyQualityAssuranceResearch
  event-place: United States -- Alabama
  genre: Ph.D.
  ISBN: '9798662404960'
  language: English
  license: >-
    Database copyright ProQuest LLC; ProQuest does not claim copyright in the
    individual underlying works.
  number-of-pages: '155'
  publisher: The University of Alabama
  publisher-place: United States -- Alabama
  source: ProQuest
  title: Quality Assurance in Research Software
  type: thesis
  URL: https://www.proquest.com/docview/2427487185/abstract/23F7DE57839B428APQ/1

- id: eistySurveySoftwareMetric2018
  abstract: >-
    Background: Breakthroughs in research increasingly depend on complex
    software libraries, tools, and applications aimed at supporting specific
    science, engineering, business, or humanities disciplines. The complexity
    and criticality of this software motivate the need for ensuring quality and
    reliability. Software metrics are a key tool for assessing, measuring, and
    understanding software quality and reliability. Aims: The goal of this work
    is to better understand how research software developers use traditional
    software engineering concepts, like metrics, to support and evaluate both
    the software and the software development process. One key aspect of this
    goal is to identify how the set of metrics relevant to research software
    corresponds to the metrics commonly used in traditional software
    engineering. Method: We surveyed research software developers to gather
    information about their knowledge and use of code metrics and software
    process metrics. We also analyzed the influence of demographics (project
    size, development role, and development stage) on these metrics. Results:
    The survey results, from 129 respondents, indicate that respondents have a
    general knowledge of metrics. However, their knowledge of specific SE
    metrics is lacking, their use even more limited. The most used metrics
    relate to performance and testing. Even though code complexity often poses a
    significant challenge to research software development, respondents did not
    indicate much use of code metrics. Conclusions: Research software developers
    appear to be interested and see some value in software metrics but may be
    encountering roadblocks when trying to use them. Further study is needed to
    determine the extent to which these metrics could provide value in
    continuous process improvement.
  accessed:
    - year: 2024
      month: 4
      day: 17
  author:
    - family: Eisty
      given: Nasir U.
    - family: Thiruvathukal
      given: George K.
    - family: Carver
      given: Jeffrey C.
  citation-key: eistySurveySoftwareMetric2018
  container-title: 2018 IEEE 14th International Conference on e-Science (e-Science)
  DOI: 10.1109/eScience.2018.00036
  event-title: 2018 IEEE 14th International Conference on e-Science (e-Science)
  issued:
    - year: 2018
      month: 10
  page: 212-222
  source: IEEE Xplore
  title: A Survey of Software Metric Use in Research Software Development
  type: paper-conference
  URL: https://ieeexplore.ieee.org/abstract/document/8588655

- id: eklundClusterFailureWhy2016
  abstract: >-
    The most widely used task functional magnetic resonance imaging (fMRI)
    analyses use parametric statistical methods that depend on a variety of
    assumptions. In this work, we use real resting-state data and a total of 3
    million random task group analyses to compute empirical familywise error
    rates for the fMRI software packages SPM, FSL, and AFNI, as well as a
    nonparametric permutation method. For a nominal familywise error rate of 5%,
    the parametric statistical methods are shown to be conservative for
    voxelwise inference and invalid for clusterwise inference. Our results
    suggest that the principal cause of the invalid cluster inferences is
    spatial autocorrelation functions that do not follow the assumed Gaussian
    shape. By comparison, the nonparametric permutation test is found to produce
    nominal results for voxelwise as well as clusterwise inference. These
    findings speak to the need of validating the statistical methods being used
    in the field of neuroimaging.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Eklund
      given: Anders
    - family: Nichols
      given: Thomas E.
    - family: Knutsson
      given: Hans
  citation-key: eklundClusterFailureWhy2016
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.1602413113
  issue: '28'
  issued:
    - year: 2016
      month: 7
      day: 12
  page: 7900-7905
  publisher: Proceedings of the National Academy of Sciences
  source: pnas.org (Atypon)
  title: >-
    Cluster failure: Why fMRI inferences for spatial extent have inflated
    false-positive rates
  title-short: Cluster failure
  type: article-journal
  URL: https://www.pnas.org/doi/full/10.1073/pnas.1602413113
  volume: '113'

- id: elbaumTechniquesImprovingRegression2014
  abstract: >-
    In continuous integration development environments, software engineers
    frequently integrate new or changed code with the mainline codebase. This
    can reduce the amount of code rework that is needed as systems evolve and
    speed up development time. While continuous integration processes
    traditionally require that extensive testing be performed following the
    actual submission of code to the codebase, it is also important to ensure
    that enough testing is performed prior to code submission to avoid breaking
    builds and delaying the fast feedback that makes continuous integration
    desirable. In this work, we present algorithms that make continuous
    integration processes more cost-effective. In an initial pre-submit phase of
    testing, developers specify modules to be tested, and we use regression test
    selection techniques to select a subset of the test suites for those modules
    that render that phase more cost-effective. In a subsequent post-submit
    phase of testing, where dependent modules as well as changed modules are
    tested, we use test case prioritization techniques to ensure that failures
    are reported more quickly. In both cases, the techniques we utilize are
    novel, involving algorithms that are relatively inexpensive and do not rely
    on code coverage information -- two requirements for conducting testing
    cost-effectively in this context. To evaluate our approach, we conducted an
    empirical study on a large data set from Google that we make publicly
    available. The results of our study show that our selection and
    prioritization techniques can each lead to cost-effectiveness improvements
    in the continuous integration process.
  accessed:
    - year: 2022
      month: 4
      day: 10
  author:
    - family: Elbaum
      given: Sebastian
    - family: Rothermel
      given: Gregg
    - family: Penix
      given: John
  citation-key: elbaumTechniquesImprovingRegression2014
  collection-title: FSE 2014
  container-title: >-
    Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations
    of Software Engineering
  DOI: 10.1145/2635868.2635910
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3056-5
  issued:
    - year: 2014
      month: 11
      day: 11
  page: 235–245
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Techniques for improving regression testing in continuous integration
    development environments
  type: paper-conference
  URL: https://doi.org/10.1145/2635868.2635910

- id: elsabbaghVortexOpenCLCompatible2020
  abstract: >-
    The current challenges in technology scaling are pushing the semiconductor
    industry towards hardware specialization, creating a proliferation of
    heterogeneous systems-on-chip, delivering orders of magnitude performance
    and power benefits compared to traditional general-purpose architectures.
    This transition is getting a significant boost with the advent of RISC-V
    with its unique modular and extensible ISA, allowing a wide range of
    low-cost processor designs for various target applications. In addition,
    OpenCL is currently the most widely adopted programming framework for
    heterogeneous platforms available on mainstream CPUs, GPUs, as well as FPGAs
    and custom DSP. In this work, we present Vortex, a RISC-V General-Purpose
    GPU that supports OpenCL. Vortex implements a SIMT architecture with a
    minimal ISA extension to RISC-V that enables the execution of OpenCL
    programs. We also extended OpenCL runtime framework to use the new ISA. We
    evaluate this design using 15nm technology. We also show the performance and
    energy numbers of running them with a subset of benchmarks from the Rodinia
    Benchmark suite.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Elsabbagh
      given: Fares
    - family: Tine
      given: Blaise
    - family: Roshan
      given: Priyadarshini
    - family: Lyons
      given: Ethan
    - family: Kim
      given: Euna
    - family: Shim
      given: Da Eun
    - family: Zhu
      given: Lingjun
    - family: Lim
      given: Sung Kyu
    - family: kim
      given: Hyesoon
  citation-key: elsabbaghVortexOpenCLCompatible2020
  DOI: 10.48550/arXiv.2002.12151
  issued:
    - year: 2020
      month: 2
      day: 27
  note: 'interest: 66'
  number: arXiv:2002.12151
  publisher: arXiv
  source: arXiv.org
  title: 'Vortex: OpenCL Compatible RISC-V GPGPU'
  title-short: Vortex
  type: article
  URL: http://arxiv.org/abs/2002.12151

- id: elsnerEmpiricallyEvaluatingReadily2021
  abstract: >-
    Regression test selection (RTS) and prioritization (RTP) techniques aim to
    reduce testing efforts and developer feedback time after a change to the
    code base. Using various information sources, including test traces, build
    dependencies, version control data, and test histories, they have been shown
    to be effective. However, not all of these sources are guaranteed to be
    available and accessible for arbitrary continuous integration (CI)
    environments. In contrast, metadata from version control systems (VCSs) and
    CI systems are readily available and inexpensive. Yet, corresponding RTP and
    RTS techniques are scattered across research and often only evaluated on
    synthetic faults or in a specific industrial context. It is cumbersome for
    practitioners to identify insights that apply to their context, let alone to
    calibrate associated parameters for maximum cost-effectiveness. This paper
    consolidates existing work on RTP and unsafe RTS into an actionable
    methodology to build and evaluate such approaches that exclusively rely on
    CI and VCS metadata. To investigate how these approaches from prior research
    compare in heterogeneous settings, we apply the methodology in a large-scale
    empirical study on a set of 23 projects covering 37,000 CI logs and 76,000
    VCS commits. We find that these approaches significantly outperform
    established RTP baselines and, while still triggering 90% of the failures,
    we show that practitioners can expect to save on average 84% of test
    execution time for unsafe RTS. We also find that it can be beneficial to
    limit training data, features from test history work better than
    change-based features, and, somewhat surprisingly, simple and well-known
    heuristics often outperform complex machine-learned models.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Elsner
      given: Daniel
    - family: Hauer
      given: Florian
    - family: Pretschner
      given: Alexander
    - family: Reimer
      given: Silke
  citation-key: elsnerEmpiricallyEvaluatingReadily2021
  collection-title: ISSTA 2021
  container-title: >-
    Proceedings of the 30th ACM SIGSOFT International Symposium on Software
    Testing and Analysis
  DOI: 10.1145/3460319.3464834
  event-place: New York, NY, USA
  ISBN: 978-1-4503-8459-9
  issued:
    - year: 2021
      month: 7
      day: 11
  note: 'interest: 99'
  page: 491–504
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Empirically evaluating readily available information for regression test
    optimization in continuous integration
  type: paper-conference
  URL: https://doi.org/10.1145/3460319.3464834

- id: EmpiricalSE
  abstract: >-
    Reading list  Easterbrook, S., Singer, J., Storey, M.-A., & Damian, D.
    (2008). Selecting empirical methods for software engineering research. In
    Guide to advanced empirical software engineering (pp. 285-311): Springer. 
    Wohlin, C., Runeson, P., Höst, M., Ohlsson, M. C., Regnell, B., & Wesslén,
    A....
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: EmpiricalSE
  container-title: Google Docs
  language: en
  note: 'interest: 75'
  title: Empirical SE
  type: webpage
  URL: >-
    https://docs.google.com/document/d/1DvCMwc6o4X5EmWeosNm8MM8VnCs3h5rC67ms5p9-jhc/edit?usp=embed_facebook

- id: engstromSystematicReviewRegression2010
  abstract: >-
    Regression testing is verifying that previously functioning software remains
    after a change. With the goal of finding a basis for further research in a
    joint industry-academia research project, we conducted a systematic review
    of empirical evaluations of regression test selection techniques. We
    identified 27 papers reporting 36 empirical studies, 21 experiments and 15
    case studies. In total 28 techniques for regression test selection are
    evaluated. We present a qualitative analysis of the findings, an overview of
    techniques for regression test selection and related empirical evidence. No
    technique was found clearly superior since the results depend on many
    varying factors. We identified a need for empirical studies where concepts
    are evaluated rather than small variations in technical implementations.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Engström
      given: Emelie
    - family: Runeson
      given: Per
    - family: Skoglund
      given: Mats
  citation-key: engstromSystematicReviewRegression2010
  container-title: Information and Software Technology
  container-title-short: Information and Software Technology
  DOI: 10.1016/j.infsof.2009.07.001
  ISSN: 0950-5849
  issue: '1'
  issued:
    - year: 2010
      month: 1
      day: 1
  language: en
  note: 'interst: 76'
  page: 14-30
  source: ScienceDirect
  title: A systematic review on regression test selection techniques
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0950584909001219
  volume: '52'

- id: enserinkDutchResearchFunding2021
  abstract: >-
    Hackers seeking to extort Netherlands Organisation for Scientific Research
    release confidential documents.
  accessed:
    - year: 2022
      month: 5
      day: 23
  author:
    - family: Enserink
      given: Martin
  citation-key: enserinkDutchResearchFunding2021
  issued:
    - year: 2021
      month: 2
      day: 25
  language: en
  title: >-
    Dutch research funding agency, paralyzed by ransomware attack, refuses to
    pay up
  type: article-newspaper
  URL: >-
    https://www.science.org/content/article/dutch-research-funding-agency-paralyzed-ransomware-attack-refuses-pay

- id: ernstDaikonSystemDynamic2007
  abstract: >-
    Daikon is an implementation of dynamic detection of likely invariants; that
    is, the Daikon invariant detector reports likely program invariants. An
    invariant is a property that holds at a certain point or points in a
    program; these are often used in assert statements, documentation, and
    formal specifications. Examples include being constant (x=a), non-zero
    (x≠0), being in a range (a≤x≤b), linear relationships (y=ax+b), ordering
    (x≤y), functions from a library (x=fn(y)), containment (x∈y), sortedness
    (xissorted), and many more. Users can extend Daikon to check for additional
    invariants. Dynamic invariant detection runs a program, observes the values
    that the program computes, and then reports properties that were true over
    the observed executions. Dynamic invariant detection is a machine learning
    technique that can be applied to arbitrary data. Daikon can detect
    invariants in C, C++, Java, and Perl programs, and in record-structured data
    sources; it is easy to extend Daikon to other applications. Invariants can
    be useful in program understanding and a host of other applications.
    Daikon’s output has been used for generating test cases, predicting
    incompatibilities in component integration, automating theorem proving,
    repairing inconsistent data structures, and checking the validity of data
    streams, among other tasks. Daikon is freely available in source and binary
    form, along with extensive documentation, at
    http://pag.csail.mit.edu/daikon/.
  accessed:
    - year: 2022
      month: 8
      day: 26
  author:
    - family: Ernst
      given: Michael D.
    - family: Perkins
      given: Jeff H.
    - family: Guo
      given: Philip J.
    - family: McCamant
      given: Stephen
    - family: Pacheco
      given: Carlos
    - family: Tschantz
      given: Matthew S.
    - family: Xiao
      given: Chen
  citation-key: ernstDaikonSystemDynamic2007
  collection-title: Special issue on Experimental Software and Toolkits
  container-title: Science of Computer Programming
  container-title-short: Science of Computer Programming
  DOI: 10.1016/j.scico.2007.01.015
  ISSN: 0167-6423
  issue: '1'
  issued:
    - year: 2007
      month: 12
      day: 1
  language: en
  note: 'interest: 64'
  page: 35-45
  source: ScienceDirect
  title: The Daikon system for dynamic detection of likely invariants
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S016764230700161X
  volume: '69'

- id: erxlebenIntroducingWikidataLinked2014
  abstract: >-
    Wikidata is the central data management platform of Wikipedia. By the
    efforts of thousands of volunteers, the project has produced a large, open
    knowledge base with many interesting applications. The data is highly
    interlinked and connected to many other datasets, but it is also very rich,
    complex, and not available in RDF. To address this issue, we introduce new
    RDF exports that connect Wikidata to the Linked Data Web. We explain the
    data model of Wikidata and discuss its encoding in RDF. Moreover, we
    introduce several partial exports that provide more selective or simplified
    views on the data. This includes a class hierarchy and several other types
    of ontological axioms that we extract from the site. All datasets we discuss
    here are freely available online and updated regularly.
  author:
    - family: Erxleben
      given: Fredo
    - family: Günther
      given: Michael
    - family: Krötzsch
      given: Markus
    - family: Mendez
      given: Julian
    - family: Vrandečić
      given: Denny
  citation-key: erxlebenIntroducingWikidataLinked2014
  collection-title: Lecture Notes in Computer Science
  container-title: The Semantic Web – ISWC 2014
  DOI: 10.1007/978-3-319-11964-9_4
  editor:
    - family: Mika
      given: Peter
    - family: Tudorache
      given: Tania
    - family: Bernstein
      given: Abraham
    - family: Welty
      given: Chris
    - family: Knoblock
      given: Craig
    - family: Vrandečić
      given: Denny
    - family: Groth
      given: Paul
    - family: Noy
      given: Natasha
    - family: Janowicz
      given: Krzysztof
    - family: Goble
      given: Carole
  event-place: Cham
  ISBN: 978-3-319-11964-9
  issued:
    - year: 2014
  language: en
  page: 50-65
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: Introducing Wikidata to the Linked Data Web
  type: paper-conference

- id: EvaluationAntiPatterns
  accessed:
    - year: 2022
      month: 7
      day: 6
  citation-key: EvaluationAntiPatterns
  container-title: Evaluate Collaboratory
  title: Evaluation Anti-Patterns
  type: webpage
  URL: http://evaluate.inf.usi.ch/anti-patterns

- id: EventTracingWin322021
  abstract: >-
    This documentation is for user-mode applications that want to use ETW. For
    information about instrumenting device drivers that run in kernel mode, see
    WPP Software Tracing and Adding Event Tracing to Kernel-Mode Drivers in the
    Windows Driver Kit (WDK).
  accessed:
    - year: 2023
      month: 8
      day: 23
  citation-key: EventTracingWin322021
  issued:
    - year: 2021
      month: 1
      day: 7
  language: en-us
  title: Event Tracing - Win32 apps
  type: webpage
  URL: https://learn.microsoft.com/en-us/windows/win32/etw/event-tracing-portal

- id: ewelsNfcoreFrameworkCommunitycurated2020
  accessed:
    - year: 2022
      month: 10
      day: 31
  author:
    - family: Ewels
      given: Philip A.
    - family: Peltzer
      given: Alexander
    - family: Fillinger
      given: Sven
    - family: Patel
      given: Harshil
    - family: Alneberg
      given: Johannes
    - family: Wilm
      given: Andreas
    - family: Garcia
      given: Maxime Ulysse
    - family: Di Tommaso
      given: Paolo
    - family: Nahnsen
      given: Sven
  citation-key: ewelsNfcoreFrameworkCommunitycurated2020
  container-title: Nature Biotechnology
  container-title-short: Nat Biotechnol
  DOI: 10.1038/s41587-020-0439-x
  ISSN: 1546-1696
  issue: '3'
  issued:
    - year: 2020
      month: 3
  language: en
  license: 2020 The Author(s), under exclusive licence to Springer Nature America, Inc.
  number: '3'
  page: 276-278
  publisher: Nature Publishing Group
  source: www.nature.com
  title: The nf-core framework for community-curated bioinformatics pipelines
  type: article-journal
  URL: https://www.nature.com/articles/s41587-020-0439-x
  volume: '38'

- id: fadolalkarimPANDDEProvenancebasedANomaly2016
  abstract: >-
    Preventing data exﬁltration by insiders is a challenging process since
    insiders are users that have access permissions to the data. Existing
    mechanisms focus on tracking users’ activities while they are connected to
    the database, and are unable to detect anomalous actions that the users
    perform on the data once they gain access to it. Being able to detect
    anomalous actions on the data is critical as these actions are often sign of
    attempts to misuse data. In this paper, we propose an approach to detect
    anomalous actions executed on data returned to the users from a database.
    The approach has been implemented as part of the Provenancebased ANomaly
    Detection of Data Exﬁltration (PANDDE) tool. PANDDE leverages data
    provenance information captured at the operating system level. Such
    information is then used to create proﬁles of users’ actions on the data
    once retrieved from the database. The proﬁles indicate actions that are
    consistent with the tasks of the users. Actions recorded in the proﬁles
    include data printing, emailing, and storage. Proﬁles are then used at
    run-time to detect anomalous actions.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Fadolalkarim
      given: Daren
    - family: Sallam
      given: Asmaa
    - family: Bertino
      given: Elisa
  citation-key: fadolalkarimPANDDEProvenancebasedANomaly2016
  collection-title: CODASPY '16
  container-title: >-
    Proceedings of the Sixth ACM Conference on Data and Application Security and
    Privacy
  DOI: 10.1145/2857705.2857710
  event-place: New Orleans Louisiana USA
  event-title: >-
    CODASPY'16: Sixth ACM Conference on Data and Application Security and
    Privacy
  ISBN: 978-1-4503-3935-3
  issued:
    - year: 2016
      month: 3
      day: 9
  language: en
  page: 267-276
  publisher: Association for Computing Machinery
  publisher-place: New Orleans Louisiana USA
  source: ACM Digital Library
  title: 'PANDDE: Provenance-based ANomaly Detection of Data Exfiltration'
  title-short: PANDDE
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2857705.2857710

- id: fanelliHowManyScientists2009
  abstract: >-
    The frequency with which scientists fabricate and falsify data, or commit
    other forms of scientific misconduct is a matter of controversy. Many
    surveys have asked scientists directly whether they have committed or know
    of a colleague who committed research misconduct, but their results appeared
    difficult to compare and synthesize. This is the first meta-analysis of
    these surveys. To standardize outcomes, the number of respondents who
    recalled at least one incident of misconduct was calculated for each
    question, and the analysis was limited to behaviours that distort scientific
    knowledge: fabrication, falsification, “cooking” of data, etc… Survey
    questions on plagiarism and other forms of professional misconduct were
    excluded. The final sample consisted of 21 surveys that were included in the
    systematic review, and 18 in the meta-analysis. A pooled weighted average of
    1.97% (N = 7, 95%CI: 0.86–4.45) of scientists admitted to have fabricated,
    falsified or modified data or results at least once –a serious form of
    misconduct by any standard– and up to 33.7% admitted other questionable
    research practices. In surveys asking about the behaviour of colleagues,
    admission rates were 14.12% (N = 12, 95% CI: 9.91–19.72) for falsification,
    and up to 72% for other questionable research practices. Meta-regression
    showed that self reports surveys, surveys using the words “falsification” or
    “fabrication”, and mailed surveys yielded lower percentages of misconduct.
    When these factors were controlled for, misconduct was reported more
    frequently by medical/pharmacological researchers than others. Considering
    that these surveys ask sensitive questions and have other limitations, it
    appears likely that this is a conservative estimate of the true prevalence
    of scientific misconduct.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Fanelli
      given: Daniele
  citation-key: fanelliHowManyScientists2009
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10.1371/journal.pone.0005738
  ISSN: 1932-6203
  issue: '5'
  issued:
    - year: 2009
      month: 5
      day: 29
  language: en
  page: e5738
  publisher: Public Library of Science
  source: PLoS Journals
  title: >-
    How Many Scientists Fabricate and Falsify Research? A Systematic Review and
    Meta-Analysis of Survey Data
  title-short: How Many Scientists Fabricate and Falsify Research?
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0005738
  volume: '4'

- id: farhoodiDevelopmentScientificSoftware2013
  abstract: >-
    Scientific and engineering research is heavily dependent on effective
    development and use of software artifacts. Many of these artifacts are
    produced by the scientists themselves, rather than by trained software
    engineers. To address the challenges in this area, a research community
    often referred to as "Development of Scientific Software" has emerged in the
    last few decades. As this research area has matured, there has been a sharp
    increase in the number of papers and results made available, and it has thus
    become important to summarize and provide an overview about those studies.
    Through a systematic mapping and bibliometrics study, we have reviewed 130
    papers in this area. We present the results of our study in this paper. Also
    we have made the mapping data available on an online repository which is
    planned to be updated on a regular basis. The results of our study seem to
    suggest that many software engineering techniques and activities are being
    used in the development of scientific software. However, there is still a
    need for further exploration of the usefulness of specific software
    engineering techniques (e.g., regarding software maintenance, evolution,
    refactoring, re(v)-engineering, process and project management) in the
    scientific context. It is hoped that this article will help (new)
    researchers get an overview of the research space and help them to
    understand the trends in the area.
  accessed:
    - year: 2022
      month: 6
      day: 6
  author:
    - family: Farhoodi
      given: Roshanak
    - family: Garousi
      given: Vahid
    - family: Pfahl
      given: Dietmar
    - family: Sillito
      given: Jonathan
  citation-key: farhoodiDevelopmentScientificSoftware2013
  container-title: International Journal of Software Engineering and Knowledge Engineering
  container-title-short: Int. J. Soft. Eng. Knowl. Eng.
  DOI: 10.1142/S0218194013500137
  ISSN: 0218-1940, 1793-6403
  issue: '04'
  issued:
    - year: 2013
      month: 5
  language: en
  page: 463-506
  source: DOI.org (Crossref)
  title: >-
    Development of Scientific Software: a Systematic Mapping, a Bibliometric
    Study, and a Paper Repository
  title-short: DEVELOPMENT OF SCIENTIFIC SOFTWARE
  type: article-journal
  URL: https://www.worldscientific.com/doi/abs/10.1142/S0218194013500137
  volume: '23'

- id: faulkScientificComputingProductivity2009
  abstract: >-
    Hardware improvements do little to improve real productivity in scientific
    programming. Indeed, the dominant barriers to productivity improvement are
    now in the software processes. To break the gridlock, we must establish a
    degree of cooperation and collaboration with the software engineering
    community that does not yet exist.
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Faulk
      given: Stuart
    - family: Loh
      given: Eugene
    - family: Vanter
      given: Michael L. Van De
    - family: Squires
      given: Susan
    - family: Votta
      given: Lawrence G.
  citation-key: faulkScientificComputingProductivity2009
  container-title: Computing in Science & Engineering
  container-title-short: Comput. Sci. Eng.
  DOI: 10.1109/MCSE.2009.205
  ISSN: 1521-9615
  issue: '6'
  issued:
    - year: 2009
      month: 11
  note: 'interest: 95'
  page: 30-39
  source: DOI.org (Crossref)
  title: >-
    Scientific Computing's Productivity Gridlock: How Software Engineering Can
    Help
  title-short: Scientific Computing's Productivity Gridlock
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/5337642/
  volume: '11'

- id: feamsterHowWriteWinning2020
  abstract: >-
    A project proposal needs to answer three important questions: Why important,
    why now, and why you?
  accessed:
    - year: 2023
      month: 1
      day: 20
  author:
    - family: Feamster
      given: Nick
  citation-key: feamsterHowWriteWinning2020
  container-title: Great Research
  issued:
    - year: 2020
      month: 7
      day: 26
  language: en
  title: How to Write a Winning Project Proposal
  type: post-weblog
  URL: >-
    https://medium.com/great-research/how-to-write-a-winning-project-proposal-fe438d4dc3a9

- id: felleisenExpressivePowerProgramming1991
  abstract: >-
    The literature on programming languages contains an abundance of informal
    claims on the relative expressive power of programming languages, but there
    is no framework for formalizing such statements nor for deriving interesting
    consequences. As a first step in this direction, we develop a formal notion
    of expressiveness and investigate its properties. To validate the theory, we
    analyze some widely held beliefs about the expressive power of several
    extensions of functional languages. Based on these results, we believe that
    our system correctly captures many of the informal ideas on expressiveness,
    and that it constitutes a foundation for further research in this direction.
  accessed:
    - year: 2023
      month: 7
      day: 7
  author:
    - family: Felleisen
      given: Matthias
  citation-key: felleisenExpressivePowerProgramming1991
  container-title: Science of Computer Programming
  container-title-short: Science of Computer Programming
  DOI: 10.1016/0167-6423(91)90036-W
  ISSN: 0167-6423
  issue: '1'
  issued:
    - year: 1991
      month: 12
      day: 1
  language: en
  page: 35-75
  source: ScienceDirect
  title: On the expressive power of programming languages
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/016764239190036W
  volume: '17'

- id: ferreiradasilvaCharacterizationWorkflowManagement2017
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Filgueira
      given: Rosa
    - family: Pietri
      given: Ilia
    - family: Jiang
      given: Ming
    - family: Sakellariou
      given: Rizos
    - family: Deelman
      given: Ewa
  citation-key: ferreiradasilvaCharacterizationWorkflowManagement2017
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.02.026
  ISSN: 0167739X
  issued:
    - year: 2017
      month: 10
  language: en
  page: 228-238
  source: DOI.org (Crossref)
  title: >-
    A characterization of workflow management systems for extreme-scale
    applications
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0167739X17302510
  volume: '75'

- id: ferreiradasilvaCommunityRoadmapScientific2021
  abstract: >-
    The landscape of workflow systems for scientific applications is notoriously
    convoluted with hundreds of seemingly equivalent workflow systems, many
    isolated research claims, and a steep learning curve. To address some of
    these challenges and lay the groundwork for transforming workflows research
    and development, the WorkflowsRI and ExaWorks projects partnered to bring
    the international workflows community together. This paper reports on
    discussions and findings from two virtual "Workflows Community Summits"
    (January and April, 2021). The overarching goals of these workshops were to
    develop a view of the state of the art, identify crucial research challenges
    in the workflows community, articulate a vision for potential community
    efforts, and discuss technical approaches for realizing this vision. To this
    end, participants identified six broad themes: FAIR computational workflows;
    AI workflows; exascale challenges; APIs, interoperability, reuse, and
    standards; training and education; and building a workflows community. We
    summarize discussions and recommendations for each of these themes.
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Casanova
      given: Henri
    - family: Chard
      given: Kyle
    - family: Altintas
      given: Ilkay
    - family: Badia
      given: Rosa M
    - family: Balis
      given: Bartosz
    - family: Coleman
      given: Taina
    - family: Coppens
      given: Frederik
    - family: Di Natale
      given: Frank
    - family: Enders
      given: Bjoern
    - family: Fahringer
      given: Thomas
    - family: Filgueira
      given: Rosa
    - family: Fursin
      given: Grigori
    - family: Garijo
      given: Daniel
    - family: Goble
      given: Carole
    - family: Howell
      given: Dorran
    - family: Jha
      given: Shantenu
    - family: Katz
      given: Daniel S.
    - family: Laney
      given: Daniel
    - family: Leser
      given: Ulf
    - family: Malawski
      given: Maciej
    - family: Mehta
      given: Kshitij
    - family: Pottier
      given: Loic
    - family: Ozik
      given: Jonathan
    - family: Peterson
      given: J. Luc
    - family: Ramakrishnan
      given: Lavanya
    - family: Soiland-Reyes
      given: Stian
    - family: Thain
      given: Douglas
    - family: Wolf
      given: Matthew
  citation-key: ferreiradasilvaCommunityRoadmapScientific2021
  container-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  DOI: 10.1109/WORKS54523.2021.00016
  event-place: St. Louis, MO, USA
  event-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  ISBN: 978-1-66541-136-3
  issued:
    - year: 2021
      month: 11
  note: 'interest: 90'
  page: 81-90
  publisher: IEEE
  publisher-place: St. Louis, MO, USA
  source: DOI.org (Crossref)
  title: A Community Roadmap for Scientific Workflows Research and Development
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/9652570/

- id: ferreiradasilvaWorkflowHubCommunityFramework2020
  abstract: >-
    Scientific workflows are a cornerstone of modern scientific computing. They
    are used to describe complex computational applications that require
    efficient and robust management of large volumes of data, which are
    typically stored/processed on heterogeneous, distributed resources. The
    workflow research and development community has employed a number of methods
    for the quantitative evaluation of existing and novel workflow algorithms
    and systems. In particular, a common approach is to simulate workflow
    executions. In previous work, we have presented a collection of tools that
    have been used for aiding research and development activities in the Pegasus
    project, and that have been adopted by others for conducting workflow
    research. Despite their popularity, there are several shortcomings that
    prevent easy adoption, maintenance, and consistency with the evolving
    structures and computational requirements of production workflows. In this
    work, we present WorkflowHub, a community framework that provides a
    collection of tools for analyzing workflow execution traces, producing
    realistic synthetic workflow traces, and simulating workflow executions. We
    demonstrate the realism of the generated synthetic traces by comparing
    simulated executions of these traces with actual workflow executions. We
    also contrast these results with those obtained when using the previously
    available collection of tools. We find that our framework not only can be
    used to generate representative synthetic workflow traces (i.e., with
    workflow structures and task characteristics distributions that resemble
    those in traces obtained from real-world workflow executions), but can also
    generate representative workflow traces at larger scales than that of
    available workflow traces.
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Pottier
      given: Loïc
    - family: Coleman
      given: Tainã
    - family: Deelman
      given: Ewa
    - family: Casanova
      given: Henri
  citation-key: ferreiradasilvaWorkflowHubCommunityFramework2020
  container-title: 2020 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)
  DOI: 10.1109/WORKS51914.2020.00012
  event-place: Georgia, USA
  event-title: 2020 IEEE/ACM Workflows in Support of Large-Scale Science (WORKS)
  issued:
    - year: 2020
      month: 11
  page: 49-56
  publisher: IEEE
  publisher-place: Georgia, USA
  source: IEEE Xplore
  title: >-
    WorkflowHub: Community Framework for Enabling Scientific Workflow Research
    and Development
  title-short: WorkflowHub
  type: paper-conference

- id: ferreiradasilvaWorkflowsCommunityInitiative
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Casanova
      given: Henri
    - family: Chard
      given: Kyle
    - family: Initiative
      given: Workflows Community
  citation-key: ferreiradasilvaWorkflowsCommunityInitiative
  container-title: Workflows Community Initiative
  language: en
  title: Workflows Community Initiative
  type: webpage
  URL: https://workflows.community/

- id: ferreiradasilvaWorkflowsCommunitySummit2021
  abstract: "Scientific workflows are a cornerstone of modern scientific computing, and they have underpinned some of the most significant discoveries of the last decade. Many of these workflows have high computational, storage, and/or communication demands, and thus must execute on a wide range of large-scale platforms, from large clouds to upcoming exascale HPC platforms. Workflows will play a crucial role in the data-oriented and post-Moore’s computing landscape as they democratize the application of cutting-edge research techniques, computationally intensive methods, and use of new computing platforms. As workflows continue to be adopted by scientific projects and user communities, they are becoming more complex. Workflows are increasingly composed of tasks that perform computations such as short machine learning inference, multi-node simulations, long-running machine learning model training, amongst others, and thus increasingly rely on heterogeneous architectures that include CPUs but also GPUs and accelerators. The workflow management system (WMS) technology landscape is currently segmented and presents significant barriers to entry due to the hundreds of seemingly comparable, yet incompatible, systems that exist. Another fundamental problem is that there are conflicting theoretical bases and abstractions for a WMS. Systems that use the same underlying abstractions can likely be translated between, which is not the case for systems that use different abstractions. More information:\_https://workflowsri.org/summits/technical"
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Casanova
      given: Henri
    - family: Chard
      given: Kyle
    - family: Coleman
      given: Tainã
    - family: Laney
      given: Dan
    - family: Ahn
      given: Dong
    - family: Jha
      given: Shantenu
    - family: Howell
      given: Dorran
    - family: Soiland-Reys
      given: Stian
    - family: Altintas
      given: Ilkay
    - family: Thain
      given: Douglas
    - family: Filgueira
      given: Rosa
    - family: Babuji
      given: Yadu
    - family: Badia
      given: Rosa M.
    - family: Balis
      given: Bartosz
    - family: Caino-Lores
      given: Silvina
    - family: Callaghan
      given: Scott
    - family: Coppens
      given: Frederik
    - family: Crusoe
      given: Michael R.
    - family: De
      given: Kaushik
    - family: Di Natale
      given: Frank
    - family: Do
      given: Tu M. A.
    - family: Enders
      given: Bjoern
    - family: Fahringer
      given: Thomas
    - family: Fouilloux
      given: Anne
    - family: Fursin
      given: Grigori
    - family: Gaignard
      given: Alban
    - family: Ganose
      given: Alex
    - family: Garijo
      given: Daniel
    - family: Gesing
      given: Sandra
    - family: Goble
      given: Carole
    - family: Hasan
      given: Adil
    - family: Huber
      given: Sebastiaan
    - family: Katz
      given: Daniel S.
    - family: Leser
      given: Ulf
    - family: Lowe
      given: Douglas
    - family: Ludaescher
      given: Bertram
    - family: Maheshwari
      given: Ketan
    - family: Malawski
      given: Maciej
    - family: Mayani
      given: Rajiv
    - family: Mehta
      given: Kshitij
    - family: Merzky
      given: Andre
    - family: Munson
      given: Todd
    - family: Ozik
      given: Jonathan
    - family: Pottier
      given: Loïc
    - family: Ristov
      given: Sashko
    - family: Roozmeh
      given: Mehdi
    - family: Souza
      given: Renan
    - family: Suter
      given: Frédéric
    - family: Tovar
      given: Benjamin
    - family: Turilli
      given: Matteo
    - family: Vahi
      given: Karan
    - family: Vidal-Torreira
      given: Alvaro
    - family: Whitcup
      given: Wendy
    - family: Wilde
      given: Michael
    - family: Williams
      given: Alan
    - family: Wolf
      given: Matthew
    - family: Wozniak
      given: Justin
  citation-key: ferreiradasilvaWorkflowsCommunitySummit2021
  DOI: 10.5281/zenodo.4915801
  issued:
    - year: 2021
      month: 6
      day: 9
  language: eng
  publisher: Zenodo
  source: Zenodo
  title: >-
    Workflows Community Summit: Advancing the State-of-the-art of Scientific
    Workflows Management Systems Research and Development
  title-short: Workflows Community Summit
  type: report
  URL: https://zenodo.org/record/4915801

- id: ferreiradasilvaWorkflowsCommunitySummit2021a
  abstract: "The importance of workflows is highlighted by the fact that they have underpinned some of the most significant discoveries of the past decades.\_Many of these workflows have significant computational, storage, and communication demands, and thus must execute on a range of large-scale computer systems, from local clusters to public clouds and upcoming exascale HPC platforms.\_Historically, infrastructures for workflow execution consisted of complex, integrated systems, developed in-house by workflow practitioners with strong dependencies on a range of legacy technologies.\_Due to the increasing need to support workflows, dedicated workflow systems were developed to provide abstractions for creating, executing, and adapting workflows conveniently and efficiently while ensuring portability. \_While these efforts are all worthwhile individually, there are now hundreds of independent workflow systems.\_The resulting workflow system technology landscape is fragmented, which may present significant barriers for future workflow users due to many seemingly comparable, yet usually mutually incompatible, systems that exist.\_In order to tackle some of these challenges, the DOE-funded ExaWorks\_and NSF-funded WorkflowsRI\_projects have organized in 2021 a series of events entitled the \"Workflows Community Summit\". The third edition of the ``Workflows Community Summit\" explored workflows challenges and opportunities from the perspective of computing centers and facilities.\_The third summit brought together a small group of facilities representatives with the aim to understand how workflows are currently being used at each facility, how facilities would like to interact with workflow developers and users, how workflows fit with facility roadmaps, and what opportunities there are for tighter integration between facilities and workflows. More information at:\_https://workflowsri.org/summits/facilities/"
  accessed:
    - year: 2022
      month: 6
      day: 28
  author:
    - family: Ferreira da Silva
      given: Rafael
    - family: Chard
      given: Kyle
    - family: Casanova
      given: Henri
    - family: Laney
      given: Dan
    - family: Ahn
      given: Dong
    - family: Jha
      given: Shantenu
    - family: Allcock
      given: William E.
    - family: Bauer
      given: Gregory
    - family: Duplyakin
      given: Dmitry
    - family: Enders
      given: Bjoern
    - family: Heer
      given: Todd M.
    - family: Lançon
      given: Eric
    - family: Sanielevici
      given: Sergiu
    - family: Sayers
      given: Kevin
  citation-key: ferreiradasilvaWorkflowsCommunitySummit2021a
  DOI: 10.2172/1842590
  issued:
    - year: 2021
      month: 11
      day: 8
  language: English
  number: ORNL/TM-2022/1832
  publisher: Oak Ridge National Lab. (ORNL), Oak Ridge, TN (United States)
  source: www.osti.gov
  title: >-
    Workflows Community Summit: Tightening the Integration between Computing
    Facilities and Scientific Workflows
  title-short: Workflows Community Summit
  type: report
  URL: https://www.osti.gov/biblio/1842590

- id: fienbergWhenDidBayesian2006
  abstract: >-
    While Bayes' theorem has a 250-year history, and the method of inverse
    probability that flowed from it dominated statistical thinking into the
    twentieth century, the adjective "Bayesian" was not part of the statistical
    lexicon until relatively recently. This paper provides an overview of key
    Bayesian developments, beginning with Bayes' posthumously published 1763
    paper and continuing up through approximately 1970, including the period of
    time when "Bayesian" emerged as the label of choice for those who advocated
    Bayesian methods.
  accessed:
    - year: 2022
      month: 9
      day: 9
  author:
    - family: Fienberg
      given: Stephen E.
  citation-key: fienbergWhenDidBayesian2006
  container-title: Bayesian Analysis
  DOI: 10.1214/06-BA101
  ISSN: 1936-0975, 1931-6690
  issue: '1'
  issued:
    - year: 2006
      month: 3
  note: 'interest: 83'
  page: 1-40
  publisher: International Society for Bayesian Analysis
  source: Project Euclid
  title: When did Bayesian inference become "Bayesian"?
  type: article-journal
  URL: >-
    https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-1/When-did-Bayesian-inference-become-Bayesian/10.1214/06-BA101.full
  volume: '1'

- id: filgueiraInspect4pyKnowledgeExtraction2022
  abstract: >-
    This work presents inspect4py, a static code analysis framework designed to
    automatically extract the main features, metadata and documentation of
    Python code repositories. Given an input folder with code, inspect4py uses
    abstract syntax trees and state of the art tools to find all functions,
    classes, tests, documentation, call graphs, module dependencies and control
    flows within all code files in that repository. Using these findings,
    inspect4py infers different ways of invoking a software component. We have
    evaluated our framework on 95 annotated repositories, obtaining promising
    results for software type classification (over 95% F1-score). With
    inspect4py, we aim to ease the understandability and adoption of software
    repositories by other researchers and developers. Code:
    https://github.com/SoftwareUnderstanding/inspect4py DOI:
    https://doi.org/10.5281/zenodo.5907936 License: Open (BSD3-Clause)
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Filgueira
      given: Rosa
    - family: Garijo
      given: Daniel
  citation-key: filgueiraInspect4pyKnowledgeExtraction2022
  collection-title: MSR '22
  container-title: >-
    Proceedings of the 19th International Conference on Mining Software
    Repositories
  DOI: 10.1145/3524842.3528497
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9303-4
  issued:
    - year: 2022
      month: 10
      day: 17
  note: 'interest: 90'
  page: 232–236
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Inspect4py: a knowledge extraction framework for python code repositories'
  title-short: Inspect4py
  type: paper-conference
  URL: https://doi.org/10.1145/3524842.3528497

- id: fishbeinBeliefAttitudeIntention1975
  author:
    - family: Fishbein
      given: Martin
    - family: Ajzen
      given: Icek
  call-number: BF323.C5 F48
  citation-key: fishbeinBeliefAttitudeIntention1975
  collection-title: Addison-Wesley series in social psychology
  event-place: Reading, Mass
  ISBN: 978-0-201-02089-2
  issued:
    - year: 1975
  number-of-pages: '578'
  publisher: Addison-Wesley Pub. Co
  publisher-place: Reading, Mass
  source: Library of Congress ISBN
  title: >-
    Belief, attitude, intention, and behavior: an introduction to theory and
    research
  title-short: Belief, attitude, intention, and behavior
  type: book

- id: fogelProducingOpenSource2022
  accessed:
    - year: 2023
      month: 2
      day: 24
  author:
    - family: Fogel
      given: Karl
  citation-key: fogelProducingOpenSource2022
  issued:
    - year: 2022
      month: 12
      day: 19
  note: 'interest: 50'
  title: >-
    Producing Open Source Software: How to Run a Successful Free Software
    Project
  type: book
  URL: https://producingoss.com/en/producingoss.html

- id: folkOverviewHDF5Technology2011
  abstract: >-
    In this paper, we give an overview of the HDF5 technology suite and some of
    its applications. We discuss the HDF5 data model, the HDF5 software
    architecture and some of its performance enhancing capabilities.
  accessed:
    - year: 2023
      month: 2
      day: 6
  author:
    - family: Folk
      given: Mike
    - family: Heber
      given: Gerd
    - family: Koziol
      given: Quincey
    - family: Pourmal
      given: Elena
    - family: Robinson
      given: Dana
  citation-key: folkOverviewHDF5Technology2011
  collection-title: AD '11
  container-title: Proceedings of the EDBT/ICDT 2011 Workshop on Array Databases
  DOI: 10.1145/1966895.1966900
  event-place: New York, NY, USA
  ISBN: 978-1-4503-0614-0
  issued:
    - year: 2011
      month: 3
      day: 25
  page: 36–47
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: An overview of the HDF5 technology suite and its applications
  type: paper-conference
  URL: https://doi.org/10.1145/1966895.1966900

- id: fosterChimeraVirtualData2002
  abstract: >-
    A lot of scientific data is not obtained from measurements but rather
    derived from other data by the application of computational procedures. We
    hypothesize that explicit representation of these procedures can enable
    documentation of data provenance, discovery of available methods, and
    on-demand data generation (so-called "virtual data"). To explore this idea,
    we have developed the Chimera virtual data system, which combines a virtual
    data catalog for representing data derivation procedures and derived data,
    with a virtual data language interpreter that translates user requests into
    data definition and query operations on the database. We couple the Chimera
    system with distributed "data grid" services to enable on-demand execution
    of computation schedules constructed from database queries. We have applied
    this system to two challenge problems, the reconstruction of simulated
    collision event data from a high-energy physics experiment, and searching
    digital sky survey data for galactic clusters, with promising results.
  accessed:
    - year: 2024
      month: 1
      day: 21
  author:
    - family: Foster
      given: I.
    - family: Vockler
      given: J.
    - family: Wilde
      given: M.
    - family: Zhao
      given: Yong
  citation-key: fosterChimeraVirtualData2002
  container-title: >-
    Proceedings 14th International Conference on Scientific and Statistical
    Database Management
  DOI: 10.1109/SSDM.2002.1029704
  event-title: >-
    Proceedings 14th International Conference on Scientific and Statistical
    Database Management
  ISSN: 1099-3371
  issued:
    - year: 2002
      month: 7
  page: 37-46
  source: IEEE Xplore
  title: >-
    Chimera: a virtual data system for representing, querying, and automating
    data derivation
  title-short: Chimera
  type: paper-conference
  URL: https://ieeexplore.ieee.org/abstract/document/1029704

- id: fouladiLaptopLambdaOutsourcing
  abstract: >-
    We present gg, a framework and a set of command-line tools that helps people
    execute everyday applications—e.g., software compilation, unit tests, video
    encoding, or object recognition—using thousands of parallel threads on a
    cloudfunctions service to achieve near-interactive completion times. In the
    future, instead of running these tasks on a laptop, or keeping a warm
    cluster running in the cloud, users might push a button that spawns 10,000
    parallel cloud functions to execute a large job in a few seconds from start.
    gg is designed to make this practical and easy.
  author:
    - family: Fouladi
      given: Sadjad
    - family: Romero
      given: Francisco
    - family: Iter
      given: Dan
    - family: Li
      given: Qian
    - family: Chatterjee
      given: Shuvo
  citation-key: fouladiLaptopLambdaOutsourcing
  language: en
  note: 'interest: 97'
  source: Zotero
  title: >-
    From Laptop to Lambda: Outsourcing Everyday Jobs to Thousands of Transient
    Functional Containers
  type: article-journal

- id: freireProvenanceComputationalTasks2008
  abstract: >-
    The problem of systematically capturing and managing provenance for
    computational tasks has recently received significant attention because of
    its relevance to a wide range of domains and applications. The authors give
    an overview of important concepts related to provenance management, so that
    potential users can make informed decisions when selecting or designing a
    provenance solution.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Freire
      given: Juliana
    - family: Koop
      given: David
    - family: Santos
      given: Emanuele
    - family: Silva
      given: Cláudio T.
  citation-key: freireProvenanceComputationalTasks2008
  container-title: Computing in Science & Engineering
  container-title-short: Comput. Sci. Eng.
  DOI: 10.1109/MCSE.2008.79
  ISSN: 1521-9615
  issue: '3'
  issued:
    - year: 2008
      month: 5
  note: 'interest: 97'
  page: 11-21
  source: DOI.org (Crossref)
  title: 'Provenance for Computational Tasks: A Survey'
  title-short: Provenance for Computational Tasks
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/4488060/
  volume: '10'

- id: frewES3DemonstrationTransparent2008
  abstract: >-
    The Earth System Science Server (ES3) is a software environment for
    data-intensive Earth science, with unique capabilities for automatically and
    transparently capturing and managing the provenance of arbitrary
    computations. Transparent acquisition avoids the scientist having to express
    their computations in specific languages or schemas for provenance to be
    available. ES3 models provenance as relationships between processes and
    their input and output files. These relationships are captured by monitoring
    read and write accesses at various levels in the science software and
    asynchronously converting them to time-ordered streams of provenance events
    which are stored in an XML database. An ES3 provenance query returns an XML
    serialization of a provenance graph, forward or backwards from a specified
    process or file. We demonstrate ES3 provenance by generating complex data
    products from Earth satellite imagery.
  author:
    - family: Frew
      given: James
    - family: Slaughter
      given: Peter
  citation-key: frewES3DemonstrationTransparent2008
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-540-89965-5_21
  editor:
    - family: Freire
      given: Juliana
    - family: Koop
      given: David
    - family: Moreau
      given: Luc
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-89965-5
  issued:
    - year: 2008
  language: en
  page: 200-207
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: 'ES3: A Demonstration of Transparent Provenance for Scientific Computation'
  title-short: ES3
  type: paper-conference

- id: fritzschResearchSoftwareLandscape2019
  abstract: >-
    Software plays a crucial role in the whole lifecycle of most of scientific
    data and must be considered in all discussions about openness of data and
    reproducibility of science. Consequently, some interest and working groups
    in RDA are dealing with software code. 

    The spectrum of research software ranges from packages driven by large teams
    of developers and used by a broad community, to small scripts that
    scientists almost casually write for their own work. The knowledge of modern
    software development practice is very different, as well as individual
    skills in programming and documentation. The quality levels are often
    correspondingly different. In order to reduce such differences in the medium
    term, general policies can be helpful.

    Policies can provide a framework for a defined way of dealing with
    software.  In Germany, some coordinated activities are delving into this
    topic. For example in the Helmholtz Association, the working group Open
    Science expanded its perspective from initially only data, and has now set
    up a Taskgroup. Research software was also anchored as an issue in the
    Priority Initiative Digital Information of the Alliance of Science
    Organisations in Germany. The panels have already published some
    recommendations on the development, use, and provision of research software.
    At present, they are working on guidelines that can serve as a basis for
    daily work at the institutions. The documents cover different facets,
    ranging from development practice and quality assurance to publication of
    software and licensing. The talk will give an overview about the status of
    these activities. It will also shed light on the players in the software
    development. Starting from the UK, people are increasingly organizing
    themselves, working at the interface between science and computer science.
    In Germany too, there has been an association de-RSE since 2018, which
    campaigns for the interests of research software engineers.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Fritzsch
      given: Bernadette
  citation-key: fritzschResearchSoftwareLandscape2019
  container-title: >-
    EPIC3GeoMünster 2019 "Earth! Past, Present, Future", Münster,
    2019-09-22-2019-09-25
  event-place: Münster
  event-title: GeoMünster 2019 "Earth! Past, Present, Future"
  issued:
    - year: 2019
      month: 9
      day: 25
  note: 'interest: 85'
  publisher-place: Münster
  source: epic.awi.de
  title: Research software landscape and stakeholders
  type: paper-conference
  URL: https://epic.awi.de/id/eprint/50451/

- id: fsspecauthorsFilesystem_spec2023
  abstract: A specification that python filesystems should adhere to.
  accessed:
    - year: 2023
      month: 5
      day: 3
  author:
    - family: FSspec authors
      given: ''
  citation-key: fsspecauthorsFilesystem_spec2023
  genre: Python
  issued:
    - year: 2023
      month: 5
      day: 1
  license: BSD-3-Clause
  original-date:
    - year: 2018
      month: 4
      day: 23
  source: GitHub
  title: filesystem_spec
  type: software
  URL: https://github.com/fsspec/filesystem_spec

- id: FUSE
  accessed:
    - year: 2023
      month: 8
      day: 24
  citation-key: FUSE
  container-title: The Linux Kernel documentation
  title: FUSE
  type: webpage
  URL: https://www.kernel.org/doc/html/latest/filesystems/fuse.html

- id: gamblinSpackPackageManager2015
  abstract: >-
    Large HPC centers spend considerable time supporting software for thousands
    of users, but the complexity of HPC software is quickly outpacing the
    capabilities of existing software management tools. Scientific applications
    require specific versions of compilers, MPI, and other dependency libraries,
    so using a single, standard software stack is infeasible. However, managing
    many configurations is difficult because the configuration space is
    combinatorial in size. We introduce Spack, a tool used at Lawrence Livermore
    National Laboratory to manage this complexity. Spack provides a novel,
    recursive specification syntax to invoke parametric builds of packages and
    dependencies. It allows any number of builds to coexist on the same system,
    and it ensures that installed packages can find their dependencies,
    regardless of the environment. We show through real-world use cases that
    Spack supports diverse and demanding applications, bringing order to HPC
    software chaos.
  accessed:
    - year: 2022
      month: 4
      day: 10
  author:
    - family: Gamblin
      given: Todd
    - family: LeGendre
      given: Matthew
    - family: Collette
      given: Michael R.
    - family: Lee
      given: Gregory L.
    - family: Moody
      given: Adam
    - family: Supinski
      given: Bronis R.
      non-dropping-particle: de
    - family: Futral
      given: Scott
  citation-key: gamblinSpackPackageManager2015
  collection-title: SC '15
  container-title: >-
    Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis
  DOI: 10.1145/2807591.2807623
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3723-6
  issued:
    - year: 2015
      month: 11
      day: 15
  note: 'interest: 80'
  page: 1–12
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'The Spack package manager: bringing order to HPC software chaos'
  title-short: The Spack package manager
  type: paper-conference
  URL: https://doi.org/10.1145/2807591.2807623

- id: gandonRDFXMLSyntax2014
  abstract: >-
    This document defines an XML syntax for RDF called RDF/XML in terms of
    Namespaces in XML, the XML Information Set and XML Base.
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Gandon
      given: Fabian
    - family: Shcreiber
      given: Guus
    - family: Beckett
      given: David
  citation-key: gandonRDFXMLSyntax2014
  container-title: W3C Standards
  issued:
    - year: 2014
      month: 2
      day: 25
  title: RDF 1.1 XML Syntax
  type: webpage
  URL: https://www.w3.org/TR/rdf-syntax-grammar/#section-Syntax-blank-nodes

- id: gannslePaulGanssleWhy2021
  abstract: >-
    The setuptools team no longer wants to be in the business of providing a
    command line interface and is actively working to become just a library for
    building packages. What you should do instead depends on your use case, but
    if you want some basic rules of thumb, there is a table in the summary
    section.


    This does not mean that setuptools itself is deprecated, or that using
    setup.py to configure your package builds is going to be removed. The only
    thing you must stop doing is directly executing the setup.py file — instead
    delegate that to purpose-built or standards-based tools, preferably those
    that work with any build backend.
  accessed:
    - year: 2023
      month: 6
      day: 14
  author:
    - family: Gannsle
      given: Paul
  citation-key: gannslePaulGanssleWhy2021
  container-title: Paul Gannsle
  issued:
    - year: 2021
      month: 10
      day: 19
  language: en
  section: programming
  title: Paul Ganssle - Why you shouldn't invoke setup.py directly
  type: post-weblog
  URL: >-
    https://blog.ganssle.io/articles/2021/10/../../../articles/2021/10/setup-py-deprecated.html

- id: garijoAbstractLinkPublish2017
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Garijo
      given: Daniel
    - family: Gil
      given: Yolanda
    - family: Corcho
      given: Oscar
  citation-key: garijoAbstractLinkPublish2017
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.01.008
  ISSN: 0167739X
  issued:
    - year: 2017
      month: 10
  language: en
  page: 271-283
  source: DOI.org (Crossref)
  title: >-
    Abstract, link, publish, exploit: An end to end framework for workflow
    sharing
  title-short: Abstract, link, publish, exploit
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0167739X17300274
  volume: '75'

- id: garijoNewApproachPublishing2011
  abstract: >-
    In recent years, a variety of systems have been developed that export the
    workflows used to analyze data and make them part of published articles. We
    argue that the workflows that are published in current approaches are
    dependent on the specific codes used for execution, the specific workflow
    system used, and the specific workflow catalogs where they are published. In
    this paper, we describe a new approach that addresses these shortcomings and
    makes workflows more reusable through: 1) the use of abstract workflows to
    complement executable workflows to make them reusable when the execution
    environment is different, 2) the publication of both abstract and executable
    workflows using standards such as the Open Provenance Model that can be
    imported by other workflow systems, 3) the publication of workflows as
    Linked Data that results in open web accessible workflow repositories. We
    illustrate this approach using a complex workflow that we re-created from an
    influential publication that describes the generation of 'drugomes'.
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Garijo
      given: Daniel
    - family: Gil
      given: Yolanda
  citation-key: garijoNewApproachPublishing2011
  collection-title: WORKS '11
  container-title: >-
    Proceedings of the 6th workshop on Workflows in support of large-scale
    science
  DOI: 10.1145/2110497.2110504
  event-place: New York, NY, USA
  ISBN: 978-1-4503-1100-7
  issued:
    - year: 2011
      month: 11
      day: 14
  page: 47–56
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    A new approach for publishing workflows: abstractions, standards, and linked
    data
  title-short: A new approach for publishing workflows
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2110497.2110504

- id: garijoNineBestPractices2022
  abstract: >-
    Scientific software registries and repositories improve software findability
    and research transparency, provide information for software citations, and
    foster preservation of computational methods in a wide range of disciplines.
    Registries and repositories play a critical role by supporting research
    reproducibility and replicability, but developing them takes effort and few
    guidelines are available to help prospective creators of these resources. To
    address this need, the FORCE11 Software Citation Implementation Working
    Group convened a Task Force to distill the experiences of the managers of
    existing resources in setting expectations for all stakeholders. In this
    article, we describe the resultant best practices which include defining the
    scope, policies, and rules that govern individual registries and
    repositories, along with the background, examples, and collaborative work
    that went into their development. We believe that establishing specific
    policies such as those presented here will help other scientific software
    registries and repositories better serve their users and their disciplines.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Garijo
      given: Daniel
    - family: Ménager
      given: Hervé
    - family: Hwang
      given: Lorraine
    - family: Trisovic
      given: Ana
    - family: Hucka
      given: Michael
    - family: Morrell
      given: Thomas
    - family: Allen
      given: Alice
  citation-key: garijoNineBestPractices2022
  container-title: PeerJ Computer Science
  container-title-short: PeerJ Comput. Sci.
  DOI: 10.7717/peerj-cs.1023
  ISSN: 2376-5992
  issued:
    - year: 2022
      month: 8
      day: 8
  language: en
  note: 'interest: 65'
  page: e1023
  publisher: PeerJ Inc.
  source: peerj.com
  title: Nine best practices for research software registries and repositories
  type: article-journal
  URL: https://peerj.com/articles/cs-1023
  volume: '8'

- id: garijoWorkflowEcosystemsSemantic2014
  abstract: >-
    Workflows are increasingly used to manage and share scientific computations
    and methods. Workflow tools can be used to design, validate, execute and
    visualize scientific workflows and their execution results. Other tools
    manage workflow libraries or mine their contents. There has been a lot of
    recent work on workflow system integration as well as common workflow
    interlinguas, but the interoperability among workflow systems remains a
    challenge. Ideally, these tools would form a workflow ecosystem such that it
    should be possible to create a workflow with a tool, execute it with
    another, visualize it with another, and use yet another tool to mine a
    repository of such workflows or their executions. In this paper, we describe
    our approach to create a workflow ecosystem through the use of standard
    models for provenance (OPM and W3C PROV) and extensions (P-PLAN and OPMW) to
    represent workflows. The ecosystem integrates different workflow tools with
    diverse functions (workflow generation, execution, browsing, mining, and
    visualization) created by a variety of research groups. This is, to our
    knowledge, the first time that such a variety of workflow systems and
    functions are integrated.
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Garijo
      given: Daniel
    - family: Gil
      given: Yolanda
    - family: Corcho
      given: Oscar
  citation-key: garijoWorkflowEcosystemsSemantic2014
  container-title: 2014 9th Workshop on Workflows in Support of Large-Scale Science
  DOI: 10.1109/WORKS.2014.13
  event-place: New Orleans, LA
  event-title: 2014 9th Workshop on Workflows in Support of Large-Scale Science (WORKS)
  ISBN: 978-1-4799-7067-4
  issued:
    - year: 2014
      month: 11
  page: 94-104
  publisher: IEEE
  publisher-place: New Orleans, LA
  source: DOI.org (Crossref)
  title: Towards Workflow Ecosystems through Semantic and Standard Representations
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/7019866/

- id: gavishUniversalIdentifierComputational2011
  abstract: >-
    We present a discipline for verifiable computational scienti_c research. Our
    discipline revolves around three simple new concepts — verifiable
    computational result (VCR), VCR repository and Verifiable Result Identifier
    (VRI). These are web- and cloud-computing oriented concepts, which exploit
    today's web infrastructure to achieve standard, simple and automatic
    reproducibility in computational scientific research. The VCR discipline
    requires very slight modifications to the way researchers already conduct
    their computational research and authoring, and to the way publishers manage
    their content. In return, the discipline marks a significant step towards
    delivering on the long-anticipated promises of making scientific computation
    truly reproducible.


    A researcher practicing this discipline in everyday work produces
    computational scripts and word processor files that look very much like
    those they already produce today, but in which a few lines change very
    subtly and naturally. Those scripts produce a stream of verifiable results,
    which are the same tables, figures, charts and datasets the researcher
    traditionally would have produced, but which are watermarked for permanent
    identification by a VRI, and are automatically and permanently stored in a
    VCR repository. In a scientific community practicing Verifiable
    Computational Research, exchange of both ideas and data involves exchanging
    result identifiers—VRIs—rather than exchanging files. These identifiers are
    controlled, trusted and automatically generated strings that point to
    publicly available result as it was originally created by the computational
    process itself. When a verifiable result is included in a publication, its
    identifier can be used by any reader with a web browser to locate, browse
    and, where appropriate, re-execute the computation that produced the result.
    Journal readers can therefore scrutinize, dispute, understand and eventually
    trust these computational results, all to an extent impossible through
    textual explanations that constitute the core of scientific publications to
    date. In addition, the result identi_er can be used by subsequent
    computations to locate and retrieve both the published result (in graphical
    or numerical form) and the original datasets used by its generating
    computation. Colleagues can thus cite and import data into their own
    computations, just as traditional publications allow them to cite and import
    ideas.


    We describe an existing software implementation of the Verifiable
    Computational Research discipline, and argue that it solves many of the
    crucial problems commonly facing computer-based and computer-aided research
    in various scientific fields. Our system is secure, naturally adapted to
    large-scale and cloud computations and to modern massive data analysis, yet
    places effectively no additional workload on either the researcher or the
    publisher.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Gavish
      given: Matan
    - family: Donoho
      given: David
  citation-key: gavishUniversalIdentifierComputational2011
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2011.04.067
  ISSN: '18770509'
  issued:
    - year: 2011
  language: en
  note: 'interest: 71'
  page: 637-647
  source: DOI.org (Crossref)
  title: A Universal Identifier for Computational Results
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1877050911001256
  volume: '4'

- id: gaynorModernWonUs
  accessed:
    - year: 2022
      month: 5
      day: 10
  author:
    - family: Gaynor
      given: Alex
  citation-key: gaynorModernWonUs
  title: Modern C++ Won't Save Us
  type: post-weblog
  URL: https://alexgaynor.net/2019/apr/21/modern-c++-wont-save-us/

- id: gedamThoughtsPythonPackaging2023
  abstract: >-
    My response to the discussion topic posed in Python Packaging Strategy
    Discussion Part 1 had become quite long, so I decided to move it to write a
    blog post instead. This post then started absorbing various draft posts I’ve
    had on this topic since this blog was started, morphing to include my
    broader thoughts on where we are today.

    Note: I’ve updated this to cover an aspect of the recent LWN article on the
    topic as well.
  accessed:
    - year: 2023
      month: 1
      day: 22
  author:
    - family: Gedam
      given: Pradyun
  citation-key: gedamThoughtsPythonPackaging2023
  issued:
    - year: 2023
      month: 1
      day: 21
  language: en
  note: 'interest: 90'
  title: Thoughts on the Python packaging ecosystem
  type: post-weblog
  URL: https://pradyunsg.me/blog/2023/01/21/thoughts-on-python-packaging/

- id: gehaniSPADESupportProvenance2012
  abstract: >-
    SPADE is an open source software infrastructure for data provenance
    collection and management. The underlying data model used throughout the
    system is graph-based, consisting of vertices and directed edges that are
    modeled after the node and relationship types described in the Open
    Provenance Model. The system has been designed to decouple the collection,
    storage, and querying of provenance metadata. At its core is a novel
    provenance kernel that mediates between the producers and consumers of
    provenance information, and handles the persistent storage of records. It
    operates as a service, peering with remote instances to enable distributed
    provenance queries. The provenance kernel on each host handles the
    buffering, filtering, and multiplexing of incoming metadata from multiple
    sources, including the operating system, applications, and manual curation.
    Provenance elements can be located locally with queries that use wildcard,
    fuzzy, proximity, range, and Boolean operators. Ancestor and descendant
    queries are transparently propagated across hosts until a terminating
    expression is satisfied, while distributed path queries are accelerated with
    provenance sketches.
  author:
    - family: Gehani
      given: Ashish
    - family: Tariq
      given: Dawood
  citation-key: gehaniSPADESupportProvenance2012
  collection-title: Lecture Notes in Computer Science
  container-title: Middleware 2012
  DOI: 10.1007/978-3-642-35170-9_6
  editor:
    - family: Narasimhan
      given: Priya
    - family: Triantafillou
      given: Peter
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-35170-9
  issued:
    - year: 2012
  language: en
  page: 101-120
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: 'SPADE: Support for Provenance Auditing in Distributed Environments'
  title-short: SPADE
  type: paper-conference

- id: gelmanConfirmationistFalsificationistParadigms
  author:
    - family: Gelman
      given: Andrew
  citation-key: gelmanConfirmationistFalsificationistParadigms
  container-title: Statistical Modeling, Causal Inference, and Social Science
  note: 'interest: 95'
  title: Confirmationist and falsificationist paradigms of science
  type: post-weblog
  URL: >-
    https://statmodeling.stat.columbia.edu/2014/09/05/confirmationist-falsificationist-paradigms-science/

- id: gelmanDifferenceSignificantNot2006
  abstract: >-
    It is common to summarize statistical comparisons by declarations of
    statistical significance or nonsignificance. Here we discuss one problem
    with such declarations, namely that changes in statistical significance are
    often not themselves statistically significant. By this, we are not merely
    making the commonplace observation that any particular threshold is
    arbitrary—for example, only a small change is required to move an estimate
    from a 5.1% significance level to 4.9%, thus moving it into statistical
    significance. Rather, we are pointing out that even large changes in
    significance levels can correspond to small, nonsignificant changes in the
    underlying quantities. The error we describe is conceptually different from
    other oft-cited problems—that statistical significance is not the same as
    practical importance, that dichotomization into significant and
    nonsignificant results encourages the dismissal of observed differences in
    favor of the usually less interesting null hypothesis of no difference, and
    that any particular threshold for declaring significance is arbitrary. We
    are troubled by all of these concerns and do not intend to minimize their
    importance. Rather, our goal is to bring attention to this additional error
    of interpretation. We illustrate with a theoretical example and two applied
    examples. The ubiquity of this statistical error leads us to suggest that
    students and practitioners be made more aware that the difference between
    “significant” and “not significant” is not itself statistically significant.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Gelman
      given: Andrew
    - family: Stern
      given: Hal
  citation-key: gelmanDifferenceSignificantNot2006
  container-title: The American Statistician
  DOI: 10.1198/000313006X152649
  ISSN: 0003-1305
  issue: '4'
  issued:
    - year: 2006
      month: 11
      day: 1
  page: 328-331
  publisher: Taylor & Francis
  source: Taylor and Francis+NEJM
  title: >-
    The Difference Between “Significant” and “Not Significant” is not Itself
    Statistically Significant
  type: article-journal
  URL: https://doi.org/10.1198/000313006X152649
  volume: '60'

- id: genelFollowingFlowTracer2013
  abstract: >-
    We present two numerical schemes for passive tracer particles in the
    hydrodynamical moving-mesh code arepo, and compare their performance for
    various problems, from simple set-ups to cosmological simulations. The
    purpose of tracer particles is to allow the flow to be followed in a
    Lagrangian way, tracing the evolution of the fluid with time, and allowing
    the thermodynamical history of individual fluid parcels to be recorded. We
    find that the commonly used ‘velocity field tracers’, which are advected
    using the fluid velocity field, do not in general follow the mass flow
    correctly, and explain why this is the case. This method can result in
    order-of-magnitude biases in simulations of driven turbulence and in
    cosmological simulations, rendering the velocity field tracers inappropriate
    for following these flows. We then discuss a novel implementation of ‘Monte
    Carlo tracers’, which are moved along with fluid cells and are exchanged
    probabilistically between them following the mass flux. This method
    reproduces the mass distribution of the fluid correctly. The main limitation
    of this approach is that it is more diffusive than the fluid itself.
    Nonetheless, we show that this novel approach is more reliable than that has
    been employed previously and demonstrate that it is appropriate for
    following hydrodynamical flows in mesh-based codes. The Monte Carlo tracers
    can also naturally be transferred between fluid cells and other types of
    particles, such as stellar particles, so that the mass flow in cosmological
    simulations can be followed in its entirety.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Genel
      given: Shy
    - family: Vogelsberger
      given: Mark
    - family: Nelson
      given: Dylan
    - family: Sijacki
      given: Debora
    - family: Springel
      given: Volker
    - family: Hernquist
      given: Lars
  citation-key: genelFollowingFlowTracer2013
  container-title: Monthly Notices of the Royal Astronomical Society
  container-title-short: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/stt1383
  ISSN: 0035-8711
  issue: '2'
  issued:
    - year: 2013
      month: 10
      day: 21
  page: 1426-1442
  source: Silverchair
  title: 'Following the flow: tracer particles in astrophysical fluid simulations'
  title-short: Following the flow
  type: article-journal
  URL: https://doi.org/10.1093/mnras/stt1383
  volume: '435'

- id: genovaComputerScienceTruly2010
  abstract: Reflections on the (experimental) scientific method in computer science.
  accessed:
    - year: 2023
      month: 2
      day: 12
  author:
    - family: Génova
      given: Gonzalo
  citation-key: genovaComputerScienceTruly2010
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/1785414.1785431
  ISSN: 0001-0782, 1557-7317
  issue: '7'
  issued:
    - year: 2010
      month: 7
  language: en
  page: 37-39
  source: DOI.org (Crossref)
  title: Is computer science truly scientific?
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1785414.1785431
  volume: '53'

- id: gibneyCouldMachineLearning2022
  abstract: >-
    ‘Data leakage’ threatens the reliability of machine-learning use across
    disciplines, researchers warn.
  accessed:
    - year: 2022
      month: 7
      day: 28
  author:
    - family: Gibney
      given: Elizabeth
  citation-key: gibneyCouldMachineLearning2022
  container-title: Nature
  container-title-short: Nature
  DOI: 10.1038/d41586-022-02035-w
  ISSN: 0028-0836, 1476-4687
  issued:
    - year: 2022
      month: 7
      day: 26
  language: en
  page: d41586-022-02035-w
  source: DOI.org (Crossref)
  title: Could machine learning fuel a reproducibility crisis in science?
  type: article-journal
  URL: https://www.nature.com/articles/d41586-022-02035-w

- id: gigerenzerMindlessStatistics2004
  abstract: >-
    Statistical rituals largely eliminate statistical thinking in the social
    sciences. Rituals are indispensable for identification with social groups,
    but they should be the subject rather than the procedure of science. What I
    call the “null ritual” consists of three steps: (1) set up a statistical
    null hypothesis, but do not specify your own hypothesis nor any alternative
    hypothesis, (2) use the 5% significance level for rejecting the null and
    accepting your hypothesis, and (3) always perform this procedure. I report
    evidence of the resulting collective confusion and fears about sanctions on
    the part of students and teachers, researchers and editors, as well as
    textbook writers.
  accessed:
    - year: 2024
      month: 1
      day: 28
  author:
    - family: Gigerenzer
      given: Gerd
  citation-key: gigerenzerMindlessStatistics2004
  collection-title: Statistical Significance
  container-title: The Journal of Socio-Economics
  container-title-short: The Journal of Socio-Economics
  DOI: 10.1016/j.socec.2004.09.033
  ISSN: 1053-5357
  issue: '5'
  issued:
    - year: 2004
      month: 11
      day: 1
  page: 587-606
  source: ScienceDirect
  title: Mindless statistics
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S1053535704000927
  volume: '33'

- id: gilExaminingChallengesScientific2007
  abstract: >-
    Workflows have emerged as a paradigm for representing and managing complex
    distributed computations and are used to accelerate the pace of scientific
    progress. A recent National Science Foundation workshop brought together
    domain, computer, and social scientists to discuss requirements of future
    scientific applications and the challenges they present to current workflow
    technologies.
  author:
    - family: Gil
      given: Yolanda
    - family: Deelman
      given: Ewa
    - family: Ellisman
      given: Mark
    - family: Fahringer
      given: Thomas
    - family: Fox
      given: Geoffrey
    - family: Gannon
      given: Dennis
    - family: Goble
      given: Carole
    - family: Livny
      given: Miron
    - family: Moreau
      given: Luc
    - family: Myers
      given: Jim
  citation-key: gilExaminingChallengesScientific2007
  container-title: Computer
  DOI: 10.1109/MC.2007.421
  ISSN: 1558-0814
  issue: '12'
  issued:
    - year: 2007
      month: 12
  note: 'interest: 91'
  page: 24-32
  source: IEEE Xplore
  title: Examining the Challenges of Scientific Workflows
  type: article-journal
  volume: '40'

- id: GiottoTDADocumentationGiottotda
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: GiottoTDADocumentationGiottotda
  title: Giotto-TDA documentation — giotto-tda 0.5.1 documentation
  type: webpage
  URL: https://giotto-ai.github.io/gtda-docs/0.5.1/index.html

- id: gligoricAutomatedMigrationBuild2014
  abstract: >-
    The efficiency of a build system is an important factor for developer
    productivity. As a result, developer teams have been increasingly adopting
    new build systems that allow higher build parallelization. However,
    migrating the existing legacy build scripts to new build systems is a
    tedious and error-prone process. Unfortunately, there is insufficient
    support for automated migration of build scripts, making the migration more
    problematic. We propose the first dynamic approach for automated migration
    of build scripts to new build systems. Our approach works in two phases.
    First, from a set of execution traces, we synthesize build scripts that
    accurately capture the intent of the original build. The synthesized build
    scripts are typically long and hard to maintain. Second, we apply
    refactorings that raise the abstraction level of the synthesized scripts
    (e.g., introduce functions for similar fragments). As different refactoring
    sequences may lead to different build scripts, we use a search-based
    approach that explores various sequences to identify the best (e.g.,
    shortest) build script. We optimize search-based refactoring with
    partial-order reduction to faster explore refactoring sequences. We
    implemented the proposed two phase migration approach in a tool called
    METAMORPHOSIS that has been recently used at Microsoft.
  accessed:
    - year: 2023
      month: 10
      day: 16
  author:
    - family: Gligoric
      given: Milos
    - family: Schulte
      given: Wolfram
    - family: Prasad
      given: Chandra
    - family: Velzen
      given: Danny
      non-dropping-particle: van
    - family: Narasamdya
      given: Iman
    - family: Livshits
      given: Benjamin
  citation-key: gligoricAutomatedMigrationBuild2014
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/2714064.2660239
  ISSN: 0362-1340
  issue: '10'
  issued:
    - year: 2014
      month: 10
      day: 15
  page: 599–616
  source: ACM Digital Library
  title: >-
    Automated migration of build scripts using dynamic analysis and search-based
    refactoring
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2714064.2660239
  volume: '49'

- id: gligoricCoDeSeFastDeserialization2011
  abstract: >-
    Many tools for automated testing, model checking, and debugging store and
    restore program states multiple times. Storing/restoring a program state is
    commonly done with serialization/deserialization. Traditionally, the format
    for stored states is based on data: serialization generates the data that
    encodes the state, and deserialization interprets this data to restore the
    state. We propose a new approach, called CoDeSe, where the format for stored
    states is based on code: serialization generates code whose execution
    restores the state, and deserialization simply executes the code. We
    implemented CoDeSe in Java and performed a number of experiments on
    deserialization of states. CoDeSe provides on average more than 6X speedup
    over the highly optimized deserialization from the standard Java library.
    Our new format also allows simple parallel deserialization that can provide
    additional speedup on top of the sequential CoDeSe but only for larger
    states.
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Gligoric
      given: Milos
    - family: Marinov
      given: Darko
    - family: Kamin
      given: Sam
  citation-key: gligoricCoDeSeFastDeserialization2011
  collection-title: ISSTA '11
  container-title: >-
    Proceedings of the 2011 International Symposium on Software Testing and
    Analysis
  DOI: 10.1145/2001420.2001456
  event-place: New York, NY, USA
  ISBN: 978-1-4503-0562-4
  issued:
    - year: 2011
      month: 7
      day: 17
  note: 'score: 50'
  page: 298–308
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'CoDeSe: fast deserialization via code generation'
  title-short: CoDeSe
  type: paper-conference
  URL: https://doi.org/10.1145/2001420.2001456

- id: gligoricEmpiricalEvaluationComparison2014
  abstract: >-
    Regression test selection speeds up regression testing by re-running only
    the tests that can be affected by the most recent code changes. Much
    progress has been made on research in automated test selection over the last
    three decades, but it has not translated into practical tools that are
    widely adopted. Therefore, developers either re-run all tests after each
    change or perform manual test selection. Re-running all tests is expensive,
    while manual test selection is tedious and error-prone. Despite such a big
    trade-off, no study assessed how developers perform manual test selection
    and compared it to automated test selection. This paper reports on our study
    of manual test selection in practice and our comparison of manual and
    automated test selection. We are the first to conduct a study that (1)
    analyzes data from manual test selection, collected in real time from 14
    developers during a three-month study and (2) compares manual test selection
    with an automated state-of-the-research test-selection tool for 450 test
    sessions. Almost all developers in our study performed manual test
    selection, and they did so in mostly ad-hoc ways. Comparing manual and
    automated test selection, we found the two approaches to select different
    tests in each and every one of the 450 test sessions investigated. Manual
    selection chose more tests than automated selection 73% of the time
    (potentially wasting time) and chose fewer tests 27% of the time
    (potentially missing bugs). These results show the need for better automated
    test-selection techniques that integrate well with developers' programming
    environments.
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Gligoric
      given: Milos
    - family: Negara
      given: Stas
    - family: Legunsen
      given: Owolabi
    - family: Marinov
      given: Darko
  citation-key: gligoricEmpiricalEvaluationComparison2014
  collection-title: ASE '14
  container-title: >-
    Proceedings of the 29th ACM/IEEE international conference on Automated
    software engineering
  DOI: 10.1145/2642937.2643019
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3013-8
  issued:
    - year: 2014
      month: 9
      day: 15
  note: 'interest: 60'
  page: 361–372
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    An empirical evaluation and comparison of manual and automated test
    selection
  type: paper-conference
  URL: https://doi.org/10.1145/2642937.2643019

- id: gligoricPracticalRegressionTest2015
  abstract: >-
    Regression testing is important but can be time-intensive. One approach to
    speed it up is regression test selection (RTS), which runs only a subset of
    tests. RTS was proposed over three decades ago but has not been widely
    adopted in practice. Meanwhile, testing frameworks, such as JUnit, are
    widely adopted and well integrated with many popular build systems. Hence,
    integrating RTS in a testing framework already used by many projects would
    increase the likelihood that RTS is adopted. We propose a new, lightweight
    RTS technique, called Ekstazi, that can integrate well with testing
    frameworks. Ekstazi tracks dynamic dependencies of tests on files, and
    unlike most prior RTS techniques, Ekstazi requires no integration with
    version-control systems. We implemented Ekstazi for Java and JUnit, and
    evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M
    LOC) with shorter- and longer-running test suites. The results show that
    Ekstazi reduced the end-to-end testing time 32% on average, and 54% for
    longer-running test suites, compared to executing all tests. Ekstazi also
    has lower end-to-end time than the existing techniques, despite the fact
    that it selects more tests. Ekstazi has been adopted by several popular open
    source projects, including Apache Camel, Apache Commons Math, and Apache
    CXF.
  accessed:
    - year: 2022
      month: 4
      day: 6
  author:
    - family: Gligoric
      given: Milos
    - family: Eloussi
      given: Lamyaa
    - family: Marinov
      given: Darko
  citation-key: gligoricPracticalRegressionTest2015
  collection-title: ISSTA 2015
  container-title: >-
    Proceedings of the 2015 International Symposium on Software Testing and
    Analysis
  DOI: 10.1145/2771783.2771784
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3620-8
  issued:
    - year: 2015
      month: 7
      day: 13
  note: 'score: 70'
  page: 211–222
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Practical regression test selection with dynamic file dependencies
  type: paper-conference
  URL: https://doi.org/10.1145/2771783.2771784

- id: glukhovaToolsEnsuringReproducible2017
  abstract: >-
    The Reproducible Builds project is a collective effort of multiple
    open-source software projects, aiming to provide a verifiable path from
    human readable source code to the binary code used by computers. To achieve
    this goal, several tools were created, allowing for identifying common
    sources of unreproducibility in build process. In this work, an overview of
    the Reproducible Builds project and the tools designed is made. One of the
    tools, named diffoscope, is discussed in details; several improvements to
    this tool are made as part of this work.
  author:
    - family: Glukhova
      given: Maria
  citation-key: glukhovaToolsEnsuringReproducible2017
  genre: Master's Thesis
  issued:
    - year: 2017
  language: en
  number-of-pages: '36'
  publisher: Lappeenranta University of Technology
  source: Zotero
  title: Tools for Ensuring Reproducible Builds for Open-Source Software
  type: thesis
  URL: >-
    https://lutpub.lut.fi/bitstream/handle/10024/135304/MariaGlukhova_ToolsForEnsuringReproducibleBuildsForOpenSourceSoftware.pdf?sequence=2

- id: gobleFAIRComputationalWorkflows2020
  abstract: >-
    Computational workflows describe the complex multi-step methods that are
    used for data collection, data preparation, analytics, predictive modelling,
    and simulation that lead to new data products. They can inherently
    contribute to the FAIR data principles: by processing data according to
    established metadata; by creating metadata themselves during the processing
    of data; and by tracking and recording data provenance. These properties aid
    data quality assessment and contribute to secondary data usage. Moreover,
    workflows are digital objects in their own right. This paper argues that
    FAIR principles for workflows need to address their specific nature in terms
    of their composition of executable software steps, their provenance, and
    their development.
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Goble
      given: Carole
    - family: Cohen-Boulakia
      given: Sarah
    - family: Soiland-Reyes
      given: Stian
    - family: Garijo
      given: Daniel
    - family: Gil
      given: Yolanda
    - family: Crusoe
      given: Michael R.
    - family: Peters
      given: Kristian
    - family: Schober
      given: Daniel
  citation-key: gobleFAIRComputationalWorkflows2020
  container-title: Data Intelligence
  container-title-short: Data Intellegence
  DOI: 10.1162/dint_a_00033
  ISSN: 2641-435X
  issue: 1-2
  issued:
    - year: 2020
      month: 1
  language: en
  page: 108-121
  source: DOI.org (Crossref)
  title: FAIR Computational Workflows
  type: article-journal
  URL: https://direct.mit.edu/dint/article/2/1-2/108-121/10003
  volume: '2'

- id: gobleMyExperimentRepositorySocial2010
  abstract: >-
    myExperiment (http://www.myexperiment.org) is an online research environment
    that supports the social sharing of bioinformatics workflows. These
    workflows are procedures consisting of a series of computational tasks using
    web services, which may be performed on data from its retrieval, integration
    and analysis, to the visualization of the results. As a public repository of
    workflows, myExperiment allows anybody to discover those that are relevant
    to their research, which can then be reused and repurposed to their specific
    requirements. Conversely, developers can submit their workflows to
    myExperiment and enable them to be shared in a secure manner. Since its
    release in 2007, myExperiment currently has over 3500 registered users and
    contains more than 1000 workflows. The social aspect to the sharing of these
    workflows is facilitated by registered users forming virtual communities
    bound together by a common interest or research project. Contributors of
    workflows can build their reputation within these communities by receiving
    feedback and credit from individuals who reuse their work. Further
    documentation about myExperiment including its REST web service is available
    from http://wiki.myexperiment.org. Feedback and requests for support can be
    sent to bugs@myexperiment.org.
  accessed:
    - year: 2022
      month: 10
      day: 31
  author:
    - family: Goble
      given: Carole A.
    - family: Bhagat
      given: Jiten
    - family: Aleksejevs
      given: Sergejs
    - family: Cruickshank
      given: Don
    - family: Michaelides
      given: Danius
    - family: Newman
      given: David
    - family: Borkum
      given: Mark
    - family: Bechhofer
      given: Sean
    - family: Roos
      given: Marco
    - family: Li
      given: Peter
    - family: De Roure
      given: David
  citation-key: gobleMyExperimentRepositorySocial2010
  container-title: Nucleic Acids Research
  container-title-short: Nucleic Acids Research
  DOI: 10.1093/nar/gkq429
  ISSN: 0305-1048
  issue: suppl_2
  issued:
    - year: 2010
      month: 7
      day: 1
  page: W677-W682
  source: Silverchair
  title: >-
    myExperiment: a repository and social network for the sharing of
    bioinformatics workflows
  title-short: myExperiment
  type: article-journal
  URL: https://doi.org/10.1093/nar/gkq429
  volume: '38'

- id: goecksGalaxyComprehensiveApproach2010
  abstract: >-
    Increased reliance on computational approaches in the life sciences has
    revealed grave concerns about how accessible and reproducible
    computation-reliant results truly are. Galaxy http://usegalaxy.org, an open
    web-based platform for genomic research, addresses these problems. Galaxy
    automatically tracks and manages data provenance and provides support for
    capturing the context and intent of computational methods. Galaxy Pages are
    interactive, web-based documents that provide users with a medium to
    communicate a complete computational analysis.
  accessed:
    - year: 2023
      month: 2
      day: 21
  author:
    - family: Goecks
      given: Jeremy
    - family: Nekrutenko
      given: Anton
    - family: Taylor
      given: James
    - literal: The Galaxy Team
  citation-key: goecksGalaxyComprehensiveApproach2010
  container-title: Genome Biology
  container-title-short: Genome Biol
  DOI: 10.1186/gb-2010-11-8-r86
  ISSN: 1474-760X
  issue: '8'
  issued:
    - year: 2010
      month: 8
      day: 25
  language: en
  page: R86
  source: Springer Link
  title: >-
    Galaxy: a comprehensive approach for supporting accessible, reproducible,
    and transparent computational research in the life sciences
  title-short: Galaxy
  type: article-journal
  URL: https://doi.org/10.1186/gb-2010-11-8-r86
  volume: '11'

- id: goertzelPotentialComputationalLinguistics2005
  author:
    - family: Goertzel
      given: Ben
  citation-key: goertzelPotentialComputationalLinguistics2005
  issued:
    - year: 2005
      month: 3
      day: 6
  language: en
  page: '16'
  source: Zotero
  title: Potential Computational Linguistics Resources
  type: report

- id: gomez-diazEvaluationResearchSoftware2019
  abstract: >-
    Background: Evaluation of the quality of research software is a challenging
    and relevant issue, still not sufficiently addressed by the scientific
    community.


    Methods: Our contribution begins by defining, precisely but widely enough,
    the notions of research software and of its authors followed by a study of
    the evaluation issues, as the basis for the proposition of a sound
    assessment protocol: the CDUR procedure.


    Results: CDUR comprises four steps introduced as follows: C itation, to deal
    with correct RS identification, D issemination, to measure good
    dissemination practices, U se, devoted to the evaluation of usability
    aspects, and R esearch, to assess the impact of the scientific work.


    Conclusions: Some conclusions and recommendations are finally included. The
    evaluation of research is the keystone to boost the evolution of the Open
    Science policies and practices.


    It is as well our belief that research software evaluation is a fundamental
    step to induce better research software practices and, thus, a step towards
    more efficient science.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Gomez-Diaz
      given: Teresa
    - family: Recio
      given: Tomas
  citation-key: gomez-diazEvaluationResearchSoftware2019
  DOI: 10.12688/f1000research.19994.2
  issued:
    - year: 2019
      month: 11
      day: 26
  language: en
  license: http://creativecommons.org/licenses/by/4.0/
  note: 'interest: 83'
  number: '8:1353'
  publisher: F1000Research
  source: f1000research.com
  title: 'On the evaluation of research software: the CDUR procedure'
  title-short: On the evaluation of research software
  type: article
  URL: https://f1000research.com/articles/8-1353

- id: gomez-perezWhenHistoryMatters2013
  abstract: >-
    Scientific workflows play an important role in computational research as
    essential artifacts for communicating the methods used to produce research
    findings. We are witnessing a growing number of efforts that treat workflows
    as first-class artifacts for sharing and exchanging scientific knowledge,
    either as part of scholarly articles or as stand-alone objects. However,
    workflows are not born to be reliable, which can seriously damage their
    reusability and trustworthiness as knowledge exchange instruments.
    Scientific workflows are commonly subject to decay, which consequently
    undermines their reliability over their lifetime. The reliability of
    workflows can be notably improved by advocating scientists to preserve a
    minimal set of information that is essential to assist the interpretations
    of these workflows and hence improve their potential for reproducibility and
    reusability. In this paper we show how, by measuring and monitoring the
    completeness and stability of scientific workflows over time we are able to
    provide scientists with a measure of their reliability, supporting the reuse
    of trustworthy scientific knowledge.
  author:
    - family: Gómez-Pérez
      given: José Manuel
    - family: García-Cuesta
      given: Esteban
    - family: Garrido
      given: Aleix
    - family: Ruiz
      given: José Enrique
    - family: Zhao
      given: Jun
    - family: Klyne
      given: Graham
  citation-key: gomez-perezWhenHistoryMatters2013
  collection-title: Lecture Notes in Computer Science
  container-title: The Semantic Web – ISWC 2013
  DOI: 10.1007/978-3-642-41338-4_6
  editor:
    - family: Alani
      given: Harith
    - family: Kagal
      given: Lalana
    - family: Fokoue
      given: Achille
    - family: Groth
      given: Paul
    - family: Biemann
      given: Chris
    - family: Parreira
      given: Josiane Xavier
    - family: Aroyo
      given: Lora
    - family: Noy
      given: Natasha
    - family: Welty
      given: Chris
    - family: Janowicz
      given: Krzysztof
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-41338-4
  issued:
    - year: 2013
  language: en
  page: 81-97
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: >-
    When History Matters - Assessing Reliability for the Reuse of Scientific
    Workflows
  type: paper-conference

- id: gommersPythonPackagingWorkflows2023
  accessed:
    - year: 2023
      month: 2
      day: 24
  author:
    - family: Gommers
      given: Ralf
  citation-key: gommersPythonPackagingWorkflows2023
  container-title: Quansight Labs blog
  issued:
    - year: 2023
      month: 1
      day: 10
  title: Python packaging & workflows - where to next?
  type: post-weblog
  URL: https://labs.quansight.org/blog/python-packaging-where-to-next

- id: goochOverviewLinuxVirtual
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Gooch
      given: ''
  citation-key: goochOverviewLinuxVirtual
  container-title: The Linux Kernel documentation
  title: Overview of the Linux Virtual File System
  type: webpage
  URL: https://docs.kernel.org/filesystems/vfs.html

- id: goodfellowGenerativeAdversarialNets2014
  abstract: >-
    We propose a new framework for estimating generative models via adversarial
    nets, in which we simultaneously train two models: a generative model G that
    captures the data distribution, and a discriminative model D that estimates
    the probability that a sample came from the training data rather than G. The
    training procedure for G is to maximize the probability of D making a
    mistake. This framework corresponds to a minimax two-player game. In the
    space of arbitrary functions G and D, a unique solution exists, with G
    recovering the training data distribution and D equal to 1/2 everywhere. In
    the case where G and D are defined by multilayer perceptrons, the entire
    system can be trained with backpropagation. There is no need for any Markov
    chains or unrolled approximate inference networks during either training or
    generation of samples. Experiments demonstrate the potential of the
    framework through qualitative and quantitatively evaluation of the generated
    samples.
  accessed:
    - year: 2022
      month: 5
      day: 2
  author:
    - family: Goodfellow
      given: Ian
    - family: Pouget-Abadie
      given: Jean
    - family: Mirza
      given: Mehdi
    - family: Xu
      given: Bing
    - family: Warde-Farley
      given: David
    - family: Ozair
      given: Sherjil
    - family: Courville
      given: Aaron
    - family: Bengio
      given: Yoshua
  citation-key: goodfellowGenerativeAdversarialNets2014
  container-title: Advances in Neural Information Processing Systems
  issued:
    - year: 2014
  publisher: Curran Associates, Inc.
  source: Neural Information Processing Systems
  title: Generative Adversarial Nets
  type: paper-conference
  URL: >-
    https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html
  volume: '27'

- id: goodmanPValuesBayesModest2001
  abstract: An abstract is unavailable.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Goodman
      given: Steven N.
  citation-key: goodmanPValuesBayesModest2001
  container-title: Epidemiology
  ISSN: 1044-3983
  issue: '3'
  issued:
    - year: 2001
      month: 5
  language: en-US
  page: '295'
  source: journals-lww-com.proxy2.library.illinois.edu
  title: 'Of P-Values and Bayes: A Modest Proposal'
  title-short: Of P-Values and Bayes
  type: article-journal
  URL: >-
    https://journals.lww.com/epidem/fulltext/2001/05000/of_p_values_and_bayes__a_modest_proposal.6.aspx
  volume: '12'

- id: goodmanTenSimpleRules2014
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Goodman
      given: Alyssa
    - family: Pepe
      given: Alberto
    - family: Blocker
      given: Alexander W.
    - family: Borgman
      given: Christine L.
    - family: Cranmer
      given: Kyle
    - family: Crosas
      given: Merce
    - family: Stefano
      given: Rosanne Di
    - family: Gil
      given: Yolanda
    - family: Groth
      given: Paul
    - family: Hedstrom
      given: Margaret
    - family: Hogg
      given: David W.
    - family: Kashyap
      given: Vinay
    - family: Mahabal
      given: Ashish
    - family: Siemiginowska
      given: Aneta
    - family: Slavkovic
      given: Aleksandra
  citation-key: goodmanTenSimpleRules2014
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1003542
  ISSN: 1553-7358
  issue: '4'
  issued:
    - year: 2014
      month: 4
      day: 24
  language: en
  page: e1003542
  publisher: Public Library of Science
  source: PLoS Journals
  title: Ten Simple Rules for the Care and Feeding of Scientific Data
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003542
  volume: '10'

- id: gouldAcademiaPolicyDavid
  abstract: >-
    An interview with David Carr about his transition from academia to science
    policy.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Gould
      given: Julie
  citation-key: gouldAcademiaPolicyDavid
  container-title: Naturejobs Blog
  title: From academia to policy with David Carr
  type: post-weblog
  URL: >-
    http://blogs.nature.com/naturejobs/2014/12/03/from-academia-to-policy-with-david-carr/

- id: gounarisInvestigatingDriversInternet2008
  abstract: >-
    Purpose: The paper seeks to compare, through empirical evidence, two widely
    adopted models (the Technology Acceptance Model (TAM) and the Diffusion of
    Innovations (DoI) model) to an underutilized one (Perceived Characteristics
    of the Innovation) in order to examine which is better in predicting
    consumer adoption of internet banking (IB), while investigating innovation
    attributes vis‐à‐vis other important predictors of adoption of innovations,
    such as consumer personal characteristics.


    Design/methodology/approach: The data derive from both users and non‐users
    of IB through a web survey. The paper assesses the psychometric properties
    of the measures through confirmatory factor analysis and then employs
    logistic regression analysis in order to assess and compare the ability of
    the models to accurately predict consumer adoption of IB.


    Findings: The paper finds that PCI performed significantly better than TAM
    and DoI in predicting consumer adoption of IB, whereas the addition of
    consumer demographics and psychographics further improved the predictive
    ability of the overall logit model.


    Research limitations/implications: Limitations of the study include the
    non‐random nature of the IB non‐users sample, and the fact that this was a
    study of a single shopping context (i.e. banking). Non‐usability innovation
    characteristics are important predictors of consumer adoption of
    technologically based innovations. Bank managers should reconsider their
    segmentation and targeting strategies in the light of more refined as well
    as new segmentation criteria.


    Originality/value: The PCI model has never been examined within online
    contexts. The paper also incorporates other non‐usability types of
    characteristics (i.e. social, psychological) into TAM and DoI, and
    identifies the moderating role of shopping context, between innovation
    characteristics and decision to adopt.
  accessed:
    - year: 2022
      month: 6
      day: 2
  author:
    - family: Gounaris
      given: Spiros
    - family: Koritos
      given: Christos
  citation-key: gounarisInvestigatingDriversInternet2008
  container-title: International Journal of Bank Marketing
  DOI: 10.1108/02652320810894370
  ISSN: 0265-2323
  issue: '5'
  issued:
    - year: 2008
      month: 7
      day: 25
  language: en
  page: 282-304
  source: DOI.org (Crossref)
  title: >-
    Investigating the drivers of internet banking adoption decision: A
    comparison of three alternative frameworks
  title-short: Investigating the drivers of internet banking adoption decision
  type: article-journal
  URL: >-
    https://www.emerald.com/insight/content/doi/10.1108/02652320810894370/full/html
  volume: '26'

- id: govoniQrespToolCurating2019
  abstract: >-
    We propose a strategy and present a simple tool to facilitate scientific
    data reproducibility by making available, in a distributed manner, all data
    and procedures presented in scientific papers, together with metadata to
    render them searchable and discoverable. In particular, we describe a
    graphical user interface (GUI), Qresp, to curate papers (i.e. generate
    metadata) and to explore curated papers and automatically access the data
    presented in scientific publications.
  accessed:
    - year: 2022
      month: 4
      day: 14
  author:
    - family: Govoni
      given: Marco
    - family: Munakami
      given: Milson
    - family: Tanikanti
      given: Aditya
    - family: Skone
      given: Jonathan H.
    - family: Runesha
      given: Hakizumwami B.
    - family: Giberti
      given: Federico
    - family: Pablo
      given: Juan
      non-dropping-particle: de
    - family: Galli
      given: Giulia
  citation-key: govoniQrespToolCurating2019
  container-title: Scientific Data
  container-title-short: Sci Data
  DOI: 10.1038/sdata.2019.2
  ISSN: 2052-4463
  issue: '1'
  issued:
    - year: 2019
      month: 1
      day: 29
  language: en
  license: 2019 The Author(s)
  note: 'interest: 30'
  number: '1'
  page: '190002'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Qresp, a tool for curating, discovering and exploring reproducible
    scientific papers
  type: article-journal
  URL: https://www.nature.com/articles/sdata20192
  volume: '6'

- id: grahamRootsLisp2002
  author:
    - family: Graham
      given: Paul
  citation-key: grahamRootsLisp2002
  issued:
    - year: 2002
      month: 1
      day: 18
  title: The Roots of Lisp
  type: manuscript

- id: grandeNfprov2023
  accessed:
    - year: 2023
      month: 5
      day: 25
  author:
    - family: Grande
      given: Bruno
    - family: Sherman
      given: Ben
    - family: Di Tomasso
      given: Paolo
  citation-key: grandeNfprov2023
  genre: Groovy
  issued:
    - year: 2023
      month: 5
      day: 7
  license: Apache-2.0
  original-date:
    - year: 2022
      month: 12
      day: 19
  publisher: Sage-Bionetworks-Workflows
  source: GitHub
  title: nf-prov
  type: software
  URL: https://github.com/Sage-Bionetworks-Workflows/nf-prov

- id: grayBioschemasPotatoSalad2017
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Gray
      given: Alasdair J. G.
    - family: Goble
      given: Carole
    - family: Jimenez
      given: Rafael C.
  citation-key: grayBioschemasPotatoSalad2017
  container-title: >-
    ISWC 2017 Posters & Demonstrations and Industry Tracks: Proceedings of the
    ISWC 2017 Posters & Demonstrations and Industry Tracks co-located with 16th
    International Semantic Web Conference (ISWC 2017)
  event-title: The 16th International Semantic Web Conference 2017
  issued:
    - year: 2017
      month: 10
      day: 22
  language: English
  publisher: RWTH Aachen University
  source: research.manchester.ac.uk
  title: 'Bioschemas: From Potato Salad to Protein Annotation'
  title-short: Bioschemas
  type: paper-conference
  URL: >-
    https://research.manchester.ac.uk/en/publications/bioschemas-from-potato-salad-to-protein-annotation

- id: grayDataMiningSDSS2002
  abstract: >-
    An earlier paper (Szalay et. al. "Designing and Mining MultiTerabyte
    Astronomy Archives: The Sloan Digital Sky Survey," ACM SIGMOD 2000)
    described the Sloan Digital Sky Survey's (SDSS) data management needs by
    defining twenty database queries and twelve data visualization tasks that a
    good data management system should support. We built a database and
    interfaces to support both the query load and also a website for ad-hoc
    access. This paper reports on the database design, describes the data
    loading pipeline, and reports on the query implementation and performance.
    The queries typically translated to a single SQL statement. Most queries run
    in less than 20 seconds, allowing scientists to interactively explore the
    database. This paper is an in-depth tour of those queries. Readers should
    first have studied the companion overview paper Szalay et. al. "The SDSS
    SkyServer, Public Access to the Sloan Digital Sky Server Data" ACM SIGMOND
    2002.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Gray
      given: Jim
    - family: Szalay
      given: Alex S.
    - family: Thakar
      given: Ani R.
    - family: Kunszt
      given: Peter Z.
    - family: Stoughton
      given: Christopher
    - family: Slutz
      given: Don
    - family: vandenBerg
      given: Jan
  citation-key: grayDataMiningSDSS2002
  container-title: arXiv:cs/0202014
  issued:
    - year: 2002
      month: 2
      day: 12
  note: 'interest: 70'
  source: arXiv.org
  title: Data Mining the SDSS SkyServer Database
  type: article-journal
  URL: http://arxiv.org/abs/cs/0202014

- id: graysonAutomaticReproductionWorkflows2023
  abstract: >-
    Workflows make it easier for scientists to assemble computational
    experiments consisting of many disparate components. However, those
    disparate components also increase the probability that the computational
    experiment fails to be reproducible. Even if software is reproducible today,
    it may become irreproducible tomorrow without the software itself changing
    at all, because of the constantly changing software environment in which the
    software is run. To alleviate irreproducibility, workflow engines integrate
    with container engines. Additionally, communities that sprung up around
    workflow engines started to host registries for workflows that follow
    standards. These standards reduce the effort needed to make workflows
    automatically reproducible. In this paper, we study automatic reproduction
    of workflows from two registries, focusing on non-crashing executions. The
    experimental data lets us analyze the upper bound to which workflow engines
    could achieve reproducibility. We identify lessons learned in achieving
    reproducibility in practice.
  accessed:
    - year: 2024
      month: 1
      day: 20
  author:
    - family: Grayson
      given: Samuel
    - family: Marinov
      given: Darko
    - family: Katz
      given: Daniel S.
    - family: Milewicz
      given: Reed
  citation-key: graysonAutomaticReproductionWorkflows2023
  collection-title: ACM REP '23
  container-title: Proceedings of the 2023 ACM Conference on Reproducibility and Replicability
  DOI: 10.1145/3589806.3600037
  event-place: New York, NY, USA
  ISBN: '9798400701764'
  issued:
    - year: 2023
      month: 6
      day: 28
  page: 74–84
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Automatic Reproduction of Workflows in the Snakemake Workflow Catalog and
    nf-core Registries
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3589806.3600037

- id: graysonBenchmarkSuitePerformance2024
  accessed:
    - year: 2024
      month: 10
      day: 5
  author:
    - family: Grayson
      given: Samuel
    - family: Aguilar
      given: Faustino
    - family: Milewicz
      given: Reed
    - family: Katz
      given: Daniel S.
    - family: Marinov
      given: Darko
  citation-key: graysonBenchmarkSuitePerformance2024
  container-title: Proceedings of the 2nd ACM Conference on Reproducibility and Replicability
  DOI: 10.1145/3641525.3663627
  event-place: Rennes France
  event-title: 'ACM REP ''24: ACM Conference on Reproducibility and Replicability'
  ISBN: '9798400705304'
  issued:
    - year: 2024
      month: 6
      day: 18
  language: en
  page: 85-95
  publisher: ACM
  publisher-place: Rennes France
  source: DOI.org (Crossref)
  title: >-
    A benchmark suite and performance analysis of user-space provenance
    collectors
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3641525.3663627

- id: graysonEvaluatingSystemLevelProvenance2023
  abstract: >-
    Tracking provenance has many applications in computational science,
    especially for im-

    proving reproducibility, but it is not yet widely used in practice. In this
    report, we execute a literature

    rapid review to find system-level provenance tools and methods for use in
    practice based on the method of

    collection, source availability, and platform compatibility.
  author:
    - family: Grayson
      given: Samuel
    - family: Reed
      given: Milewicz
  citation-key: graysonEvaluatingSystemLevelProvenance2023
  collection-title: Computer Science Research Institute Summer Proceedings 2023
  event-place: Albuquerque, NM and Livermore, CA
  issued:
    - year: 2023
  language: en
  number: SAND2023-13916R
  page: 172–181
  publisher: Sandia National Laboratories
  publisher-place: Albuquerque, NM and Livermore, CA
  source: Zotero
  title: Evaluating System-Level Provenance Tools for Practical Use
  type: report
  URL: >-
    https://www.sandia.gov/ccr/csri-summer-programs/computer-science-research-institute-summer-proceedings-2023/

- id: graysonWantedStandardsAutomatic2023
  abstract: >-
    Those seeking to reproduce a computational experiment often need to manually
    look at the code to see how to build necessary libraries, configure
    parameters, find data, and invoke the experiment; it is not automatic.
    Automatic reproducibility is a more stringent goal, but working towards it
    would benefit the community. This work discusses a machine-readable language
    for specifying how to execute a computational experiment. We invite
    interested stakeholders to discuss this language at
    https://github.com/charmoniumQ/execution-description .
  accessed:
    - year: 2023
      month: 8
      day: 1
  author:
    - family: Grayson
      given: Samuel
    - family: Milewicz
      given: Reed
    - family: Teves
      given: Joshua
    - family: Katz
      given: Daniel S.
    - family: Marinov
      given: Darko
  citation-key: graysonWantedStandardsAutomatic2023
  container-title: Software Engineering for Research Science (SE4RS)
  DOI: 10.48550/arXiv.2307.11383
  event-place: Portland, OR, USA
  issued:
    - year: 2023
      month: 7
      day: 21
  publisher: arXiv
  publisher-place: Portland, OR, USA
  source: arXiv.org
  title: 'Wanted: standards for automatic reproducibility of computational experiments'
  title-short: Wanted
  type: paper-conference
  URL: http://arxiv.org/abs/2307.11383

- id: greenlandStatisticalTestsValues2016
  abstract: >-
    Misinterpretation and abuse of statistical tests, confidence intervals, and
    statistical power have been decried for decades, yet remain rampant. A key
    problem is that there are no interpretations of these concepts that are at
    once simple, intuitive, correct, and foolproof. Instead, correct use and
    interpretation of these statistics requires an attention to detail which
    seems to tax the patience of working scientists. This high cognitive demand
    has led to an epidemic of shortcut definitions and interpretations that are
    simply wrong, sometimes disastrously so-and yet these misinterpretations
    dominate much of the scientific literature. In light of this problem, we
    provide definitions and a discussion of basic statistics that are more
    general and critical than typically found in traditional introductory
    expositions. Our goal is to provide a resource for instructors, researchers,
    and consumers of statistics whose knowledge of statistical theory and
    technique may be limited but who wish to avoid and spot misinterpretations.
    We emphasize how violation of often unstated analysis protocols (such as
    selecting analyses for presentation based on the P values they produce) can
    lead to small P values even if the declared test hypothesis is correct, and
    can lead to large P values even if that hypothesis is incorrect. We then
    provide an explanatory list of 25 misinterpretations of P values, confidence
    intervals, and power. We conclude with guidelines for improving statistical
    interpretation and reporting.
  author:
    - family: Greenland
      given: Sander
    - family: Senn
      given: Stephen J.
    - family: Rothman
      given: Kenneth J.
    - family: Carlin
      given: John B.
    - family: Poole
      given: Charles
    - family: Goodman
      given: Steven N.
    - family: Altman
      given: Douglas G.
  citation-key: greenlandStatisticalTestsValues2016
  container-title: European Journal of Epidemiology
  container-title-short: Eur J Epidemiol
  DOI: 10.1007/s10654-016-0149-3
  ISSN: 1573-7284
  issue: '4'
  issued:
    - year: 2016
      month: 4
  language: eng
  page: 337-350
  PMCID: PMC4877414
  PMID: '27209009'
  source: PubMed
  title: >-
    Statistical tests, P values, confidence intervals, and power: a guide to
    misinterpretations
  title-short: Statistical tests, P values, confidence intervals, and power
  type: article-journal
  volume: '31'

- id: greenTruthBayesianPriors2017
  abstract: >-
    Have you ever thought about how strong a prior is compared to observed data?
    It’s not an entirely easy thing to conceptualize. In order to…
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Green
      given: Michael
  citation-key: greenTruthBayesianPriors2017
  container-title: Medium
  issued:
    - year: 2017
      month: 8
      day: 2
  language: en
  note: 'interest: 90'
  title: The truth about Bayesian priors and overfitting
  type: webpage
  URL: >-
    https://towardsdatascience.com/the-truth-about-bayesian-priors-and-overfitting-84e24d3a1153

- id: gregwilsonNotShelvesWhat2016
  abstract: >-
    Hundreds of books about writing compilers are currently on the market, but
    there are only three about writing debuggers. Spreadsheets are used to do
    calculations more often than every other kind of tool combined, but
    thirty-five years after their invention, version control systems still can't
    handle them. Everyone thinks we should teach children how to program, but
    undergraduate courses on computing education are practically nonexistent.
    This talk will explore what these gaps in our books, tools, and courses tell
    us about the state of computing today, and about what it could look like
    tomorrow.


    Greg Wilson is the co-founder of Software Carpentry, a volunteer
    organization that teaches basic computing skills to researchers in a wide
    range of disciplines. He has worked for 30 years in both industry and
    academia, and is the author or editor of several books on computing
    (including the 2008 Jolt Award winner "Beautiful Code") and two for
    children. Greg received a Ph.D. in Computer Science from the University of
    Edinburgh in 1993.
  accessed:
    - year: 2023
      month: 2
      day: 24
  author:
    - literal: Greg Wilson
  citation-key: gregwilsonNotShelvesWhat2016
  event-title: Software Carpentry
  issued:
    - year: 2016
      month: 10
      day: 11
  title: >-
    Not on the Shelves: What Nonexistent Books, Tools, and Courses Can Tell Us
    about Ourselves
  title-short: Not on the Shelves
  type: speech
  URL: https://www.youtube.com/watch?v=vx0DUiv1Gvw

- id: grenningPlanningPokerHow2002
  author:
    - family: Grenning
      given: James
  citation-key: grenningPlanningPokerHow2002
  container-title: 'Hawthorn Woods: Renaissance Software Consulting'
  issued:
    - year: 2002
  note: 'interest: 30'
  page: 22-23
  title: Planning poker or how to avoid analysis paralysis while release planning
  type: article-journal
  volume: '3'

- id: grothAnatomyNanopublication2010
  abstract: >-
    As the amount of scholarly communication increases, it is increasingly
    difficult for specific core scientific statements to be found, connected and
    curated. Additionally, the redundancy of these statements in multiple fora
    makes it difficult to deter
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Groth
      given: Paul
    - family: Gibson
      given: Andrew
    - family: Velterop
      given: Jan
  citation-key: grothAnatomyNanopublication2010
  container-title: Information Services & Use
  DOI: 10.3233/ISU-2010-0613
  ISSN: 0167-5265
  issue: 1-2
  issued:
    - year: 2010
      month: 1
      day: 1
  language: en
  page: 51-56
  publisher: IOS Press
  source: content.iospress.com
  title: The anatomy of a nanopublication
  type: article-journal
  URL: https://content.iospress.com/articles/information-services-and-use/isu613
  volume: '30'

- id: gruberEmpiricalStudyFlaky2021
  abstract: >-
    Tests that cause spurious failures without any code changes, i.e., flaky
    tests, hamper regression testing, increase maintenance costs, may shadow
    real bugs, and decrease trust in tests. While the prevalence and importance
    of flakiness is well established, prior research focused on Java projects,
    thus raising the question of how the findings generalize. In order to
    provide a better understanding of the role of flakiness in software
    development beyond Java, we empirically study the prevalence, causes, and
    degree of flakiness within software written in Python, one of the currently
    most popular programming languages. For this, we sampled 22 352 open source
    projects from the popular PyPI package index, and analyzed their 876 186
    test cases for flakiness. Our investigation suggests that flakiness is
    equally prevalent in Python as it is in Java. The reasons, however, are
    different: Order dependency is a much more dominant problem in Python,
    causing 59 % of the 7 571 flaky tests in our dataset. Another 28 % were
    caused by test infrastructure problems, which represent a previously
    undocumented cause of flakiness. The remaining 13 % can mostly be attributed
    to the use of network and randomness APIs by the projects, which is
    indicative of the type of software commonly written in Python. Our data also
    suggests that finding flaky tests requires more runs than are often done in
    the literature: A 95 % confidence that a passing test case is not flaky on
    average would require 170 reruns.
  author:
    - family: Gruber
      given: Martin
    - family: Lukasczyk
      given: Stephan
    - family: Kroiß
      given: Florian
    - family: Fraser
      given: Gordon
  citation-key: gruberEmpiricalStudyFlaky2021
  container-title: >-
    2021 14th IEEE Conference on Software Testing, Verification and Validation
    (ICST)
  DOI: 10.1109/ICST49551.2021.00026
  event-title: >-
    2021 14th IEEE Conference on Software Testing, Verification and Validation
    (ICST)
  ISSN: 2159-4848
  issued:
    - year: 2021
      month: 4
  note: 'intersest: 98'
  page: 148-158
  source: IEEE Xplore
  title: An Empirical Study of Flaky Tests in Python
  type: paper-conference

- id: gruenpeterDefiningResearchSoftware2021
  abstract: >-
    Software is essential in modern research; it plays vital roles at multiple
    stages of the research lifecycle. The term Research Software is widely used
    in the academic community but, what do we mean when we use these terms?
    Software and research? When you think of software, you may think of a
    digital object that is executed on a machine. Yet software is more than just
    this, it is a complex and evolving artifact. It may be a concept or a
    project designed to solve a puzzle by a team or a community that develops
    its functionalities and algorithms, which might not be digital objects.
    Furthermore, the software artifacts are digital objects, e.g., executables
    and source code files for different environments. These digital artifacts,
    which are used in a scholarly setting, might be important in the research
    process, but should all these be considered Research Software? This report
    is the result of a discussion examining the scope of the community
    definition of the FAIR principles for Research Software as part of the work
    in the FAIR for Research Software working group (FAIR4RS). We aim to clarify
    the scope of the FAIR principles by identifying which software artifacts the
    FAIR principles should apply to. This discussion portrayed a complex
    landscape of software uses in research and existing definitions that can
    help to better understand the complexity of different types of software in
    academia. Finally we determine the scope of the FAIR4RS with a short and
    concise definition of Research Software as a separate metaphor of software
    in research.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Gruenpeter
      given: Morane
    - family: Katz
      given: Daniel S.
    - family: Lamprecht
      given: Anna-Lena
    - family: Honeyman
      given: Tom
    - family: Garijo
      given: Daniel
    - family: Struck
      given: Alexander
    - family: Niehues
      given: Anna
    - family: Martinez
      given: Paula Andrea
    - family: Castro
      given: Leyla Jael
    - family: Rabemanantsoa
      given: Tovo
    - family: Chue Hong
      given: Neil P.
    - family: Martinez-Ortiz
      given: Carlos
    - family: Sesink
      given: Laurents
    - family: Liffers
      given: Matthias
    - family: Fouilloux
      given: Anne Claire
    - family: Erdmann
      given: Chris
    - family: Peroni
      given: Silvio
    - family: Martinez Lavanchy
      given: Paula
    - family: Todorov
      given: Ilian
    - family: Sinha
      given: Manodeep
  citation-key: gruenpeterDefiningResearchSoftware2021
  DOI: 10.5281/zenodo.5504016
  issued:
    - year: 2021
      month: 9
      day: 13
  publisher: Zenodo
  source: Zenodo
  title: 'Defining Research Software: a controversial discussion'
  title-short: Defining Research Software
  type: report
  URL: https://zenodo.org/record/5504016

- id: gueretFindingAchillesHeel2010
  abstract: >-
    The Web of Data is increasingly becoming an important infrastructure for
    such diverse sectors as entertainment, government, e-commerce and science.
    As a result, the robustness of this Web of Data is now crucial. Prior
    studies show that the Web of Data is strongly dependent on a small number of
    central hubs, making it highly vulnerable to single points of failure. In
    this paper, we present concepts and algorithms to analyse and repair the
    brittleness of the Web of Data. We apply these on a substantial subset of
    it, the 2010 Billion Triple Challenge dataset. We first distinguish the
    physical structure of the Web of Data from its semantic structure. For both
    of these structures, we then calculate their robustness, taking betweenness
    centrality as a robustness-measure. To the best of our knowledge, this is
    the first time that such robustness-indicators have been calculated for the
    Web of Data. Finally, we determine which links should be added to the Web of
    Data in order to improve its robustness most effectively. We are able to
    determine such links by interpreting the question as a very large
    optimisation problem and deploying an evolutionary algorithm to solve this
    problem. We believe that with this work, we offer an effective method to
    analyse and improve the most important structure that the Semantic Web
    community has constructed to date.
  author:
    - family: Guéret
      given: Christophe
    - family: Groth
      given: Paul
    - family: Harmelen
      given: Frank
      non-dropping-particle: van
    - family: Schlobach
      given: Stefan
  citation-key: gueretFindingAchillesHeel2010
  collection-title: Lecture Notes in Computer Science
  container-title: The Semantic Web – ISWC 2010
  DOI: 10.1007/978-3-642-17746-0_19
  editor:
    - family: Patel-Schneider
      given: Peter F.
    - family: Pan
      given: Yue
    - family: Hitzler
      given: Pascal
    - family: Mika
      given: Peter
    - family: Zhang
      given: Lei
    - family: Pan
      given: Jeff Z.
    - family: Horrocks
      given: Ian
    - family: Glimm
      given: Birte
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-17746-0
  issued:
    - year: 2010
  language: en
  page: 289-304
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: >-
    Finding the Achilles Heel of the Web of Data: Using Network Analysis for
    Link-Recommendation
  title-short: Finding the Achilles Heel of the Web of Data
  type: paper-conference

- id: guilloteauLongevityArtifactsLeading2024
  accessed:
    - year: 2024
      month: 9
      day: 4
  author:
    - family: Guilloteau
      given: Quentin
    - family: Ciorba
      given: Florina
    - family: Poquet
      given: Millian
    - family: Goepp
      given: Dorian
    - family: Richard
      given: Olivier
  citation-key: guilloteauLongevityArtifactsLeading2024
  container-title: Proceedings of the 2nd ACM Conference on Reproducibility and Replicability
  DOI: 10.1145/3641525.3663631
  event-place: Rennes France
  event-title: 'ACM REP ''24: ACM Conference on Reproducibility and Replicability'
  ISBN: '9798400705304'
  issued:
    - year: 2024
      month: 6
      day: 18
  language: en
  page: 121-133
  publisher: ACM
  publisher-place: Rennes France
  source: DOI.org (Crossref)
  title: >-
    Longevity of Artifacts in Leading Parallel and Distributed Systems
    Conferences: a Review of the State of the Practice in 2023
  title-short: >-
    Longevity of Artifacts in Leading Parallel and Distributed Systems
    Conferences
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3641525.3663631

- id: gundersenStateArtReproducibility2018
  abstract: >-
    Background: Research results in artificial intelligence (AI) are criticized
    for not being reproducible. Objective: To quantify the state of
    reproducibility of empirical AI research using six reproducibility metrics
    measuring three different degrees of reproducibility. Hypotheses: 1) AI
    research is not documented well enough to reproduce the reported results. 2)
    Documentation practices have improved over time. Method: The literature is
    reviewed and a set of variables that should be documented to enable
    reproducibility are grouped into three factors: Experiment, Data and Method.
    The metrics describe how well the factors have been documented for a paper.
    A total of 400 research papers from the conference series IJCAI and AAAI
    have been surveyed using the metrics. Findings: None of the papers document
    all of the variables. The metrics show that between 20% and 30% of the
    variables for each factor are documented. One of the metrics show
    statistically significant increase over time while the others show no
    change. Interpretation: The reproducibility scores decrease with in- creased
    documentation requirements. Improvement over time is found. Conclusion: Both
    hypotheses are supported.
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Gundersen
      given: Odd Erik
    - family: Kjensmo
      given: Sigbjørn
  citation-key: gundersenStateArtReproducibility2018
  container-title: Proceedings of the AAAI Conference on Artificial Intelligence
  container-title-short: AAAI
  DOI: 10.1609/aaai.v32i1.11503
  ISSN: 2374-3468, 2159-5399
  issue: '1'
  issued:
    - year: 2018
      month: 4
      day: 25
  source: DOI.org (Crossref)
  title: 'State of the Art: Reproducibility in Artificial Intelligence'
  title-short: State of the Art
  type: article-journal
  URL: https://ojs.aaai.org/index.php/AAAI/article/view/11503
  volume: '32'

- id: guntherGuerrillaCapacityPlanning2007
  abstract: >-
    Guerrilla Capacity Planning techniques are the only way to address the
    so-called 3-month planning horizon


    This book contains the first and only complete presentation of the
    "Universal Law of Computational Scaling"


    Learn how to apply the 3 themes: Guerrilla tactics, Guerrilla scalability,
    and Guerrilla victories


    Benefit from the detailed case studies showing how to do Guerrilla website
    and Internet planning


    Use the Guerrilla Manual fold-out as an authoritative source to convince
    your colleagues about capacity planning
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Gunther
      given: Neil J.
  citation-key: guntherGuerrillaCapacityPlanning2007
  DOI: 10.1007/978-3-540-31010-5
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-26138-4
  issued:
    - year: 2007
  language: en
  note: |-
    - http://www.perfdynamics.com/Manifesto/USLscalability.html
    interest: 76
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: Guerrilla Capacity Planning
  type: book
  URL: http://link.springer.com/10.1007/978-3-540-31010-5

- id: guntherHadoopSuperlinearScalability2015
  abstract: The perpetual motion of parallel performance.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Gunther
      given: Neil J.
    - family: Puglia
      given: Paul
    - family: Tomasette
      given: Kristofer
  citation-key: guntherHadoopSuperlinearScalability2015
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/2719919
  ISSN: 0001-0782
  issue: '4'
  issued:
    - year: 2015
      month: 3
      day: 23
  note: 'interest: 88'
  page: 46–55
  source: April 2015
  title: Hadoop superlinear scalability
  type: article-journal
  URL: https://doi.org/10.1145/2719919
  volume: '58'

- id: guoCDEUsingSystem2011
  abstract: >-
    It can be painfully hard to take software that runs on one person’s machine
    and get it to run on another machine. Online forums and mailing lists are
    filled with discussions of users' troubles with compiling, installing, and
    configuring software and their myriad of dependencies. To eliminate this
    dependency problem, we created a system called CDE that uses system call
    interposition to monitor the execution of x86-Linux programs and package up
    the Code, Data, and Environment required to run them on other x86-Linux
    machines. Creating a CDE package is completely automatic, and running
    programs within a package requires no installation, configuration, or root
    permissions. Hundreds of people in both academia and industry have used CDE
    to distribute software, demo prototypes, make their scientific experiments
    reproducible, run software natively on older Linux distributions, and deploy
    experiments to compute clusters.
  author:
    - family: Guo
      given: Philip
    - family: Engler
      given: Dawson
  citation-key: guoCDEUsingSystem2011
  container-title: 2011 USENIX Annual Technical Conference
  event-place: Portland, OR, USA
  event-title: USENIX Annual Technical Conference
  issued:
    - year: 2011
      month: 6
      day: 14
  publisher: USENIX
  publisher-place: Portland, OR, USA
  title: >-
    CDE: Using System Call Interposition to Automatically Create Portable
    Software Packages
  type: paper-conference
  URL: https://www.usenix.org/legacy/events/atc11/tech/final_files/GuoEngler.pdf

- id: guoUsingAutomaticPersistent2011
  abstract: >-
    Programmers across a wide range of disciplines (e.g., bioinformatics,
    neuroscience, econometrics, finance, data mining, information retrieval,
    machine learning) write scripts to parse, transform, process, and extract
    insights from data. To speed up iteration times, they split their analyses
    into stages and write extra code to save the intermediate results of each
    stage to files so that those results do not have to be re-computed in every
    subsequent run. As they explore and refine hypotheses, their scripts often
    create and process lots of intermediate data files. They need to properly
    manage the myriad of dependencies between their code and data files, or else
    their analyses will produce incorrect results. To enable programmers to
    iterate quickly without needing to manage intermediate data files, we added
    a set of dynamic analyses to the programming language interpreter so that it
    automatically memoizes (caches) the results of long-running pure function
    calls to disk, manages dependencies between code and on-disk data, and later
    re-uses memoized results, rather than re-executing those functions, when
    guaranteed safe to do so. We created an implementation for Python and show
    how it enables programmers to iterate faster on their data analysis scripts
    while writing less code and not having to manage dependencies between their
    code and datasets.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Guo
      given: Philip J.
    - family: Engler
      given: Dawson
  citation-key: guoUsingAutomaticPersistent2011
  collection-title: ISSTA '11
  container-title: >-
    Proceedings of the 2011 International Symposium on Software Testing and
    Analysis
  DOI: 10.1145/2001420.2001455
  event-place: New York, NY, USA
  ISBN: 978-1-4503-0562-4
  issued:
    - year: 2011
      month: 7
      day: 17
  page: 287–297
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Using automatic persistent memoization to facilitate data analysis scripting
  type: paper-conference
  URL: https://doi.org/10.1145/2001420.2001455

- id: guptaTestingSpeedScale2011
  author:
    - family: Gupta
      given: Pooja
    - family: Ivey
      given: Mark
    - family: Penix
      given: John
  citation-key: guptaTestingSpeedScale2011
  issued:
    - year: 2011
      month: 6
      day: 7
  title: Testing at the speed and scale of Google
  type: post-weblog
  URL: >-
    https://google-engtools.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html

- id: guyl.steelejr.GrowingLanguage1998
  abstract: >-
    This talk argues that most programming languages have impoverished
    vocabularies that make

    them awkward to use. The talk also makes a case for designing facilities
    into programming

    languages to allow the programmer, not just the language designer, to add
    new facilities to the

    language, to grow the language for the benefit of other programmers. What
    makes the talk

    unusual is that it is reflexive, or metacircular, serving as an example of
    the phenomenon

    being described.
  author:
    - literal: Guy L. Steele Jr.
  citation-key: guyl.steelejr.GrowingLanguage1998
  event-place: OOPSLA
  genre: Lecture
  issued:
    - year: 1998
  publisher-place: OOPSLA
  title: Growing A Language
  type: manuscript
  URL: >-
    https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.68.7907&rep=rep1&type=pdf#page=256

- id: gyoriEvaluatingRegressionTest2018
  abstract: >-
    Regression testing in very large software ecosystems is notoriously costly,
    requiring computational resources that even large corporations struggle to
    cope with. Very large ecosystems contain thousands of rapidly evolving,
    interconnected projects where client projects transitively depend on library
    projects. Regression test selection (RTS) reduces regression testing costs
    by rerunning only tests whose pass/fail behavior may flip after code
    changes. For single projects, researchers showed that class-level RTS is
    more effective than lower method-or statement-level RTS. Meanwhile, several
    very large ecosystems in industry, e.g., at Facebook, Google, and Microsoft,
    perform project-level RTS, rerunning tests in a changed library and in all
    its transitive clients. However, there was no previous study of the
    comparative benefits of class-level and project-level RTS in such
    ecosystems. We evaluate RTS opportunities in the MAVEN Central open-source
    ecosystem. There, some popular libraries have up to 924589 clients; in turn,
    clients can depend on up to 11190 libraries. We sampled 408 popular projects
    and found that 202 (almost half) cannot update to latest library versions
    without breaking compilation or tests. If developers want to detect these
    breakages earlier, they need to run very many tests. We compared four
    variants of class-level RTS with project-level RTS in MAVEN Central. The
    results showed that class-level RTS may be an order of magnitude less costly
    than project-level RTS in very large ecosystems. Specifically, various
    class-level RTS variants select, on average, 7.8%-17.4% of tests selected by
    project-level RTS.
  author:
    - family: Gyori
      given: Alex
    - family: Legunsen
      given: Owolabi
    - family: Hariri
      given: Farah
    - family: Marinov
      given: Darko
  citation-key: gyoriEvaluatingRegressionTest2018
  container-title: >-
    2018 IEEE 29th International Symposium on Software Reliability Engineering
    (ISSRE)
  DOI: 10.1109/ISSRE.2018.00022
  event-title: >-
    2018 IEEE 29th International Symposium on Software Reliability Engineering
    (ISSRE)
  ISSN: 2332-6549
  issued:
    - year: 2018
      month: 10
  page: 112-122
  source: IEEE Xplore
  title: >-
    Evaluating Regression Test Selection Opportunities in a Very Large
    Open-Source Ecosystem
  type: paper-conference

- id: gyoriNonDexToolDetecting2016
  abstract: >-
    We present NonDex, a tool for detecting and debugging wrong assumptions on
    Java APIs. Some APIs have underdetermined specifications to allow
    implementations to achieve different goals, e.g., to optimize performance.
    When clients of such APIs assume stronger-than-specified guarantees, the
    resulting client code can fail. For example, HashSet’s iteration order is
    underdetermined, and code assuming some implementation-specific iteration
    order can fail. NonDex helps to proactively detect and debug such wrong
    assumptions. NonDex performs detection by randomly exploring different
    behaviors of underdetermined APIs during test execution. When a test fails
    during exploration, NonDex searches for the invocation instance of the API
    that caused the failure. NonDex is open source, well-integrated with Maven,
    and also runs from the command line. During our experiments with the NonDex
    Maven plugin, we detected 21 new bugs in eight Java projects from GitHub,
    and, using the debugging feature of NonDex, we identified the underlying
    wrong assumptions for these 21 new bugs and 54 previously detected bugs. We
    opened 13 pull requests; developers already accepted 12, and one project
    changed the continuous-integration configuration to run NonDex on every
    push. The demo video is at: https://youtu.be/h3a9ONkC59c
  accessed:
    - year: 2022
      month: 4
      day: 10
  author:
    - family: Gyori
      given: Alex
    - family: Lambeth
      given: Ben
    - family: Shi
      given: August
    - family: Legunsen
      given: Owolabi
    - family: Marinov
      given: Darko
  citation-key: gyoriNonDexToolDetecting2016
  collection-title: FSE 2016
  container-title: >-
    Proceedings of the 2016 24th ACM SIGSOFT International Symposium on
    Foundations of Software Engineering
  DOI: 10.1145/2950290.2983932
  event-place: New York, NY, USA
  ISBN: 978-1-4503-4218-6
  issued:
    - year: 2016
      month: 11
      day: 1
  page: 993–997
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    NonDex: a tool for detecting and debugging wrong assumptions on Java API
    specifications
  title-short: NonDex
  type: paper-conference
  URL: https://doi.org/10.1145/2950290.2983932

- id: haferAssessingOpenSource2009
  abstract: >-
    Academic computer science has an odd relationship with software: Publishing
    papers about software is considered a distinctly stronger contribution than
    publishing the software. The historical reasons for this paradox no longer
    apply, but their legacy remains. This limits researchers who see the
    open-source software movement as an opportunity to make a scholarly
    contribution. Expanded definitions of scholarship acknowledge both
    application and discovery as important components. 

    One obstacle remains: evaluation. To raise software to the status of a
    first-class contribution, we propose "best practices" for the evaluation of
    the scholarly contribution of open-source software. 

    Typically, scholars who develop software do not include it as a primary
    contribution for performance reviews. Instead, they write articles about the
    software and present the articles as contributions. This conflation of
    articles and software serves neither medium well. An article describes an
    original intellectual contribution consisting of an idea, the argument for
    its importance and correctness, and supporting data. In contrast, software
    is more often an implementation of prior ideas in a usable form. It bridges
    the often considerable gap between an idea and the practical application of
    that idea. The original idea and its implementation represent distinct kinds
    of contribution. 

    The critical gap is the perceived incomparability of these two
    contributions. Lacking a concise description adapted to the traditional
    practices of performance review committees, software is difficult to
    evaluate as a scholarly contribution and is often relegated to second-class
    status. We propose a framework for common assessment based on widely
    accepted definitions of scholarship. Within this general framework, we
    consider the material and procedures that a performance review committee
    uses to evaluate a publication. We then describe how software can be
    summarized in a compatible form of bibliographic citation and supplementary
    material.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Hafer
      given: Lou
    - family: Kirkpatrick
      given: Arthur E.
  citation-key: haferAssessingOpenSource2009
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/1610252.1610285
  ISSN: 0001-0782, 1557-7317
  issue: '12'
  issued:
    - year: 2009
      month: 12
  language: en
  note: 'interest: 89'
  page: 126-129
  source: DOI.org (Crossref)
  title: Assessing open source software as a scholarly contribution
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1610252.1610285
  volume: '52'

- id: hahnMultiscaleInitialConditions2011
  abstract: >-
    We discuss a new algorithm to generate multi-scale initial conditions with
    multiple levels of refinements for cosmological "zoom-in" simulations. The
    method uses an adaptive convolution of Gaussian white noise with a real
    space transfer function kernel together with an adaptive multi-grid Poisson
    solver to generate displacements and velocities following first (1LPT) or
    second order Lagrangian perturbation theory (2LPT). The new algorithm
    achieves RMS relative errors of order 10^(-4) for displacements and
    velocities in the refinement region and thus improves in terms of errors by
    about two orders of magnitude over previous approaches. In addition, errors
    are localized at coarse-fine boundaries and do not suffer from Fourier-space
    induced interference ringing. An optional hybrid multi-grid and Fast Fourier
    Transform (FFT) based scheme is introduced which has identical Fourier space
    behaviour as traditional approaches. Using a suite of re-simulations of a
    galaxy cluster halo our real space based approach is found to reproduce
    correlation functions, density profiles, key halo properties and subhalo
    abundances with per cent level accuracy. Finally, we generalize our approach
    for two-component baryon and dark-matter simulations and demonstrate that
    the power spectrum evolution is in excellent agreement with linear
    perturbation theory. For initial baryon density fields, it is suggested to
    use the local Lagrangian approximation in order to generate a density field
    for mesh based codes that is consistent with Lagrangian perturbation theory
    instead of the current practice of using the Eulerian linearly scaled
    densities.
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Hahn
      given: Oliver
    - family: Abel
      given: Tom
  citation-key: hahnMultiscaleInitialConditions2011
  container-title: Monthly Notices of the Royal Astronomical Society
  container-title-short: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1111/j.1365-2966.2011.18820.x
  ISSN: '00358711'
  issue: '3'
  issued:
    - year: 2011
      month: 8
      day: 11
  page: 2101-2121
  source: Silverchair
  title: Multi-scale initial conditions for cosmological simulations
  type: article-journal
  URL: https://doi.org/10.1111/j.1365-2966.2011.18820.x
  volume: '415'

- id: haidtWhy10Years2022
  abstract: It’s not just a phase.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Haidt
      given: Jonathan
  citation-key: haidtWhy10Years2022
  container-title: The Atlantic
  issued:
    - year: 2022
      month: 4
      day: 11
  language: en
  note: 'interest: 73'
  section: Ideas
  title: Why the Past 10 Years of American Life Have Been Uniquely Stupid
  type: article-newspaper
  URL: >-
    https://www.theatlantic.com/magazine/archive/2022/05/social-media-democracy-trust-babel/629369/

- id: halpinWhenOwlSameAs2010
  abstract: >-
    In Linked Data, the use of owl:sameAs is ubiquitous in interlinking
    data-sets. There is however, ongoing discussion about its use, and potential
    misuse, particularly with regards to interactions with inference. In fact,
    owl:sameAs can be viewed as encoding only one point on a scale of
    similarity, one that is often too strong for many of its current uses. We
    describe how referentially opaque contexts that do not allow inference
    exist, and then outline some varieties of referentially-opaque alternatives
    to owl:sameAs. Finally, we report on an empirical experiment over randomly
    selected owl:sameAs statements from the Web of data. This theoretical
    apparatus and experiment shed light upon how owl:sameAs is being used (and
    misused) on the Web of data.
  author:
    - family: Halpin
      given: Harry
    - family: Hayes
      given: Patrick J.
    - family: McCusker
      given: Jamie P.
    - family: McGuinness
      given: Deborah L.
    - family: Thompson
      given: Henry S.
  citation-key: halpinWhenOwlSameAs2010
  collection-title: Lecture Notes in Computer Science
  container-title: The Semantic Web – ISWC 2010
  DOI: 10.1007/978-3-642-17746-0_20
  editor:
    - family: Patel-Schneider
      given: Peter F.
    - family: Pan
      given: Yue
    - family: Hitzler
      given: Pascal
    - family: Mika
      given: Peter
    - family: Zhang
      given: Lei
    - family: Pan
      given: Jeff Z.
    - family: Horrocks
      given: Ian
    - family: Glimm
      given: Birte
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-17746-0
  issued:
    - year: 2010
  language: en
  page: 305-320
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: 'When owl:sameAs Isn’t the Same: An Analysis of Identity in Linked Data'
  title-short: When owl
  type: paper-conference

- id: hammondBewareXHTML2005a
  abstract: >-
    If you're a web developer, you've probably worked a lot with XHTML, the
    markup language developed in 1999 to implement HTML as an XML format. Most
    people who use and promote XHTML do so because they think it's the “next
    version” of HTML, and they may have heard of some benefits here and there.
    But there is a lot more to it than you may realize, and if you're using it
    on your website, even if it validates, you are probably using it
    incorrectly.


    I believe that XHTML has many good potential applications, and I hope it
    continues to thrive as a standard. This is precisely why I have written this
    article. The state of XHTML on the Web today is more broken than the state
    of HTML, and most people don't realize because the major browsers are using
    classic HTML parsers that hide the problems. Even among the few sites that
    know how to trigger the XML parser, the authors tend to overlook some
    important issues. If you really hope for the XHTML standard to succeed, you
    should read this article carefully.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Hammond
      given: David
  citation-key: hammondBewareXHTML2005a
  container-title: Web Devout
  issued:
    - year: 2005
      month: 12
      day: 6
  title: Beware of XHTML
  type: post-weblog
  URL: http://www.webdevout.net/articles/beware-of-xhtml

- id: haneyNPMLeftpadHave
  abstract: >-
    Intro Okay developers, time to have a serious talk. As you are probably
    already aware, this week React, Babel, and a bunch of other high-profile
    packages on NPM broke. The reason they broke is rather astounding:

    A simple NPM package called left-pad that was a dependency of their code.

    left-pad, at the time of writing this, has 11 stars on GitHub. The entire
    package is 11 simple lines that implement a basic left-pad string function.
  accessed:
    - year: 2022
      month: 4
      day: 13
  author:
    - family: Haney
      given: David
  citation-key: haneyNPMLeftpadHave
  container-title: David Haney
  language: en
  title: 'NPM & left-pad: Have We Forgotten How To Program?'
  title-short: NPM & left-pad
  type: webpage
  URL: https://www.davidhaney.io/npm-left-pad-have-we-forgotten-how-to-program/

- id: hannartCausalCounterfactualTheory2016
  abstract: >-
    Abstract The emergence of clear semantics for causal claims and of a sound
    logic for causal reasoning is relatively recent, with the consolidation over
    the past decades of a coherent theoretical corpus of definitions, concepts,
    and methods of general applicability that is anchored into counterfactuals.
    The latter corpus has proved to be of high practical interest in numerous
    applied fields (e.g., epidemiology, economics, and social science). In spite
    of their rather consensual nature and proven efficacy, these definitions and
    methods are to a large extent not used in detection and attribution (D&A).
    This article gives a brief overview of the main concepts underpinning the
    causal theory and proposes some methodological extensions for the causal
    attribution of weather and climate-related events that are rooted into the
    latter. Implications for the formulation of causal claims and their
    uncertainty are finally discussed.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Hannart
      given: A.
    - family: Pearl
      given: J.
    - family: Otto
      given: F. E. L.
    - family: Naveau
      given: P.
    - family: Ghil
      given: M.
  citation-key: hannartCausalCounterfactualTheory2016
  container-title: Bulletin of the American Meteorological Society
  DOI: 10.1175/BAMS-D-14-00034.1
  ISSN: 0003-0007, 1520-0477
  issue: '1'
  issued:
    - year: 2016
      month: 1
      day: 1
  language: EN
  note: 'interest: 95'
  page: 99-110
  publisher: American Meteorological Society
  section: Bulletin of the American Meteorological Society
  source: journals.ametsoc.org
  title: >-
    Causal Counterfactual Theory for the Attribution of Weather and
    Climate-Related Events
  type: article-journal
  URL: https://journals.ametsoc.org/view/journals/bams/97/1/bams-d-14-00034.1.xml
  volume: '97'

- id: hannayHowScientistsDevelop2009
  abstract: >-
    New knowledge in science and engineering relies increasingly on results
    produced by scientific software. Therefore, knowing how scientists develop
    and use software in their research is critical to assessing the necessity
    for improving current development practices and to making decisions about
    the future allocation of resources. To that end, this paper presents the
    results of a survey conducted online in October-December 2008 which received
    almost 2000 responses. Our main conclusions are that (1) the knowledge
    required to develop and use scientific software is primarily acquired from
    peers and through self-study, rather than from formal education and
    training; (2) the number of scientists using supercomputers is small
    compared to the number using desktop or intermediate computers; (3) most
    scientists rely primarily on software with a large user base; (4) while many
    scientists believe that software testing is important, a smaller number
    believe they have sufficient understanding about testing concepts; and (5)
    that there is a tendency for scientists to rank standard software
    engineering concepts higher if they work in large software development
    projects and teams, but that there is no uniform trend of association
    between rank of importance of software engineering concepts and project/team
    size.
  author:
    - family: Hannay
      given: Jo Erskine
    - family: MacLeod
      given: Carolyn
    - family: Singer
      given: Janice
    - family: Langtangen
      given: Hans Petter
    - family: Pfahl
      given: Dietmar
    - family: Wilson
      given: Greg
  citation-key: hannayHowScientistsDevelop2009
  container-title: >-
    2009 ICSE Workshop on Software Engineering for Computational Science and
    Engineering
  DOI: 10.1109/SECSE.2009.5069155
  event-title: >-
    2009 ICSE Workshop on Software Engineering for Computational Science and
    Engineering
  issued:
    - year: 2009
      month: 5
  note: 'interest: 90'
  page: 1-8
  source: IEEE Xplore
  title: How do scientists develop and use scientific software?
  type: paper-conference

- id: hannaySystematicReviewTheory2007
  abstract: >-
    Empirically based theories are generally perceived as foundational to
    science. However, in many disciplines, the nature, role and even the
    necessity of theories remain matters for debate, particularly in young or
    practical disciplines such as software engineering. This article reports a
    systematic review of the explicit use of theory in a comprehensive set of
    103 articles reporting experiments, from of a total of 5,453 articles
    published in major software engineering journals and conferences in the
    decade 1993-2002. Of the 103 articles, 24 use a total of 40 theories in
    various ways to explain the cause-effect relationship(s) under
    investigation. The majority of these use theory in the experimental design
    to justify research questions and hypotheses, some use theory to provide
    post hoc explanations of their results, and a few test or modify theory. A
    third of the theories are proposed by authors of the reviewed articles. The
    interdisciplinary nature of the theories used is greater than that of
    research in software engineering in general. We found that theory use and
    awareness of theoretical issues are present, but that theory-driven research
    is, as yet, not a major issue in empirical software engineering. Several
    articles comment explicitly on the lack of relevant theory. We call for an
    increased awareness of the potential benefits of involving theory, when
    feasible. To support software engineering researchers who wish to use
    theory, we show which of the reviewed articles on which topics use which
    theories for what purposes, as well as details of the theories'
    characteristics.
  accessed:
    - year: 2022
      month: 7
      day: 25
  author:
    - family: Hannay
      given: Jo E.
    - family: Sjoberg
      given: Dag I.K.
    - family: Dyba
      given: Tore
  citation-key: hannaySystematicReviewTheory2007
  container-title: IEEE Transactions on Software Engineering
  container-title-short: IIEEE Trans. Software Eng.
  DOI: 10.1109/TSE.2007.12
  ISSN: 0098-5589, 1939-3520
  issue: '2'
  issued:
    - year: 2007
      month: 2
  page: 87-107
  source: DOI.org (Crossref)
  title: A Systematic Review of Theory Use in Software Engineering Experiments
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/4052585/
  volume: '33'

- id: hanPROVIOOCentricProvenance2022
  abstract: >-
    cData provenance, or data lineage, describes the life cycle of data. In
    scientific workflows on HPC systems, scientists often seek diverse
    provenance (e.g., origins of data products, usage patterns of datasets).
    Unfortunately, existing provenance solutions cannot address the challenges
    due to their incompatible provenance models and/or system implementations.
    In this paper, we analyze three representative scientific workflows in
    collaboration with the domain scientists to identify concrete provenance
    needs. Based on the first-hand analysis, we propose a provenance framework
    called PROV-IO, which includes an I/O-centric provenance model for
    describing scientific data and the associated I/O operations and
    environments precisely. Moreover, we build a prototype of PROV-IO to enable
    end-to-end provenance support on real HPC systems with little manual effort.
    The PROV-IO framework provides flexibility in selecting various classes of
    provenance. Our experiments with realistic workflows show that PROV-IO can
    address the provenance needs of the domain scientists effectively with
    reasonable performance (e.g., less than 3.5% tracking overhead for most
    experiments). Moreover, PROV-IO outperforms a state-of-the-art system (i.e.,
    ProvLake) in our experiments.
  accessed:
    - year: 2024
      month: 2
      day: 14
  author:
    - family: Han
      given: Runzhou
    - family: Byna
      given: Suren
    - family: Tang
      given: Houjun
    - family: Dong
      given: Bin
    - family: Zheng
      given: Mai
  citation-key: hanPROVIOOCentricProvenance2022
  collection-title: HPDC '22
  container-title: >-
    Proceedings of the 31st International Symposium on High-Performance Parallel
    and Distributed Computing
  DOI: 10.1145/3502181.3531477
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9199-3
  issued:
    - year: 2022
      month: 6
      day: 27
  page: 213–226
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    PROV-IO: An I/O-Centric Provenance Framework for Scientific Data on HPC
    Systems
  title-short: PROV-IO
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3502181.3531477

- id: hassanOmegaLogHighFidelityAttack2020
  abstract: >-
    Recent advances in causality analysis have enabled investigators to trace
    multi-stage attacks using whole- system provenance graphs. Based on
    system-layer audit logs (e.g., syscalls), these approaches omit vital
    sources of application context (e.g., email addresses, HTTP response codes)
    that can found in higher layers of the system. Although this information is
    often essential to understanding attack behaviors, incorporating this
    evidence into causal analysis engines is difficult due to the semantic gap
    that exists between system layers. To address this shortcoming, we propose
    the notion of universal provenance, which encodes all forensically-relevant
    causal dependencies regardless of their layer of origin. To transparently
    realize this vision on commodity systems, we present ωLOG (“Omega Log”), a
    provenance tracking mechanism that bridges the semantic gap between system
    and application logging contexts. ωLOG analyzes program binaries to identify
    and model application-layer logging behaviors, enabling application events
    to be accurately reconciled with system-layer accesses. ωLOG then intercepts
    applications’ runtime logging activities and grafts those events onto the
    system-layer provenance graph, allowing investigators to reason more
    precisely about the nature of attacks. We demonstrate that ωLOG is
    widely-applicable to existing software projects and can transparently
    facilitate execution partitioning of dependency graphs without any training
    or developer intervention. Evaluation on real-world attack scenarios shows
    that universal provenance graphs are concise and rich with semantic
    information as compared to the state-of-the-art, with 12% average runtime
    overhead.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Hassan
      given: Wajih Ul
    - family: Noureddine
      given: Mohammad Ali
    - family: Datta
      given: Pubali
    - family: Bates
      given: Adam
  citation-key: hassanOmegaLogHighFidelityAttack2020
  container-title: Network and Distributed System Security Symposium
  issued:
    - year: 2020
      month: 1
  language: en
  source: par.nsf.gov
  title: >-
    OmegaLog: High-Fidelity Attack Investigation via Transparent Multi-layer Log
    Analysis
  title-short: OmegaLog
  type: article-journal
  URL: >-
    https://par.nsf.gov/biblio/10146531-omegalog-high-fidelity-attack-investigation-via-transparent-multi-layer-log-analysis

- id: hassanScalableClusterAuditing2018
  abstract: >-
    Investigating the nature of system intrusions in large distributed systems
    remains a notoriously difﬁcult challenge. While monitoring tools (e.g.,
    Firewalls, IDS) provide preliminary alerts through easy-to-use
    administrative interfaces, attack reconstruction still requires that
    administrators sift through gigabytes of system audit logs stored locally on
    hundreds of machines. At present, two fundamental obstacles prevent synergy
    between system-layer auditing and modern cluster monitoring tools: 1) the
    sheer volume of audit data generated in a data center is prohibitively
    costly to transmit to a central node, and 2) systemlayer auditing poses a
    “needle-in-a-haystack” problem, such that hundreds of employee hours may be
    required to diagnose a single intrusion.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Hassan
      given: Wajih Ul
    - family: Lemay
      given: Mark
    - family: Aguse
      given: Nuraini
    - family: Bates
      given: Adam
    - family: Moyer
      given: Thomas
  citation-key: hassanScalableClusterAuditing2018
  container-title: Proceedings 2018 Network and Distributed System Security Symposium
  DOI: 10.14722/ndss.2018.23141
  event-place: San Diego, CA
  event-title: Network and Distributed System Security Symposium
  ISBN: 978-1-891562-49-5
  issued:
    - year: 2018
  language: en
  publisher: Internet Society
  publisher-place: San Diego, CA
  source: DOI.org (Crossref)
  title: >-
    Towards Scalable Cluster Auditing through Grammatical Inference over
    Provenance Graphs
  type: paper-conference
  URL: >-
    https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-1_Hassan_paper.pdf

- id: heffernanAcademicExodusRole2019
  abstract: >-
    Recent studies argue that in the next 5 years, the higher education sector
    will see half to two-thirds of its academic workforce leave the academy due
    to retirement, career burnout, or job dissatisfaction. This study surveyed
    over 100 working academics in Australia, North America, and the United
    Kingdom to determine their aspirations for remaining within, or leaving, the
    academy. The study found that the professional development and career
    support available to academics played major roles in their career
    satisfaction. The study’s significance lies in highlighting the types of
    support academics most value. The paper also explores what motivates
    participants’ intentions to remain in, or leave, their current positions, or
    the academy entirely. This assessment occurs at a time when the literature
    indicates that a significant period of staff turnover is imminent.
  accessed:
    - year: 2022
      month: 5
      day: 31
  author:
    - family: Heffernan
      given: Troy A.
    - family: Heffernan
      given: Amanda
  citation-key: heffernanAcademicExodusRole2019
  container-title: Professional Development in Education
  container-title-short: Professional Development in Education
  DOI: 10.1080/19415257.2018.1474491
  ISSN: 1941-5257, 1941-5265
  issue: '1'
  issued:
    - year: 2019
      month: 1
  language: en
  page: 102-113
  source: DOI.org (Crossref)
  title: >-
    The academic exodus: the role of institutional support in academics leaving
    universities and the academy
  title-short: The academic exodus
  type: article-journal
  URL: https://www.tandfonline.com/doi/full/10.1080/19415257.2018.1474491
  volume: '45'

- id: hemsothWhatBadPOSIX2017
  abstract: >-
    POSIX I/O is almost universally agreed to be one of the most significant
    limitations standing in the way of I/O performance exascale system designs
    push
  accessed:
    - year: 2023
      month: 1
      day: 25
  author:
    - family: Hemsoth
      given: Nicole
  citation-key: hemsothWhatBadPOSIX2017
  container-title: The Next Platform
  issued:
    - year: 2017
      month: 9
      day: 11
  language: en-US
  title: What's So Bad About POSIX I/O?
  type: webpage
  URL: https://www.nextplatform.com/2017/09/11/whats-bad-posix-io/

- id: hendersonSoftwareEngineeringGoogle2020
  abstract: We catalog and describe Google's key software engineering practices.
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Henderson
      given: Fergus
  citation-key: hendersonSoftwareEngineeringGoogle2020
  container-title: arXiv:1702.01715 [cs]
  issued:
    - year: 2020
      month: 1
      day: 30
  source: arXiv.org
  title: Software Engineering at Google
  type: article-journal
  URL: http://arxiv.org/abs/1702.01715

- id: henkelShipwrightHumanintheLoopSystem2021
  abstract: >-
    Docker is a tool for lightweight OS-level virtualization. Docker images are
    created by performing a build, controlled by a source-level artifact called
    a Dockerfile. We studied Dockerfiles on GitHub, and-to our great
    surprise-found that over a quarter of the examined Dockerfiles failed to
    build (and thus to produce images). To address this problem, we propose
    SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken
    Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model
    to embed build logs and to cluster broken Dockerfiles. Using these clusters
    and a search-based procedure, we were able to design 13 rules for making
    automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted
    45 pull requests (with a 42.2% acceptance rate) to GitHub projects with
    broken Dockerfiles. Furthermore, in a "time-travel" analysis of broken
    Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs
    that were equivalent to human-authored patches in 22.77% of the cases we
    studied. Finally, we compared our work with recent, state-of-the-art, static
    Dockerfile analyses, and found that, while static tools detected possible
    build-failure-inducing issues in 20.6-33.8% of the files we examined,
    SHIPWRIGHT was able to detect possible issues in 73.25% of the files and,
    additionally, provide automated repairs for 18.9% of the files.
  author:
    - family: Henkel
      given: Jordan
    - family: Silva
      given: Denini
    - family: Teixeira
      given: Leopoldo
    - family: Amorim
      given: Marcelo
      non-dropping-particle: d’
    - family: Reps
      given: Thomas
  citation-key: henkelShipwrightHumanintheLoopSystem2021
  container-title: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
  DOI: 10.1109/ICSE43902.2021.00106
  event-title: 2021 IEEE/ACM 43rd International Conference on Software Engineering (ICSE)
  ISSN: 1558-1225
  issued:
    - year: 2021
      month: 5
  page: 1148-1160
  source: IEEE Xplore
  title: 'Shipwright: A Human-in-the-Loop System for Dockerfile Repair'
  title-short: Shipwright
  type: paper-conference

- id: henningSPECCPU2000Measuring2000
  abstract: >-
    As computers and software have become more powerful, it seems almost human
    nature to want the biggest and fastest toy you can afford. But how do you
    know if your toy is tops? Even if your application never does any I/O, it's
    not just the speed of the CPU that dictates performance. Cache, main memory,
    and compilers also play a role. Software applications also have differing
    performance requirements. So whom do you trust to provide this information?
    The Standard Performance Evaluation Corporation (SPEC) is a nonprofit
    consortium whose members include hardware vendors, software vendors,
    universities, customers, and consultants. SPEC's mission is to develop
    technically credible and objective component- and system-level benchmarks
    for multiple operating systems and environments, including high-performance
    numeric computing, Web servers, and graphical subsystems. On 30 June 2000,
    SPEC retired the CPU95 benchmark suite. Its replacement is CPU2000, a new
    CPU benchmark suite with 19 applications that have never before been in a
    SPEC CPU suite. The article discusses how SPEC developed this benchmark
    suite and what the benchmarks do.
  author:
    - family: Henning
      given: J.L.
  citation-key: henningSPECCPU2000Measuring2000
  container-title: Computer
  DOI: 10.1109/2.869367
  ISSN: 1558-0814
  issue: '7'
  issued:
    - year: 2000
      month: 7
  page: 28-35
  source: IEEE Xplore
  title: 'SPEC CPU2000: measuring CPU performance in the New Millennium'
  title-short: SPEC CPU2000
  type: article-journal
  volume: '33'

- id: henningSPECCPU2006Benchmark2006
  abstract: >-
    On August 24, 2006, the Standard Performance Evaluation Corporation (SPEC)
    announced CPU2006 [2], which replaces CPU2000. The SPEC CPU benchmarks are
    widely used in both industry and academia [3].
  accessed:
    - year: 2024
      month: 2
      day: 8
  author:
    - family: Henning
      given: John L.
  citation-key: henningSPECCPU2006Benchmark2006
  container-title: ACM SIGARCH Computer Architecture News
  container-title-short: SIGARCH Comput. Archit. News
  DOI: 10.1145/1186736.1186737
  ISSN: 0163-5964
  issue: '4'
  issued:
    - year: 2006
      month: 9
  language: en
  page: 1-17
  source: DOI.org (Crossref)
  title: SPEC CPU2006 benchmark descriptions
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1186736.1186737
  volume: '34'

- id: hermannCommunityExpectationsResearch2020
  abstract: >-
    Background. Artifact evaluation has been introduced into the software
    engineering and programming languages research community with a pilot at
    ESEC/FSE 2011 and has since then enjoyed a healthy adoption throughout the
    conference landscape. Objective. In this qualitative study, we examine the
    expectations of the community toward research artifacts and their evaluation
    processes. Method. We conducted a survey including all members of artifact
    evaluation committees of major conferences in the software engineering and
    programming language field since the first pilot and compared the answers to
    expectations set by calls for artifacts and reviewing guidelines. Results.
    While we find that some expectations exceed the ones expressed in calls and
    reviewing guidelines, there is no consensus on quality thresholds for
    artifacts in general. We observe very specific quality expectations for
    specific artifact types for review and later usage, but also a lack of their
    communication in calls. We also find problematic inconsistencies in the
    terminology used to express artifact evaluation’s most important purpose –
    replicability. Conclusion. We derive several actionable suggestions which
    can help to mature artifact evaluation in the inspected community and also
    to aid its introduction into other communities in computer science.
  accessed:
    - year: 2023
      month: 1
      day: 31
  author:
    - family: Hermann
      given: Ben
    - family: Winter
      given: Stefan
    - family: Siegmund
      given: Janet
  citation-key: hermannCommunityExpectationsResearch2020
  collection-title: ESEC/FSE 2020
  container-title: >-
    Proceedings of the 28th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3368089.3409767
  event-place: New York, NY, USA
  ISBN: 978-1-4503-7043-1
  issued:
    - year: 2020
      month: 11
      day: 8
  note: 'interest: 92'
  page: 469–480
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Community expectations for research artifacts and evaluation processes
  type: paper-conference
  URL: https://doi.org/10.1145/3368089.3409767

- id: herndonDoesHighPublic2014
  abstract: >-
    We replicate Reinhart and Rogoff (2010A and 2010B) and find that selective
    exclusion of available data, coding errors and inappropriate weighting of
    summary statistics lead to serious miscalculations that inaccurately
    represent the relationship between public debt and GDP growth among 20
    advanced economies. Over 1946–2009, countries with public debt/GDP ratios
    above 90% averaged 2.2% real annual GDP growth, not −0.1% as published. The
    published results for (i) median GDP growth rates for the 1946–2009 period
    and (ii) mean and median GDP growth figures over 1790–2009 are all distorted
    by similar methodological errors, although the magnitudes of the distortions
    are somewhat smaller than with the mean figures for 1946–2009. Contrary to
    Reinhart and Rogoff’s broader contentions, both mean and median GDP growth
    when public debt levels exceed 90% of GDP are not dramatically different
    from when the public debt/GDP ratios are lower. The relationship between
    public debt and GDP growth varies significantly by period and country. Our
    overall evidence refutes RR’s claim that public debt/GDP ratios above 90%
    consistently reduce a country’s GDP growth.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Herndon
      given: T.
    - family: Ash
      given: M.
    - family: Pollin
      given: R.
  citation-key: herndonDoesHighPublic2014
  container-title: Cambridge Journal of Economics
  container-title-short: Cambridge Journal of Economics
  DOI: 10.1093/cje/bet075
  ISSN: 0309-166X, 1464-3545
  issue: '2'
  issued:
    - year: 2014
      month: 3
      day: 1
  language: en
  page: 257-279
  source: DOI.org (Crossref)
  title: >-
    Does high public debt consistently stifle economic growth? A critique of
    Reinhart and Rogoff
  title-short: Does high public debt consistently stifle economic growth?
  type: article-journal
  URL: https://academic.oup.com/cje/article-lookup/doi/10.1093/cje/bet075
  volume: '38'

- id: herouxCompatibleReproducibilityTaxonomy2018
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Heroux
      given: Michael
    - family: Barba
      given: Lorena
    - family: Parashar
      given: Manish
    - family: Stodden
      given: Victoria
    - family: Taufer
      given: Michela
  citation-key: herouxCompatibleReproducibilityTaxonomy2018
  DOI: 10.2172/1481626
  issued:
    - year: 2018
      month: 10
      day: 1
  number: SAND--2018-11186, 1481626, 669580
  page: SAND--2018-11186, 1481626, 669580
  source: DOI.org (Crossref)
  title: >-
    Toward a Compatible Reproducibility Taxonomy for Computational and Computing
    Sciences
  type: report
  URL: https://www.osti.gov/servlets/purl/1481626/

- id: herouxEditorialACMTOMS2015
  abstract: >-
    The scientific community relies on the peer review process for assuring the
    quality of published material, the goal of which is to build a body of work
    we can trust. Computational journals such as the ACM Transactions on
    Mathematical Software (TOMS) use this process for rigorously promoting the
    clarity and completeness of content, and citation of prior work. At the same
    time, it is unusual to independently confirm computational results. ACM TOMS
    has established a Replicated Computational Results (RCR) review process as
    part of the manuscript peer review process. The purpose is to provide
    independent confirmation that results contained in a manuscript are
    replicable. Successful completion of the RCR process awards a manuscript
    with the Replicated Computational Results Designation. This issue of ACM
    TOMS contains the first [Van Zee and van de Geijn 2015] of what we
    anticipate to be a growing number of articles to receive the RCR
    designation, and the related RCR reviewer report [Willenbring 2015]. We hope
    that the TOMS RCR process will serve as a model for other publications and
    increase the confidence in and value of computational results in TOMS
    articles.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Heroux
      given: Michael A.
  citation-key: herouxEditorialACMTOMS2015
  container-title: ACM Transactions on Mathematical Software
  container-title-short: ACM Trans. Math. Softw.
  DOI: 10.1145/2743015
  ISSN: 0098-3500
  issue: '3'
  issued:
    - year: 2015
      month: 6
      day: 1
  page: 13:1–13:5
  source: June 2015
  title: 'Editorial: ACM TOMS Replicated Computational Results Initiative'
  title-short: Editorial
  type: article-journal
  URL: https://doi.org/10.1145/2743015
  volume: '41'

- id: herouxLightweightSoftwareProcess2019
  abstract: Abstract not provided.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Heroux
      given: Michael A.
    - family: Gonsiorowski
      given: Elsa
    - family: Gupta
      given: Rinku
    - family: Milewicz
      given: Reed
    - family: Moulton
      given: David
    - family: Watson
      given: Gregory
    - family: Willenbring
      given: James Michael
    - family: Zamora
      given: Richard
    - family: Raybourn
      given: Elaine M.
  citation-key: herouxLightweightSoftwareProcess2019
  DOI: 10.1007/978-3-030-44728-1_6
  ISSN: 1865--0929
  issued:
    - year: 2019
      month: 8
      day: 1
  language: English
  note: 'interest: 65'
  number: SAND2019-9693C
  publisher: >-
    Sandia National Lab. (SNL-NM), Albuquerque, NM (United States); Sandia
    National Laboratories, Unknown, Unknown
  source: www.osti.gov
  title: >-
    Lightweight Software Process Improvement using Productivity and
    Sustainability Improvement Planning (PSIP).
  type: report
  URL: https://www.osti.gov/biblio/1641678
  volume: '1190'

- id: hettneBestPracticesWorkflow2012
  abstract: >-
    In this position paper we present a set of best practices for workflow
    design to prevent workflow decay and increase reuse and re-purposing of
    scientific workflows. MyExperiment provides access to a large number of
    scientific workflows. However, scientists find it difficult to reuse or
    re-purpose these workflows for mainly two reasons: workflows suffer from
    decay over time and lack sufficient metadata to understand their purpose. We
    argue that good workflow design is a prerequisite for repairing a workflow,
    or redesigning an equivalent workflow pattern with new components. We
    present a set of best practices for workflow design and the semantic tooling
    that is being developed in the Workflow4Ever (Wf4Ever) project to support
    these best practices.
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Hettne
      given: Kristina
    - family: Wolstencroft
      given: Katy
    - family: Belhajjame
      given: Khalid
    - family: Goble
      given: Carole
    - family: Mina
      given: Eleni
    - family: Dharuri
      given: Harish
    - family: Verdes-Montenegro
      given: Lourdes
    - family: Garrido
      given: Julián
    - family: Roure
      given: David
      dropping-particle: de
    - family: Roos
      given: Marco
  citation-key: hettneBestPracticesWorkflow2012
  container-title: >-
    SWAT4LS: semantic web applications and tools for life sciences 2012:
    proceedings of the 5th international workshop on semantic web applications
    and tools for life sciences
  event-title: >-
    5th International Workshop on Semantic Web Applications and Tools for Life
    Sciences
  issued:
    - year: 2012
      month: 11
      day: 28
  language: English
  note: 'interest: 90'
  page: '23'
  publisher: RWTH Aachen University
  source: www.research.manchester.ac.uk
  title: 'Best practices for workflow design: how to prevent workflow decay'
  title-short: Best practices for workflow design
  type: paper-conference
  URL: >-
    https://www.research.manchester.ac.uk/portal/en/publications/best-practices-for-workflow-design(f7ded259-61de-46a6-86a0-611027afa683).html

- id: hettrickJourneyReproducibilityExcel2017
  abstract: >-
    This is a story about reproducibility. It’s about the first study I
    conducted at the Institute, the difficulties I’ve faced in reproducing
    analysis that was originally conducted in Excel, and it’s testament to the
    power of a tweet that’s haunted me for three years.
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Hettrick
      given: Simon
  citation-key: hettrickJourneyReproducibilityExcel2017
  container-title: 'Software and research: the Institute''s Blog'
  issued:
    - year: 2017
      month: 9
      day: 6
  language: en
  title: A journey of reproducibility from Excel to Pandas
  type: post-weblog
  URL: >-
    https://www.software.ac.uk/blog/2017-09-06-journey-reproducibility-excel-pandas

- id: hettrickSoftwaresavedSoftware_In_Research_Survey_2014Software2018
  abstract: >-
    This reproducible, Python-based re-analysis of the Software Sustainability
    Institute's 2014 research software survey. The original analysis was
    conducted in Excel, so this re-analysis was performed to improve the
    reproducibility of the results.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Hettrick
      given: Simon
  citation-key: hettrickSoftwaresavedSoftware_In_Research_Survey_2014Software2018
  DOI: 10.5281/ZENODO.1183562
  issued:
    - year: 2018
      month: 2
      day: 23
  license: Open Access
  publisher: Zenodo
  source: DOI.org (Datacite)
  title: 'Softwaresaved/Software_In_Research_Survey_2014: Software In Research Survey'
  title-short: Softwaresaved/Software_In_Research_Survey_2014
  type: software
  URL: https://zenodo.org/record/1183562

- id: hicksSCOREAgileResearch2010
  abstract: >-
    Adapting agile software development methodology toward more efficient
    management of academic research groups.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Hicks
      given: Michael
    - family: Foster
      given: Jeffrey S.
  citation-key: hicksSCOREAgileResearch2010
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/1831407.1831421
  ISSN: 0001-0782, 1557-7317
  issue: '10'
  issued:
    - year: 2010
      month: 10
  language: en
  page: 30-31
  source: DOI.org (Crossref)
  title: 'SCORE: agile research group management'
  title-short: SCORE
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1831407.1831421
  volume: '53'

- id: hiltonUsageCostsBenefits2016
  abstract: >-
    Continuous integration (CI) systems automate the compilation, building, and
    testing of software. Despite CI rising as a big success story in automated
    software engineering, it has received almost no attention from the research
    community. For example, how widely is CI used in practice, and what are some
    costs and benefits associated with CI? Without answering such questions,
    developers, tool builders, and researchers make decisions based on folklore
    instead of data. In this paper, we use three complementary methods to study
    the usage of CI in open-source projects. To understand which CI systems
    developers use, we analyzed 34,544 open-source projects from GitHub. To
    understand how developers use CI, we analyzed 1,529,291 builds from the most
    commonly used CI system. To understand why projects use or do not use CI, we
    surveyed 442 developers. With this data, we answered several key questions
    related to the usage, costs, and benefits of CI. Among our results, we show
    evidence that supports the claim that CI helps projects release more often,
    that CI is widely adopted by the most popular projects, as well as finding
    that the overall percentage of projects using CI continues to grow, making
    it important and timely to focus more research on CI.
  accessed:
    - year: 2023
      month: 2
      day: 20
  author:
    - family: Hilton
      given: Michael
    - family: Tunnell
      given: Timothy
    - family: Huang
      given: Kai
    - family: Marinov
      given: Darko
    - family: Dig
      given: Danny
  citation-key: hiltonUsageCostsBenefits2016
  collection-title: ASE '16
  container-title: >-
    Proceedings of the 31st IEEE/ACM International Conference on Automated
    Software Engineering
  DOI: 10.1145/2970276.2970358
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3845-5
  issued:
    - year: 2016
      month: 8
      day: 25
  page: 426–437
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Usage, costs, and benefits of continuous integration in open-source projects
  type: paper-conference
  URL: https://doi.org/10.1145/2970276.2970358

- id: hinesPerformanceComparisonFilesystem2022
  author:
    - family: Hines
      given: Jesse
    - family: Cunningham
      given: Nicholas
  citation-key: hinesPerformanceComparisonFilesystem2022
  container-title: Campus Research Day
  issued:
    - year: 2022
      month: 4
      day: 21
  note: 'interest: 60'
  title: Performance Comparison of the Filesystem and Embedded Key-Value Databases
  type: article-journal
  URL: https://knowledge.e.southern.edu/crd/2022/noncompetingsessionpm/3

- id: hinsenApproximationTowerComputational2015
  abstract: >-
    Numerical solutions of mathematical equations in scientific models are the
    result of several approximation steps. Konrad Hinsen uses a simulation of
    the solar system as an example for illustrating these approximations and
    explaining their role in the difficult problem of testing scientific
    software.
  author:
    - family: Hinsen
      given: Konrad
  citation-key: hinsenApproximationTowerComputational2015
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2015.75
  ISSN: 1558-366X
  issue: '4'
  issued:
    - year: 2015
      month: 7
  note: 'interest: 75'
  page: 72-77
  source: IEEE Xplore
  title: >-
    The Approximation Tower in Computational Science: Why Testing Scientific
    Software Is Difficult
  title-short: The Approximation Tower in Computational Science
  type: article-journal
  volume: '17'

- id: hinsenDealingSoftwareCollapse2019
  abstract: >-
    Discusses the concept of software collapse. There is A good chance that you
    have never heard of software collapse before, for the simple reason that it
    is a term I have made up myself two years ago in a blog post. However, if
    you have been doing computational science for a few years, there is a good
    chance that you have experienced software collapse, and probably it was not
    a pleasant experience. In this paper, I will explain what software collapse
    is, what causes it, and how you can manage the risk of it happening to you.
    What I call software collapse is more commonly referred to as software rot:
    the fact that software stops working eventually if is not actively
    maintained. The rot metaphor has a long history, the first documented
    reference being the 1983 edition of the Hacker’s Dictionary. Back then, it
    was used jokingly by a small community of computer experts who understood
    the phenomenon perfectly well, and therefore a funny but technically
    inaccurate metaphor was not a problem. Today, it is being discussed in much
    wider circles, for example, in the context of reproducible research. In my
    opinion, it is appropriate to introduce a useful metaphor in place of the
    traditional humorous one, because good metaphors contribute to a better
    understanding of what is actually going on. The main issue with the rot
    metaphor is that it puts the blame on the wrong piece of the puzzle. If
    software becomes unusable over time, it is not because of any alteration to
    that software that needs to be reversed. Rather, it is the foundation on
    which the software has been built, ranging from the actual hardware via the
    operating system to programming languages and libraries, that has changed so
    much that the software is no longer compatible with it. Since unstable
    foundations resemble how a house is destroyed by an earthquake rather than
    how spoiling food is transformed by fungi, I consider collapse an
    appropriate metaphor.
  author:
    - family: Hinsen
      given: Konrad
  citation-key: hinsenDealingSoftwareCollapse2019
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2019.2900945
  ISSN: 1558-366X
  issue: '3'
  issued:
    - year: 2019
      month: 5
  page: 104-108
  source: IEEE Xplore
  title: Dealing With Software Collapse
  type: article-journal
  volume: '21'

- id: hinsenEssentialToolsVersion2009
  abstract: >-
    Did you ever wish you'd made a backup copy of a file before changing it? Or
    before applying a collaborator's modifications? Version control systems make
    this easier, and do a lot more.
  author:
    - family: Hinsen
      given: Konrad
    - family: Läufer
      given: Konstantin
    - family: Thiruvathukal
      given: George K.
  citation-key: hinsenEssentialToolsVersion2009
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2009.194
  ISSN: 1558-366X
  issue: '6'
  issued:
    - year: 2009
      month: 11
  note: 'interest: 72'
  page: 84-91
  source: IEEE Xplore
  title: 'Essential Tools: Version Control Systems'
  title-short: Essential Tools
  type: article-journal
  volume: '11'

- id: hinsenMagicContentAddressableStorage2020
  abstract: >-
    The term “content-addressable storage” does not sound exciting, but
    superficial explanations can make it look like magic, to the point of
    raising suspicion. In this article, I will show that content-addressable
    storage is a technology that works, is already in widespread use, and holds
    many promises for the future of both scientific programming and the
    management of scientific data. I will start by outlining the theory, and
    then illustrate how it works in practice, using IPFS, the Inter-Planetary
    FileSystem.
  author:
    - family: Hinsen
      given: Konrad
  citation-key: hinsenMagicContentAddressableStorage2020
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2019.2949441
  ISSN: 1558-366X
  issue: '3'
  issued:
    - year: 2020
      month: 5
  note: 'interest: 83'
  page: 113-119
  source: IEEE Xplore
  title: The Magic of Content-Addressable Storage
  type: article-journal
  volume: '22'

- id: hinsenVerifiabilityComputeraidedResearch2018
  abstract: >-
    Most of today’s scientific research relies on computers and software for
    processing scientific information. Examples of such computer-aided research
    are the analysis of experimental data or the simulation of phenomena based
    on theoretical models. With the rapid increase of computational power,
    scientific software has integrated more and more complex scientific
    knowledge in a black-box fashion. As a consequence, its users do not know,
    and do not even have a chance of finding out, which assumptions and
    approximations their computations are based on. This black-box nature of
    scientific software has made the verification of much computer-aided
    research close to impossible. The present work starts with an analysis of
    this situation from the point of view of human-computer interaction in
    scientific research. It identifies the key role of digital scientific
    notations at the human-computer interface, reviews the most popular ones in
    use today, and describes a proof-of-concept implementation of Leibniz, a
    language designed as a verifiable digital scientific notation for models
    formulated as mathematical equations.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Hinsen
      given: Konrad
  citation-key: hinsenVerifiabilityComputeraidedResearch2018
  container-title: PeerJ Computer Science
  container-title-short: PeerJ Comput. Sci.
  DOI: 10.7717/peerj-cs.158
  ISSN: 2376-5992
  issued:
    - year: 2018
      month: 7
      day: 23
  language: en
  page: e158
  publisher: PeerJ Inc.
  source: peerj.com
  title: >-
    Verifiability in computer-aided research: the role of digital scientific
    notations at the human-computer interface
  title-short: Verifiability in computer-aided research
  type: article-journal
  URL: https://peerj.com/articles/cs-158
  volume: '4'

- id: hinshawNineyearWilkinsonMicrowave2013
  abstract: >-
    We present cosmological parameter constraints based on the final nine-year
    Wilkinson Microwave Anisotropy Probe (WMAP) data, in conjunction with a
    number of additional cosmological data sets. The WMAP data alone, and in
    combination, continue to be remarkably well fit by a six-parameter ΛCDM
    model. When WMAP data are combined with measurements of the high-l cosmic
    microwave background anisotropy, the baryon acoustic oscillation scale, and
    the Hubble constant, the matter and energy densities, Ω b h 2, Ω c h 2, and
    ΩΛ, are each determined to a precision of \~1.5%. The amplitude of the
    primordial spectrum is measured to within 3%, and there is now evidence for
    a tilt in the primordial spectrum at the 5σ level, confirming the first
    detection of tilt based on the five-year WMAP data. At the end of the WMAP
    mission, the nine-year data decrease the allowable volume of the
    six-dimensional ΛCDM parameter space by a factor of 68,000 relative to
    pre-WMAP measurements. We investigate a number of data combinations and show
    that their ΛCDM parameter fits are consistent. New limits on deviations from
    the six-parameter model are presented, for example: the fractional
    contribution of tensor modes is limited to r < 0.13 (95% CL); the spatial
    curvature parameter is limited to \Omega _k = -0.0027\^{+ 0.0039}_{-
    0.0038}; the summed mass of neutrinos is limited to ∑m ν < 0.44 eV (95% CL);
    and the number of relativistic species is found to lie within N eff = 3.84 ±
    0.40, when the full data are analyzed. The joint constraint on N eff and the
    primordial helium abundance, Y He, agrees with the prediction of standard
    big bang nucleosynthesis. We compare recent Planck measurements of the
    Sunyaev-Zel'dovich effect with our seven-year measurements, and show their
    mutual agreement. Our analysis of the polarization pattern around
    temperature extrema is updated. This confirms a fundamental prediction of
    the standard cosmological model and provides a striking illustration of
    acoustic oscillations and adiabatic initial conditions in the early
    universe.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Hinshaw
      given: G.
    - family: Larson
      given: D.
    - family: Komatsu
      given: E.
    - family: Spergel
      given: D. N.
    - family: Bennett
      given: C. L.
    - family: Dunkley
      given: J.
    - family: Nolta
      given: M. R.
    - family: Halpern
      given: M.
    - family: Hill
      given: R. S.
    - family: Odegard
      given: N.
    - family: Page
      given: L.
    - family: Smith
      given: K. M.
    - family: Weiland
      given: J. L.
    - family: Gold
      given: B.
    - family: Jarosik
      given: N.
    - family: Kogut
      given: A.
    - family: Limon
      given: M.
    - family: Meyer
      given: S. S.
    - family: Tucker
      given: G. S.
    - family: Wollack
      given: E.
    - family: Wright
      given: E. L.
  citation-key: hinshawNineyearWilkinsonMicrowave2013
  container-title: The Astrophysical Journal Supplement Series
  DOI: 10.1088/0067-0049/208/2/19
  ISSN: 0067-0049
  issued:
    - year: 2013
      month: 10
      day: 1
  note: 'ADS Bibcode: 2013ApJS..208...19H'
  page: '19'
  source: NASA ADS
  title: >-
    Nine-year Wilkinson Microwave Anisotropy Probe (WMAP) Observations:
    Cosmological Parameter Results
  title-short: Nine-year Wilkinson Microwave Anisotropy Probe (WMAP) Observations
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/2013ApJS..208...19H
  volume: '208'

- id: hoeflerBenchmarkingDataScience2022
  abstract: >-
    We humorously discuss 12 fallacies when focusing on compute performance that
    we have frequently observed in practice. We follow each with a
    recommendation to mitigate the danger and hope to contribute to good
    benchmarking etiquette for data science.
  author:
    - family: Hoefler
      given: Torsten
  citation-key: hoeflerBenchmarkingDataScience2022
  container-title: Computer
  DOI: 10.1109/MC.2022.3152681
  ISSN: 1558-0814
  issue: '8'
  issued:
    - year: 2022
      month: 8
  note: 'interest: 86'
  page: 49-56
  source: IEEE Xplore
  title: >-
    Benchmarking Data Science: 12 Ways to Lie With Statistics and Performance on
    Parallel Computers
  title-short: Benchmarking Data Science
  type: article-journal
  volume: '55'

- id: hoekstraPROVOVizUnderstandingRole2015
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Hoekstra
      given: Rinke
    - family: Groth
      given: Paul
  citation-key: hoekstraPROVOVizUnderstandingRole2015
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-319-16462-5_18
  editor:
    - family: Ludäscher
      given: Bertram
    - family: Plale
      given: Beth
  event-place: Cham
  ISBN: 978-3-319-16461-8 978-3-319-16462-5
  issued:
    - year: 2015
  page: 215-220
  publisher: Springer International Publishing
  publisher-place: Cham
  source: DOI.org (Crossref)
  title: PROV-O-Viz - Understanding the Role of Activities in Provenance
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-319-16462-5_18
  volume: '8628'

- id: hoffmanSandiaAnalysisWorkbench2015
  abstract: Abstract not provided.
  accessed:
    - year: 2022
      month: 6
      day: 14
  author:
    - family: Hoffman
      given: Edward L.
    - family: Friedman-Hill
      given: Ernest J.
    - family: Gibson
      given: Marcus J.
    - family: Clay
      given: Robert L.
  citation-key: hoffmanSandiaAnalysisWorkbench2015
  issued:
    - year: 2015
      month: 4
      day: 1
  language: English
  number: SAND2015-3400C
  publisher: >-
    Sandia National Lab. (SNL-CA), Livermore, CA (United States); Sandia
    National Laboratories.,
  source: www.osti.gov
  title: >-
    Sandia Analysis Workbench Enabling Advanced Modeling & Simulation
    Technologies.
  type: report
  URL: https://www.osti.gov/biblio/1251360

- id: hoggDataAnalysisRecipes2010
  abstract: >-
    We go through the many considerations involved in fitting a model to data,
    using as an example the fit of a straight line to a set of points in a
    two-dimensional plane. Standard weighted least-squares fitting is only
    appropriate when there is a dimension along which the data points have
    negligible uncertainties, and another along which all the uncertainties can
    be described by Gaussians of known variance; these conditions are rarely met
    in practice. We consider cases of general, heterogeneous, and arbitrarily
    covariant two-dimensional uncertainties, and situations in which there are
    bad data (large outliers), unknown uncertainties, and unknown but expected
    intrinsic scatter in the linear relationship being fit. Above all we
    emphasize the importance of having a "generative model" for the data, even
    an approximate one. Once there is a generative model, the subsequent fitting
    is non-arbitrary because the model permits direct computation of the
    likelihood of the parameters or the posterior probability distribution.
    Construction of a posterior probability distribution is indispensible if
    there are "nuisance parameters" to marginalize away.
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Hogg
      given: David W.
    - family: Bovy
      given: Jo
    - family: Lang
      given: Dustin
  citation-key: hoggDataAnalysisRecipes2010
  container-title: arXiv:1008.4686 [astro-ph, physics:physics]
  issued:
    - year: 2010
      month: 8
      day: 27
  note: 'interest: 80'
  source: arXiv.org
  title: 'Data analysis recipes: Fitting a model to data'
  title-short: Data analysis recipes
  type: article-journal
  URL: http://arxiv.org/abs/1008.4686

- id: holdenIncreasingAccessResults2013
  author:
    - family: Holden
      given: John P.
  citation-key: holdenIncreasingAccessResults2013
  issued:
    - year: 2013
      month: 2
      day: 22
  publisher: Executive Office of the President, Office of Science and Technology Policy
  title: Increasing Access to the Results of Federally Funded Scientific Research
  type: document

- id: hollandPASSingProvenanceChallenge2008
  abstract: >-
    Provenance-aware storage systems (PASS) are a new class of storage system
    treating provenance as a first-class object, providing automatic collection,
    storage, and management of provenance as well as query capabilities. We
    developed the first PASS prototype between 2005 and 2006, targeting
    scientific end users. Prior to undertaking the provenance challenge, we had
    focused on provenance collection and storage, without much emphasis on a
    query model or language. The challenge forced us to (quickly) develop a
    query model and infrastructure implementing this model. We present a brief
    overview of the PASS prototype and a discussion of the evolution of the
    query model that we developed for the challenge. Copyright © 2007 John Wiley
    & Sons, Ltd.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Holland
      given: David A.
    - family: Seltzer
      given: Margo I.
    - family: Braun
      given: Uri
    - family: Muniswamy-Reddy
      given: Kiran-Kumar
  citation-key: hollandPASSingProvenanceChallenge2008
  container-title: 'Concurrency and Computation: Practice and Experience'
  DOI: 10.1002/cpe.1227
  ISSN: 1532-0634
  issue: '5'
  issued:
    - year: 2008
  language: en
  license: Copyright © 2007 John Wiley & Sons, Ltd.
  page: 531-540
  source: Wiley Online Library
  title: PASSing the provenance challenge
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1227
  volume: '20'

- id: hongFAIRPrinciplesResearch2022
  accessed:
    - year: 2022
      month: 9
      day: 14
  author:
    - family: Hong
      given: Neil P. Chue
    - family: Katz
      given: Daniel S.
    - family: Barker
      given: Michelle
    - family: Lamprecht
      given: Anna-Lena
    - family: Martinez
      given: Carlos
    - family: Psomopoulos
      given: Fotis E.
    - family: Harrow
      given: Jen
    - family: Castro
      given: Leyla Jael
    - family: Gruenpeter
      given: Morane
    - family: Martinez
      given: Paula Andrea
    - family: Honeyman
      given: Tom
    - family: Struck
      given: Alexander
    - family: Lee
      given: Allen
    - family: Loewe
      given: Axel
    - family: Werkhoven
      given: Ben
      dropping-particle: van
    - family: Garijo
      given: Daniel
    - family: Plomp
      given: Esther
    - family: Genova
      given: Francoise
    - family: Shanahan
      given: Hugh
    - family: Hellström
      given: Maggie
    - family: Sandström
      given: Malin
    - family: Sinha
      given: Manodeep
    - family: Kuzak
      given: Mateusz
    - family: Herterich
      given: Patricia
    - family: Islam
      given: Sharif
    - family: Sansone
      given: Susanna-Assunta
    - family: Pollard
      given: Tom
    - family: Atmojo
      given: Udayanto Dwi
    - family: Williams
      given: Alan
    - family: Czerniak
      given: Andreas
    - family: Niehues
      given: Anna
    - family: Fouilloux
      given: Anne Claire
    - family: Desinghu
      given: Bala
    - family: Goble
      given: Carole
    - family: Richard
      given: Céline
    - family: Gray
      given: Charles
    - family: Erdmann
      given: Chris
    - family: Nüst
      given: Daniel
    - family: Tartarini
      given: Daniele
    - family: Ranguelova
      given: Elena
    - family: Anzt
      given: Hartwig
    - family: Todorov
      given: Ilian
    - family: McNally
      given: James
    - family: Burnett
      given: Jessica
    - family: Garrido-Sánchez
      given: Julián
    - family: Belhajjame
      given: Khalid
    - family: Sesink
      given: Laurents
    - family: Hwang
      given: Lorraine
    - family: Tovani-Palone
      given: Marcos Roberto
    - family: Wilkinson
      given: Mark D.
    - family: Servillat
      given: Mathieu
    - family: Liffers
      given: Matthias
    - family: Fox
      given: Merc
    - family: Miljković
      given: Nadica
    - family: Lynch
      given: Nick
    - family: Lavanchy
      given: Paula Martinez
    - family: Gesing
      given: Sandra
    - family: Stevens
      given: Sarah
    - family: Cuesta
      given: Sergio Martinez
    - family: Peroni
      given: Silvio
    - family: Soiland-Reyes
      given: Stian
    - family: Bakker
      given: Tom
    - family: Rabemanantsoa
      given: Tovo
    - family: Sochat
      given: Vanessa
    - family: Yehudi
      given: Yo
    - family: Wg
      given: Fair4rs
  citation-key: hongFAIRPrinciplesResearch2022
  DOI: 10.15497/RDA00065
  issued:
    - year: 2022
      month: 3
      day: 16
  language: English
  source: www.research.manchester.ac.uk
  title: FAIR Principles for Research Software (FAIR4RS Principles)
  type: article-journal
  URL: >-
    https://www.research.manchester.ac.uk/portal/en/publications/fair-principles-for-research-software-fair4rs-principles(751dfce3-56e5-441f-8e3f-8c1401e0a1e0).html

- id: hongWhyWeNeed2016
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Hong
      given: Neil Chue
  citation-key: hongWhyWeNeed2016
  container-title: >-
    Proceedings of the Fourth Workshop on Sustainable Software for Science:
    Practice and Experiences (WSSSPE4): University of Manchester, Manchester,
    UK, September 12-­14, 2016
  issued:
    - year: 2016
      month: 9
      day: 14
  language: English
  note: 'interest: 85'
  publisher: CEUR Workshop Proceedings (CEUR-WS.org)
  source: www.research.ed.ac.uk
  title: Why do we need to compare research software, and how should we do it?
  type: article-journal
  URL: >-
    https://www.research.ed.ac.uk/en/publications/why-do-we-need-to-compare-research-software-and-how-should-we-do-

- id: horaAtomPublishingProtocol2007
  abstract: >-
    The Atom Publishing Protocol (AtomPub) is an application-level protocol for
    publishing and editing Web resources. The protocol is based on HTTP transfer
    of Atom-formatted representations. The Atom format is documented in the Atom
    Syndication Format.
  accessed:
    - year: 2022
      month: 8
      day: 5
  author:
    - family: hÓra
      given: Bill
      dropping-particle: de
    - family: Gregorio
      given: Joe
  citation-key: horaAtomPublishingProtocol2007
  genre: Request for Comments
  issued:
    - year: 2007
      month: 10
  number: RFC 5023
  publisher: Internet Engineering Task Force
  source: IETF
  title: The Atom Publishing Protocol
  type: report
  URL: https://datatracker.ietf.org/doc/rfc5023/

- id: hortonDockerizeMeAutomaticInference2019
  abstract: >-
    Platforms like Stack Overflow and GitHub's gist system promote the sharing
    of ideas and programming techniques via the distribution of code snippets
    designed to illustrate particular tasks. Python, a popular and fast-growing
    programming language, sees heavy use on both sites, with nearly one million
    questions asked on Stack Overflow and 400 thousand public gists on GitHub.
    Unfortunately, around 75% of the Python example code shared through these
    sites cannot be directly executed. When run in a clean environment, over 50%
    of public Python gists fail due to an import error for a missing library. We
    present DockerizeMe, a technique for inferring the dependencies needed to
    execute a Python code snippet without import error. DockerizeMe starts with
    offline knowledge acquisition of the resources and dependencies for popular
    Python packages from the Python Package Index (PyPI). It then builds Docker
    specifications using a graph-based inference procedure. Our inference
    procedure resolves import errors in 892 out of nearly 3,000 gists from the
    Gistable dataset for which Gistable's baseline approach could not find and
    install all dependencies.
  author:
    - family: Horton
      given: Eric
    - family: Parnin
      given: Chris
  citation-key: hortonDockerizeMeAutomaticInference2019
  container-title: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
  DOI: 10.1109/ICSE.2019.00047
  event-title: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
  ISSN: 1558-1225
  issued:
    - year: 2019
      month: 5
  note: 'interest: 97'
  page: 328-338
  source: IEEE Xplore
  title: >-
    DockerizeMe: Automatic Inference of Environment Dependencies for Python Code
    Snippets
  title-short: DockerizeMe
  type: paper-conference

- id: hortonGistableEvaluatingExecutability2018
  abstract: >-
    Software developers create and share code online to demonstrate programming
    language concepts and programming tasks. Code snippets can be a useful way
    to explain and demonstrate a programming concept, but may not always be
    directly executable. A code snippet can contain parse errors, or fail to
    execute if the environment contains unmet dependencies. This paper presents
    an empirical analysis of the executable status of Python code snippets
    shared through the GitHub gist system, and the ability of developers
    familiar with software configuration to correctly configure and run them. We
    find that 75.6% of gists require non-trivial configuration to overcome
    missing dependencies, configuration files, reliance on a specific operating
    system, or some other environment configuration. Our study also suggests the
    natural assumption developers make about resource names when resolving
    configuration errors is correct less than half the time. We also present
    Gistable, a database and extensible framework built on GitHub's gist system,
    which provides executable code snippets to enable reproducible studies in
    software engineering. Gistable contains 10,259 code snippets, approximately
    5,000 with a Dockerfile to configure and execute them without import error.
    Gistable is publicly available at https://github.com/gistable/gistable.
  author:
    - family: Horton
      given: Eric
    - family: Parnin
      given: Chris
  citation-key: hortonGistableEvaluatingExecutability2018
  container-title: >-
    2018 IEEE International Conference on Software Maintenance and Evolution
    (ICSME)
  DOI: 10.1109/ICSME.2018.00031
  event-title: >-
    2018 IEEE International Conference on Software Maintenance and Evolution
    (ICSME)
  ISSN: 2576-3148
  issued:
    - year: 2018
      month: 9
  note: 'interest: 97'
  page: 217-227
  source: IEEE Xplore
  title: 'Gistable: Evaluating the Executability of Python Code Snippets on GitHub'
  title-short: Gistable
  type: paper-conference

- id: hortonV2FastDetection2019
  abstract: >-
    Code snippets are prevalent, but are hard to reuse because they often lack
    an accompanying environment configuration. Most are not actively maintained,
    allowing for drift between the most recent possible configuration and the
    code snippet as the snippet becomes out-of-date over time. Recent work has
    identified the problem of validating and detecting out-of-date code snippets
    as the most important consideration for code reuse. However, determining if
    a snippet is correct, but simply out-of-date, is a non-trivial task. In the
    best case, breaking changes are well documented, allowing developers to
    manually determine when a code snippet contains an out-of-date API usage. In
    the worst case, determining if and when a breaking change was made requires
    an exhaustive search through previous dependency versions. We present V2, a
    strategy for determining if a code snippet is out-of-date by detecting
    discrete instances of configuration drift, where the snippet uses an API
    which has since undergone a breaking change. Each instance of configuration
    drift is classified by a failure encountered during validation and a
    configuration patch, consisting of dependency version changes, which fixes
    the underlying fault. V2 uses feedback-directed search to explore the
    possible configuration space for a code snippet, reducing the number of
    potential environment configurations that need to be validated. When run on
    a corpus of public Python snippets from prior research, V2 identifies 248
    instances of configuration drift.
  author:
    - family: Horton
      given: Eric
    - family: Parnin
      given: Chris
  citation-key: hortonV2FastDetection2019
  container-title: >-
    2019 34th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)
  DOI: 10.1109/ASE.2019.00052
  event-title: >-
    2019 34th IEEE/ACM International Conference on Automated Software
    Engineering (ASE)
  ISSN: 2643-1572
  issued:
    - year: 2019
      month: 11
  note: 'interest: 97'
  page: 477-488
  source: IEEE Xplore
  title: 'V2: Fast Detection of Configuration Drift in Python'
  title-short: V2
  type: paper-conference

- id: hosnyYourResearchReproducible2016
  abstract: >-
    The XRDS blog highlights a range of topics from conference coverage, to
    security and privacy, to CS theory. Selected blog posts, edited for print,
    are featured in every issue. Please visit xrds.acm.org/blog to read each
    post in its entirety. If you are interested in joining as a student blogger,
    please contact us.
  accessed:
    - year: 2022
      month: 9
      day: 14
  author:
    - family: Hosny
      given: Abdelrahman
  citation-key: hosnyYourResearchReproducible2016
  container-title: 'XRDS: Crossroads, The ACM Magazine for Students'
  container-title-short: XRDS
  DOI: 10.1145/2951008
  ISSN: 1528-4972
  issue: '4'
  issued:
    - year: 2016
      month: 6
      day: 13
  note: 'interest: 92'
  page: 14–15
  source: Summer 2016
  title: Is your research reproducible?
  type: article-journal
  URL: https://doi.org/10.1145/2951008
  volume: '22'

- id: hosteHowMakePackage2018
  abstract: >-
    In this talk, I will outline how (primarily scientific) software developers
    have found ways to complicate the job of the people who are responsible for
    compiling, installing and/or packaging their software, mainly in the context
    of multi-user high-performance computing environments.


    Next to an overview of the commonly used techniques, the motivations behind
    them, and the excuses that software developers can use to get away with it,
    I will showcase a couple of examples of software applications that have done
    a great job to make the life of package managers (in the broad sense) as
    miserable as possible.
  accessed:
    - year: 2023
      month: 1
      day: 31
  author:
    - family: Hoste
      given: Kenneth
  citation-key: hosteHowMakePackage2018
  event-place: Brussels, Belgium
  event-title: FOSDEM '18
  issued:
    - year: 2018
      month: 2
      day: 3
  language: en
  license: http://creativecommons.org/licenses/by-sa/2.0/be/
  publisher-place: Brussels, Belgium
  title: How To Make Package Managers Cry
  type: speech
  URL: >-
    https://archive.fosdem.org/2018/schedule/event/how_to_make_package_managers_cry/

- id: HowEditorsEdit2019
  abstract: >-
    We shed some light on how the Nature Methods editorial team evaluates papers
    submitted to the journal.
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: HowEditorsEdit2019
  container-title: Nature Methods
  container-title-short: Nat Methods
  DOI: 10.1038/s41592-019-0324-z
  ISSN: 1548-7105
  issue: '2'
  issued:
    - year: 2019
      month: 2
  language: en
  license: 2019 Springer Nature America, Inc.
  note: 'interest: 89'
  number: '2'
  page: 135-135
  publisher: Nature Publishing Group
  source: www.nature.com
  title: How editors edit
  type: article-journal
  URL: https://www.nature.com/articles/s41592-019-0324-z
  volume: '16'

- id: HowEffectiveASLR2021
  accessed:
    - year: 2022
      month: 11
      day: 15
  citation-key: HowEffectiveASLR2021
  issued:
    - year: 2021
      month: 1
      day: 18
  title: How Effective is ASLR on Linux Systems?
  title-short: How Effective is ASLR on Linux Systems?
  type: post-weblog
  URL: >-
    https://web.archive.org/web/20210118035736/http://securityetalii.es/2013/02/03/how-effective-is-aslr-on-linux-systems/

- id: howisonIncentivesIntegrationScientific2013
  abstract: >-
    Science policy makers are looking for approaches to increase the extent of
    collaboration in the production of scientific software, looking to open
    collaborations in open source software for inspiration. We examine the
    software ecosystem surrounding BLAST, a key bioinformatics tool, identifying
    outside improvements and interviewing their authors. We find that academic
    credit is a powerful motivator for the production and revealing of
    improvements. Yet surprisingly, we also find that improvements motivated by
    academic credit are less likely to be integrated than those with other
    motivations, including financial gain. We argue that this is because
    integration makes it harder to see who has contributed what and thereby
    undermines the ability of reputation to function as a reward for
    collaboration. We consider how open source avoids these issues and conclude
    with policy approaches to promoting wider collaboration by addressing
    incentives for integration.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Howison
      given: James
    - family: Herbsleb
      given: James D.
  citation-key: howisonIncentivesIntegrationScientific2013
  collection-title: CSCW '13
  container-title: Proceedings of the 2013 conference on Computer supported cooperative work
  DOI: 10.1145/2441776.2441828
  event-place: New York, NY, USA
  ISBN: 978-1-4503-1331-5
  issued:
    - year: 2013
      month: 2
      day: 23
  note: 'interest: 87'
  page: 459–470
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Incentives and integration in scientific software production
  type: paper-conference
  URL: https://doi.org/10.1145/2441776.2441828

- id: howisonRetractBitrottenPublications2014
  abstract: A provocation for the WSSSPE2 workshop
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Howison
      given: James
  citation-key: howisonRetractBitrottenPublications2014
  DOI: 10.6084/m9.figshare.1111632.v1
  issued:
    - year: 2014
      month: 7
      day: 20
  language: en
  publisher: figshare
  source: figshare.com
  title: >-
    Retract bit-rotten publications: Aligning incentives for sustaining
    scientific software
  title-short: Retract bit-rotten publications
  type: article-journal
  URL: >-
    https://figshare.com/articles/journal_contribution/Retract_bit_rotten_publications_Aligning_incentives_for_sustaining_scientific_software/1111632/1

- id: howisonScientificSoftwareProduction2011
  abstract: >-
    Software plays an increasingly critical role in science, including data
    analysis, simulations, and managing workflows. Unlike other technologies
    supporting science, software can be copied and distributed at essentially no
    cost, potentially opening the door to unprecedented levels of sharing and
    collaborative innovation. Yet we do not have a clear picture of how software
    development for science fits into the day-to-day practice of science, or how
    well the methods and incentives of its production facilitate realization of
    this potential. We report the results of a multiple-case study of software
    development in three fields: high energy physics, structural biology, and
    microbiology. In each case, we identify a typical publication, and use
    qualitative methods to explore the production of the software used in the
    science represented by the publication. We identify several different
    production systems, characterized primarily by differences in incentive
    structures. We identify ways in which incentives are matched and mismatched
    with the needs of the science fields, especially with respect to
    collaboration.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Howison
      given: James
    - family: Herbsleb
      given: James D.
  citation-key: howisonScientificSoftwareProduction2011
  collection-title: CSCW '11
  container-title: >-
    Proceedings of the ACM 2011 conference on Computer supported cooperative
    work
  DOI: 10.1145/1958824.1958904
  event-place: New York, NY, USA
  ISBN: 978-1-4503-0556-3
  issued:
    - year: 2011
      month: 3
      day: 19
  note: 'interest: 87'
  page: 513–522
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Scientific software production: incentives and collaboration'
  title-short: Scientific software production
  type: paper-conference
  URL: https://doi.org/10.1145/1958824.1958904

- id: howisonUnderstandingScientificSoftware2015
  abstract: >-
    Software is increasingly important to the scientific enterprise, and
    science-funding agencies are increasingly funding software work.
    Accordingly, many different participants need insight into how to understand
    the relationship between software, its development, its use, and its
    scientific impact. In this article, we draw on interviews and participant
    observation to describe the information needs of domain scientists, software
    component producers, infrastructure providers, and ecosystem stewards,
    including science funders. We provide a framework by which to categorize
    different types of measures and their relationships as they reach around
    from funding, development, scientific use, and through to scientific impact.
    We use this framework to organize a presentation of existing measures and
    techniques, and to identify areas in which techniques are either not
    widespread, or are entirely missing. We conclude with policy recommendations
    designed to improve insight into the scientific software ecosystem, make it
    more understandable, and thereby contribute to the progress of science.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Howison
      given: James
    - family: Deelman
      given: Ewa
    - family: McLennan
      given: Michael J.
    - family: Ferreira da Silva
      given: Rafael
    - family: Herbsleb
      given: James D.
  citation-key: howisonUnderstandingScientificSoftware2015
  container-title: Research Evaluation
  container-title-short: Research Evaluation
  DOI: 10.1093/reseval/rvv014
  ISSN: 0958-2029
  issue: '4'
  issued:
    - year: 2015
      month: 10
      day: 1
  note: 'interest: 85'
  page: 454-470
  source: Silverchair
  title: >-
    Understanding the scientific software ecosystem and its impact: Current and
    future measures
  title-short: Understanding the scientific software ecosystem and its impact
  type: article-journal
  URL: https://doi.org/10.1093/reseval/rvv014
  volume: '24'

- id: hullTavernaToolBuilding2006
  abstract: >-
    Taverna is an application that eases the use and integration of the growing
    number of molecular biology tools and databases available on the web,
    especially web services. It allows bioinformaticians to construct workflows
    or pipelines of services to perform a range of different analyses, such as
    sequence analysis and genome annotation. These high-level workflows can
    integrate many different resources into a single analysis. Taverna is
    available freely under the terms of the GNU Lesser General Public License
    (LGPL) from http://taverna.sourceforge.net/ .
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Hull
      given: Duncan
    - family: Wolstencroft
      given: Katy
    - family: Stevens
      given: Robert
    - family: Goble
      given: Carole
    - family: Pocock
      given: Mathew R.
    - family: Li
      given: Peter
    - family: Oinn
      given: Tom
  citation-key: hullTavernaToolBuilding2006
  container-title: Nucleic Acids Research
  container-title-short: Nucleic Acids Research
  DOI: 10.1093/nar/gkl320
  ISSN: 0305-1048
  issue: suppl_2
  issued:
    - year: 2006
      month: 7
      day: 1
  note: 'interest: 89'
  page: W729-W732
  source: Silverchair
  title: 'Taverna: a tool for building and running workflows of services'
  title-short: Taverna
  type: article-journal
  URL: https://doi.org/10.1093/nar/gkl320
  volume: '34'

- id: hunter-zinckTenSimpleRules2021
  abstract: >-
    Functional, usable, and maintainable open-source software is increasingly
    essential to scientific research, but there is a large variation in formal
    training for software development and maintainability. Here, we propose 10
    “rules” centered on 2 best practice components: clean code and testing.
    These 2 areas are relatively straightforward and provide substantial utility
    relative to the learning investment. Adopting clean code practices helps to
    standardize and organize software code in order to enhance readability and
    reduce cognitive load for both the initial developer and subsequent
    contributors; this allows developers to concentrate on core functionality
    and reduce errors. Clean coding styles make software code more amenable to
    testing, including unit tests that work best with modular and consistent
    software code. Unit tests interrogate specific and isolated coding behavior
    to reduce coding errors and ensure intended functionality, especially as
    code increases in complexity; unit tests also implicitly provide example
    usages of code. Other forms of testing are geared to discover erroneous
    behavior arising from unexpected inputs or emerging from the interaction of
    complex codebases. Although conforming to coding styles and designing tests
    can add time to the software development project in the short term, these
    foundational tools can help to improve the correctness, quality, usability,
    and maintainability of open-source scientific software code. They also
    advance the principal point of scientific research: producing accurate
    results in a reproducible way. In addition to suggesting several tips for
    getting started with clean code and testing practices, we recommend numerous
    tools for the popular open-source scientific software languages Python, R,
    and Julia.
  accessed:
    - year: 2023
      month: 4
      day: 18
  author:
    - family: Hunter-Zinck
      given: Haley
    - family: Siqueira
      given: Alexandre Fioravante
      dropping-particle: de
    - family: Vásquez
      given: Váleri N.
    - family: Barnes
      given: Richard
    - family: Martinez
      given: Ciera C.
  citation-key: hunter-zinckTenSimpleRules2021
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1009481
  ISSN: 1553-7358
  issue: '11'
  issued:
    - year: 2021
      month: 11
      day: 11
  language: en
  page: e1009481
  publisher: Public Library of Science
  source: PLoS Journals
  title: >-
    Ten simple rules on writing clean and reliable open-source scientific
    software
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009481
  volume: '17'

- id: hvatumWhatThinkConway
  author:
    - family: Hvatum
      given: Lise
    - family: Kelly
      given: Allan
    - family: EuroPLoP focus group
      given: ''
  citation-key: hvatumWhatThinkConway
  event-title: EuroPLoP 2005
  title: What do I think about Conway's Law now?
  type: paper-conference

- id: igarashiPolymorphicGradualTyping2017
  abstract: >-
    We study an extension of gradual typing—a method to integrate dynamic typing
    and static typing smoothly in a single language—to parametric polymorphism
    and its theoretical properties, including conservativity of typing and
    semantics over both statically and dynamically typed languages, type safety,
    blame-subtyping theorem, and the gradual guarantee—the so-called refined
    criteria, advocated by Siek et al. We develop System FG, which is a
    gradually typed extension of System F with the dynamic type and a new type
    consistency relation, and translation to a new polymorphic blame calculus
    System FC, which is based on previous polymorphic blame calculi by Ahmed et
    al. The design of System FG and System FC, geared to the criteria, is
    influenced by the distinction between static and gradual type variables,
    first observed by Garcia and Cimini. This distinction is also useful to
    execute statically typed code without incurring additional overhead to
    manage type names as in the prior calculi. We prove that System FG satisfies
    most of the criteria: all but the hardest property of the gradual guarantee
    on semantics. We show that a key conjecture to prove the gradual guarantee
    leads to the Jack-of-All-Trades property, conjectured as an important
    property of the polymorphic blame calculus by Ahmed et al.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Igarashi
      given: Yuu
    - family: Sekiyama
      given: Taro
    - family: Igarashi
      given: Atsushi
  citation-key: igarashiPolymorphicGradualTyping2017
  container-title: Proceedings of the ACM on Programming Languages
  container-title-short: Proc. ACM Program. Lang.
  DOI: 10.1145/3110284
  issue: ICFP
  issued:
    - year: 2017
      month: 8
      day: 29
  note: 'interest: 84'
  page: 40:1–40:29
  source: September 2017
  title: On polymorphic gradual typing
  type: article-journal
  URL: https://doi.org/10.1145/3110284
  volume: '1'

- id: ike-nwosuPythonVirtualMachine2015
  abstract: "You know how to program in Python but are interested in what goes on under the covers of the interpreter? Well, fasten your seat-belts as this book will take you on a tour of \_the virtual machine that runs your Python code.\nIt will describe how Python code is compiled and run, how the language itself can be modified and will demystify the myster..."
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Ike-Nwosu
      given: Obi
  citation-key: ike-nwosuPythonVirtualMachine2015
  issued:
    - year: 2015
      month: 8
      day: 25
  language: eng
  publisher: Leanpub
  source: leanpub.com
  title: Inside The Python Virtual Machine
  type: book
  URL: https://leanpub.com/insidethepythonvirtualmachine

- id: ioannidisDiscussionWhyEstimate2014
  abstract: >-
    Jager and Leek have tried to estimate a false-discovery rate (FDR) in
    abstracts of articles published in five medical journals during 2000–2010.
    Their approach is flawed in sampling, calculations, and conclusions. It uses
    a tiny portion of select papers in highly select journals. Randomized
    controlled trials and systematic reviews (designs with the lowest
    anticipated false-positive rates) are 52% of the analyzed papers, while
    these designs account for only 4% in PubMed in the same period. The FDR
    calculations consider the entire published literature as equivalent to a
    single genomic experiment where all performed analyses are reported without
    selection or distortion. However, the data used are the P-values reported in
    the abstracts of published papers; these P-values are a highly distorted,
    highly select sample. Besides selective reporting biases, all other biases,
    in particular confounding in observational studies, are also ignored, while
    these are often the main drivers for high false-positive rates in the
    biomedical literature. A reproducibility check of the raw data shows that
    much of the data Jager and Leek used are either wrong or make no sense: most
    of the usable data were missed by their script, 94% of the abstracts that
    reported ≥2 P-values had high correlation/overlap between reported outcomes,
    and only a minority of P-values corresponded to relevant primary outcomes.
    The Jager and Leek paper exemplifies the dreadful combination of using
    automated scripts with wrong methods and unreliable data. Sadly, this
    combination is common in the medical literature.
  accessed:
    - year: 2022
      month: 11
      day: 15
  author:
    - family: Ioannidis
      given: John P. A.
  citation-key: ioannidisDiscussionWhyEstimate2014
  container-title: Biostatistics
  container-title-short: Biostatistics
  DOI: 10.1093/biostatistics/kxt036
  ISSN: 1465-4644
  issue: '1'
  issued:
    - year: 2014
      month: 1
      day: 1
  note: 'interest: 83'
  page: 28-36
  source: Silverchair
  title: >-
    Discussion: Why “An estimate of the science-wise false discovery rate and
    application to the top medical literature” is false
  title-short: Discussion
  type: article-journal
  URL: https://doi.org/10.1093/biostatistics/kxt036
  volume: '15'

- id: ioannidisWhyMostPublished2005
  abstract: >-
    Summary There is increasing concern that most current published research
    findings are false. The probability that a research claim is true may depend
    on study power and bias, the number of other studies on the same question,
    and, importantly, the ratio of true to no relationships among the
    relationships probed in each scientific field. In this framework, a research
    finding is less likely to be true when the studies conducted in a field are
    smaller; when effect sizes are smaller; when there is a greater number and
    lesser preselection of tested relationships; where there is greater
    flexibility in designs, definitions, outcomes, and analytical modes; when
    there is greater financial and other interest and prejudice; and when more
    teams are involved in a scientific field in chase of statistical
    significance. Simulations show that for most study designs and settings, it
    is more likely for a research claim to be false than true. Moreover, for
    many current scientific fields, claimed research findings may often be
    simply accurate measures of the prevailing bias. In this essay, I discuss
    the implications of these problems for the conduct and interpretation of
    research.
  accessed:
    - year: 2022
      month: 9
      day: 14
  author:
    - family: Ioannidis
      given: John P. A.
  citation-key: ioannidisWhyMostPublished2005
  container-title: PLOS Medicine
  container-title-short: PLOS Medicine
  DOI: 10.1371/journal.pmed.0020124
  ISSN: 1549-1676
  issue: '8'
  issued:
    - year: 2005
      month: 8
      day: 30
  language: en
  page: e124
  publisher: Public Library of Science
  source: PLoS Journals
  title: Why Most Published Research Findings Are False
  type: article-journal
  URL: >-
    https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124
  volume: '2'

- id: iryBriefIncompleteMostly2009
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Iry
      given: James
  citation-key: iryBriefIncompleteMostly2009
  container-title: One Div Zero
  issued:
    - year: 2009
      month: 5
      day: 7
  title: A Brief, Incomplete, and Mostly Wrong History of Programming Languages
  type: post-weblog
  URL: http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html

- id: iso/iec25010:2011SystemsSoftwareEngineering2011
  abstract: >-
    Systems and software engineering — Systems and software Quality Requirements
    and Evaluation (SQuaRE) — System and software quality models
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: ISO/IEC 25010:2011
      given: ''
  citation-key: iso/iec25010:2011SystemsSoftwareEngineering2011
  issued:
    - year: 2011
      month: 3
  language: en
  publisher: ISO/IEC
  title: >-
    Systems and software engineering — Systems and software Quality Requirements
    and Evaluation (SQuaRE) — System and software quality models
  title-short: ISO/IEC 25010
  type: report
  URL: >-
    https://www.iso.org/cms/render/live/en/sites/isoorg/contents/data/standard/03/57/35733.html

- id: jacobToolDesigningReplicable2021
  abstract: >-
    When designing their performance evaluations, networking researchers often
    encounter questions such as: How long should a run be? How many runs to
    perform? How to account for the variability across multiple runs? What
    statistical methods should be used to analyze the data? Despite their best
    intentions, researchers often answer these questions differently, thus
    impairing the replicability of their evaluations and the confidence in their
    results. In this paper, we propose a concrete methodology for the design and
    analysis of performance evaluations. Our approach hierarchically partitions
    the performance evaluation into three timescales, following the principle of
    separation of concerns. The idea is to understand, for each timescale, the
    temporal characteristics of variability sources, and then to apply rigorous
    statistical methods to derive performance results with quantifiable
    confidence in spite of the inherent variability. We implement this
    methodology in a software framework called TriScale. For each performance
    metric, TriScale computes a variability score that estimates, with a given
    confidence, how similar the results would be if the evaluation were
    replicated; in other words, TriScale quantifies the replicability of
    evaluations. We showcase the practicality and usefulness of TriScale on four
    different case studies demonstrating that TriScale helps to generalize and
    strengthen published results. Improving the standards of replicability in
    networking is a complex challenge. This paper is an important contribution
    to this endeavor; it provides networking researchers with a rational and
    concrete experimental methodology rooted in sound statistical foundations.
    The first of its kind.
  accessed:
    - year: 2022
      month: 11
      day: 15
  author:
    - family: Jacob
      given: Romain
    - family: Zimmerling
      given: Marco
    - family: Boano
      given: Carlo Alberto
    - family: Vanbever
      given: Laurent
    - family: Thiele
      given: Lothar
  citation-key: jacobToolDesigningReplicable2021
  container-title: Journal of Systems Research
  DOI: 10.5070/SR31155408
  issue: '1'
  issued:
    - year: 2021
  language: en
  source: escholarship.org
  title: '[Tool] Designing Replicable Networking Experiments With Triscale'
  type: article-journal
  URL: https://escholarship.org/uc/item/63n4s9w2
  volume: '1'

- id: jagerEstimateSciencewiseFalse2014
  abstract: >-
    The accuracy of published medical research is critical for scientists,
    physicians and patients who rely on these results. However, the fundamental
    belief in the medical literature was called into serious question by a paper
    suggesting that most published medical research is false. Here we adapt
    estimation methods from the genomics community to the problem of estimating
    the rate of false discoveries in the medical literature using reported
    $P$-values as the data. We then collect $P$-values from the abstracts of all
    77 430 papers published in The Lancet, The Journal of the American Medical
    Association, The New England Journal of Medicine, The British Medical
    Journal, and The American Journal of Epidemiology between 2000 and 2010.
    Among these papers, we found 5322 reported $P$-values. We estimate that the
    overall rate of false discoveries among reported results is 14% (s.d. 1%),
    contrary to previous claims. We also found that there is no a significant
    increase in the estimated rate of reported false discovery results over time
    (0.5% more false positives (FP) per year, $P = 0.18$) or with respect to
    journal submissions (0.5% more FP per 100 submissions, $P = 0.12$).
    Statistical analysis must allow for false discoveries in order to make
    claims on the basis of noisy data. But our analysis suggests that the
    medical literature remains a reliable record of scientific progress.
  accessed:
    - year: 2022
      month: 11
      day: 15
  author:
    - family: Jager
      given: Leah R.
    - family: Leek
      given: Jeffrey T.
  citation-key: jagerEstimateSciencewiseFalse2014
  container-title: Biostatistics
  container-title-short: Biostatistics
  DOI: 10.1093/biostatistics/kxt007
  ISSN: 1465-4644
  issue: '1'
  issued:
    - year: 2014
      month: 1
      day: 1
  note: 'interest: 83'
  page: 1-12
  source: Silverchair
  title: >-
    An estimate of the science-wise false discovery rate and application to the
    top medical literature
  type: article-journal
  URL: https://doi.org/10.1093/biostatistics/kxt007
  volume: '15'

- id: janinCAREComprehensiveArchiver2014
  abstract: >-
    We present CARE, the Comprehensive Archiver for Reproducible Execution on
    Linux. CARE runs in userland, requires no setup and performs a single task:
    building an archive that contains selected executables and files accessed by
    a given application during an observation run. To reproduce computational
    results from this initial run, it is then enough to unpack the archive that
    comes equipped with all necessary tools for re-execution in a confined
    environment. Technically, CARE leverages on PRoot, a generic system call
    interposition engine that relies on the ptrace mechanism to monitor (and if
    needed to modify) system calls emitted by applications under scrutiny. PRoot
    is extensible and CARE is properly speaking an extension of PRoot. CARE is
    available on x86_64, x86 and ARM processors, and benefits from a new
    history-based algorithm that automatically selects files to be stored in a
    CARE archive.
  accessed:
    - year: 2024
      month: 2
      day: 14
  author:
    - family: Janin
      given: Yves
    - family: Vincent
      given: Cédric
    - family: Duraffort
      given: Rémi
  citation-key: janinCAREComprehensiveArchiver2014
  collection-title: TRUST '14
  container-title: >-
    Proceedings of the 1st ACM SIGPLAN Workshop on Reproducible Research
    Methodologies and New Publication Models in Computer Engineering
  DOI: 10.1145/2618137.2618138
  event-place: New York, NY, USA
  ISBN: 978-1-4503-2951-4
  issued:
    - year: 2014
      month: 6
      day: 9
  page: 1–7
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: CARE, the comprehensive archiver for reproducible execution
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2618137.2618138

- id: jiangGeneratingMergerTrees2014
  abstract: >-
    Halo merger trees describe the hierarchical assembly of dark matter haloes,
    and are the backbone for modelling galaxy formation and evolution. Merger
    trees constructed using Monte Carlo algorithms based on the extended
    Press–Schechter (EPS) formalism are complementary to using N-body
    simulations and have the advantage that they are not trammelled by limited
    numerical resolution and uncertainties in identifying and linking
    (sub)haloes. This paper compares multiple EPS-based merger tree algorithms
    to simulation results using four diagnostics: progenitor mass function, mass
    assembly history (MAH), merger rate per descendant halo and the unevolved
    subhalo mass function. Spherical collapse-based methods typically
    overpredict major-merger rates, whereas ellipsoidal collapse dramatically
    overpredicts the minor-merger rate for massive haloes. The only algorithm in
    our comparison that yields results in good agreement with simulations is
    that by Parkinson et al. (P08). We emphasize, though, that the simulation
    results used as benchmarks in testing the merger trees are hampered by
    significant uncertainties themselves: MAHs and merger rates from different
    studies easily disagree by 50 per cent, even when based on the same
    simulation. Given this status quo, the P08 merger trees can be considered as
    accurate as those extracted from simulations.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Jiang
      given: Fangzhou
    - family: Bosch
      given: Frank C.
      non-dropping-particle: van den
  citation-key: jiangGeneratingMergerTrees2014
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/stu280
  ISSN: 1365-2966, 0035-8711
  issue: '1'
  issued:
    - year: 2014
      month: 5
      day: 1
  language: en
  page: 193-207
  source: DOI.org (Crossref)
  title: 'Generating merger trees for dark matter haloes: a comparison of methods'
  title-short: Generating merger trees for dark matter haloes
  type: article-journal
  URL: >-
    http://academic.oup.com/mnras/article/440/1/193/1747580/Generating-merger-trees-for-dark-matter-haloes-a
  volume: '440'

- id: jiEnablingRefinableCrossHost2018
  abstract: >-
    Investigating attacks across multiple hosts is challenging. The true
    dependencies between security-sensitive files, network endpoints, or memory
    objects from different hosts can be easily concealed by dependency explosion
    or undefined program behavior (e.g., memory corruption). Dynamic information
    flow tracking (DIFT) is a potential solution to this problem, but, existing
    DIFT techniques only track information flow within a single host and lack an
    efficient mechanism to maintain and synchronize the data flow tags globally
    across multiple hosts. 

    In this paper, we propose RTAG, an efficient data flow tagging and tracking
    mechanism that enables practical cross-host attack investigations. RTAG is
    based on three novel techniques. First, by using a record-and-replay
    technique, it decouples the dependencies between different data flow tags
    from the analysis, enabling lazy synchronization between independent and
    parallel DIFT instances of different hosts. Second, it takes advantage of
    systemcall-level provenance information to calculate and allocate the
    optimal tag map in terms of memory consumption. Third, it embeds tag
    information into network packets to track cross-host data flows with less
    than 0.05% network bandwidth overhead. Evaluation results show that RTAG is
    able to recover the true data flows of realistic cross-host attack
    scenarios. Performance wise, RTAG reduces the memory consumption of
    DIFT-based analysis by up to 90% and decreases the overall analysis time by
    60%–90% compared with previous investigation systems.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Ji
      given: Yang
    - family: Lee
      given: Sangho
    - family: Fazzini
      given: Mattia
    - family: Allen
      given: Joey
    - family: Downing
      given: Evan
    - family: Kim
      given: Taesoo
    - family: Orso
      given: Alessandro
    - family: Lee
      given: Wenke
  citation-key: jiEnablingRefinableCrossHost2018
  event-title: 27th USENIX Security Symposium (USENIX Security 18)
  ISBN: 978-1-939133-04-5
  issued:
    - year: 2018
  language: en
  page: 1705-1722
  source: www.usenix.org
  title: >-
    Enabling Refinable {Cross-Host} Attack Investigation with Efficient Data
    Flow Tagging and Tracking
  type: paper-conference
  URL: https://www.usenix.org/conference/usenixsecurity18/presentation/jia-yang

- id: jindalMagpiePythonSpeed2021
  abstract: "Python has become overwhelmingly popular for ad-hoc data analysis, and Pandas dataframes have quickly become the de facto standard\_ API for data science. However, performance and scaling to large datasets remain significant challenges. This is in stark contrast with the\_ world of databases, where decades of investments have led to both sub-millisecond latencies for small queries and many orders of\_ magnitude better scalability for large analytical queries. Furthermore, databases offer enterprise-grade features (e.g., transactions, fine-grained access control, tamper-proof logging, encryption) as well as a mature ecosystem of tools in modern clouds.\nIn this paper, we bring together the ease of use and versatility of Python environments with the enterprise-grade, high-performance query processing of cloud database systems. We describe a system we are building, coined Magpie, which exposes the popular Pandas API while lazily pushing large chunks of computation into scalable, efficient, and secured database engines. Magpie assists the data scientist by automatically selecting the most efficient engine (e.g., SQL DW, SCOPE, Spark) in cloud environments that offer multiple engines atop a data lake. Magpie’s common data layer virtually eliminates data transfer costs across potentially many such engines. We describe experiments pushing Python dataframe programs into the SQL DW, Spark, and SCOPE query engines. An initial analysis of our production workloads suggest that over a quarter of the computations in our internal analytics clusters could be optimized through Magpie by picking the optimal backend."
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Jindal
      given: Alekh
    - family: Emani
      given: K. Venkatesh
    - family: Daum
      given: Maureen
    - family: Poppe
      given: Olga
    - family: Haynes
      given: Brandon
    - family: Pavlenko
      given: Anna
    - family: Gupta
      given: Ayushi
    - family: Ramachandra
      given: Karthik
    - family: Curino
      given: Carlo
    - family: Müller
      given: Andreas
    - family: Wu
      given: Wentao
    - family: Patel
      given: Hiren
  citation-key: jindalMagpiePythonSpeed2021
  container-title: >-
    11th Conference on Innovative Data Systems Research, CIDR 2021, Virtual
    Event, January 11-15, 2021, Online Proceedings
  event-place: Virtual event
  event-title: Conference on Innovative Data Systems Research (CIDR)
  issued:
    - year: 2021
      month: 2
  note: 'interest: 75'
  publisher: www.cidrdb.org
  publisher-place: Virtual event
  source: DBLP Computer Science Bibliography
  title: 'Magpie: Python at Speed and Scale using Cloud Backends'
  title-short: Magpie
  type: paper-conference
  URL: >-
    https://www.microsoft.com/en-us/research/publication/magpie-python-at-speed-and-scale-using-cloud-backends/

- id: jiRAINRefinableAttack2017
  abstract: >-
    As modern attacks become more stealthy and persistent, detecting or
    preventing them at their early stages becomes virtually impossible. Instead,
    an attack investigation or provenance system aims to continuously monitor
    and log interesting system events with minimal overhead. Later, if the
    system observes any anomalous behavior, it analyzes the log to identify who
    initiated the attack and which resources were affected by the attack and
    then assess and recover from any damage incurred. However, because of a
    fundamental tradeoff between log granularity and system performance,
    existing systems typically record system-call events without detailed
    program-level activities (e.g., memory operation) required for accurately
    reconstructing attack causality or demand that every monitored program be
    instrumented to provide program-level information. To address this issue, we
    propose RAIN, a Refinable Attack INvestigation system based on a
    record-replay technology that records system-call events during runtime and
    performs instruction-level dynamic information flow tracking (DIFT) during
    on-demand process replay. Instead of replaying every process with DIFT, RAIN
    conducts system-call-level reachability analysis to filter out unrelated
    processes and to minimize the number of processes to be replayed, making
    inter-process DIFT feasible. Evaluation results show that RAIN effectively
    prunes out unrelated processes and determines attack causality with
    negligible false positive rates. In addition, the runtime overhead of RAIN
    is similar to existing system-call level provenance systems and its analysis
    overhead is much smaller than full-system DIFT.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Ji
      given: Yang
    - family: Lee
      given: Sangho
    - family: Downing
      given: Evan
    - family: Wang
      given: Weiren
    - family: Fazzini
      given: Mattia
    - family: Kim
      given: Taesoo
    - family: Orso
      given: Alessandro
    - family: Lee
      given: Wenke
  citation-key: jiRAINRefinableAttack2017
  collection-title: CCS '17
  container-title: >-
    Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
    Security
  DOI: 10.1145/3133956.3134045
  event-place: New York, NY, USA
  ISBN: 978-1-4503-4946-8
  issued:
    - year: 2017
      month: 10
      day: 30
  page: 377–390
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    RAIN: Refinable Attack Investigation with On-demand Inter-Process
    Information Flow Tracking
  title-short: RAIN
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3133956.3134045

- id: jiRecProvProvenanceAwareUser2016
  abstract: >-
    Deterministic record and replay systems have widely been used in software
    debugging, failure diagnosis, and intrusion detection. In order to detect
    the Advanced Persistent Threat (APT), online execution needs to be recorded
    with acceptable runtime overhead; then, investigators can analyze the
    replayed execution with heavy dynamic instrumentation. While most record and
    replay systems rely on kernel module or OS virtualization, those running at
    user space are favoured for being lighter weight and more portable without
    any of the changes needed for OS/Kernel virtualization. On the other hand,
    higher level provenance data at a higher level provides dynamic analysis
    with system causalities and hugely increases its efficiency. Considering
    both benefits, we propose a provenance-aware user space record and replay
    system, called RecProv. RecProv is designed to provide high provenance
    fidelity; specifically, with versioning files from the recorded trace logs
    and integrity protection to provenance data through real-time trace
    isolation. The collected provenance provides the high-level system
    dependency that helps pinpoint suspicious activities where further analysis
    can be applied. We show that RecProv is able to output accurate provenance
    in both visualized graph and W3C standardized PROV-JSON formats.
  author:
    - family: Ji
      given: Yang
    - family: Lee
      given: Sangho
    - family: Lee
      given: Wenke
  citation-key: jiRecProvProvenanceAwareUser2016
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-319-40593-3_1
  editor:
    - family: Mattoso
      given: Marta
    - family: Glavic
      given: Boris
  event-place: Cham
  ISBN: 978-3-319-40593-3
  issued:
    - year: 2016
  language: en
  page: 3-15
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: 'RecProv: Towards Provenance-Aware User Space Record and Replay'
  title-short: RecProv
  type: paper-conference

- id: johnsonMoreDotsSyntactic
  abstract: >-
    More Dots: Syntactic Loop Fusion in Julia | After a lengthy design process
    (https://github.com/JuliaLang/julia/issues/8450) and preliminary foundations
    in Julia 0.5
    (/blog/2016-10-11-julia-0.5-highlights#vectorized_function_calls), Julia 0.6
    includes new facilities for writing code in the
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Johnson
      given: Steven G.
  citation-key: johnsonMoreDotsSyntactic
  language: en
  note: 'interest: 72'
  title: 'More Dots: Syntactic Loop Fusion in Julia'
  title-short: More Dots
  type: webpage
  URL: https://julialang.org/blog/2017/01/moredots/

- id: joppaTroublingTrendsScientific2013
  abstract: |-
    "Blind trust" is dangerous when choosing software to support research. 
              ,  
                 
                  Software pervades every domain of science ( 
                   
                    1 
                   
                  – 
                   
                    3 
                   
                  ), perhaps nowhere more decisively than in modeling. In key scientific areas of great societal importance, models and the software that implement them define both how science is done and what science is done ( 
                   
                    4 
                   
                  , 
                   
                    5 
                   
                  ). Across all science, this dependence has led to concerns around the need for open access to software ( 
                   
                    6 
                   
                  , 
                   
                    7 
                   
                  ), centered on the reproducibility of research ( 
                   
                    1 
                   
                  , 
                   
                    8 
                   
                  – 
                   
                    10 
                   
                  ). From fields such as high-performance computing, we learn key insights and best practices for how to develop, standardize, and implement software ( 
                   
                    11 
                   
                  ). Open and systematic approaches to the development of software are essential for all sciences. But for many scientists this is not sufficient. We describe problems with the adoption and use of scientific software.
  accessed:
    - year: 2022
      month: 6
      day: 6
  author:
    - family: Joppa
      given: Lucas N.
    - family: McInerny
      given: Greg
    - family: Harper
      given: Richard
    - family: Salido
      given: Lara
    - family: Takeda
      given: Kenji
    - family: O'Hara
      given: Kenton
    - family: Gavaghan
      given: David
    - family: Emmott
      given: Stephen
  citation-key: joppaTroublingTrendsScientific2013
  container-title: Science
  container-title-short: Science
  DOI: 10.1126/science.1231535
  ISSN: 0036-8075, 1095-9203
  issue: '6134'
  issued:
    - year: 2013
      month: 5
      day: 17
  language: en
  note: 'interest: 69'
  page: 814-815
  source: DOI.org (Crossref)
  title: Troubling Trends in Scientific Software Use
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.1231535
  volume: '340'

- id: josephCreatingEconomicModels2013
  abstract: >-
    This pilot study investigates how high-performance computing (HPC)
    investments can improve economic success and increase scientific innovation.
    This research is focused on the common good and should be useful to DOE,
    other government agencies, industry, and academia. The study has created two
    unique economic models and an innovation index:

    - A macroeconomic model that depicts the way HPC investments result in
    economic advancements in the form of ROI in revenue (GDP), profits (and cost
    savings), and jobs.

    - A macroeconomic model that depicts the way HPC investments result in basic
    and applied innovations, looking at variations by sector, industry, country,
    and organization size.

    - A new innovation index that provides a means of measuring and comparing
    innovation levels.

    Key findings of the pilot study include:

    - IDC is able to collect the required data across a broad set of
    organizations, with enough detail to create the economic models and the
    innovation index.

    - Early results indicate very substantial returns for investments in HPC:

    - $356.5 on average in revenue per dollar of HPC invested

    - $38.7 on average of profits (or cost savings) per dollar of HPC invested

    - The average number of years before returns started was 1.9 years.

    - The average HPC investment per innovation was $3.1 million.
  accessed:
    - year: 2022
      month: 5
      day: 23
  author:
    - family: Joseph
      given: Earl C.
    - family: Dekate
      given: Chirag
    - family: Conway
      given: Steve
  citation-key: josephCreatingEconomicModels2013
  issued:
    - year: 2013
      month: 9
  number: '243296'
  publisher: IDC
  title: >-
    Creating Economic Models Showing the Relationship Between Investments in HPC
    and the Resulting Financial ROI and Innovation — and How It Can Impact a
    Nation's Competitiveness and Innovation
  type: report
  URL: https://www.hpcuserforum.com/ROI/

- id: jupyterBinder20Reproducible2018
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Jupyter
      given: Project
    - family: Bussonnier
      given: Matthias
    - family: Forde
      given: Jessica
    - family: Freeman
      given: Jeremy
    - family: Granger
      given: Brian
    - family: Head
      given: Tim
    - family: Holdgraf
      given: Chris
    - family: Kelley
      given: Kyle
    - family: Nalvarte
      given: Gladys
    - family: Osheroff
      given: Andrew
    - family: Pacer
      given: M
    - family: Panda
      given: Yuvi
    - family: Perez
      given: Fernando
    - family: Ragan-Kelley
      given: Benjamin
    - family: Willing
      given: Carol
  citation-key: jupyterBinder20Reproducible2018
  DOI: 10.25080/Majora-4af1f417-011
  event-place: Austin, Texas
  event-title: Python in Science Conference
  issued:
    - year: 2018
  page: 113-120
  publisher-place: Austin, Texas
  source: DOI.org (Crossref)
  title: >-
    Binder 2.0 - Reproducible, interactive, sharable environments for science at
    scale
  type: paper-conference
  URL: https://doi.curvenote.com/10.25080/Majora-4af1f417-011

- id: juristoReplicationSoftwareEngineering2012
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Juristo
      given: Natalia
    - family: Gómez
      given: Omar S.
  citation-key: juristoReplicationSoftwareEngineering2012
  container-title: Empirical Software Engineering and Verification
  DOI: 10.1007/978-3-642-25231-0_2
  editor:
    - family: Meyer
      given: Bertrand
    - family: Nordio
      given: Martin
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-25230-3 978-3-642-25231-0
  issued:
    - year: 2012
  page: 60-88
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: Replication of Software Engineering Experiments
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-642-25231-0_2
  volume: '7007'

- id: kaliberaQuantifyingPerformanceChanges2020
  abstract: >-
    Measuring performance & quantifying a performance change are core evaluation
    techniques in programming language and systems research. Of 122 recent
    scientific papers, as many as 65 included experimental evaluation that
    quantified a performance change using a ratio of execution times. Few of
    these papers evaluated their results with the level of rigour that has come
    to be expected in other experimental sciences. The uncertainty of measured
    results was largely ignored. Scarcely any of the papers mentioned
    uncertainty in the ratio of the mean execution times, and most did not even
    mention uncertainty in the two means themselves. Most of the papers failed
    to address the non-deterministic execution of computer programs (caused by
    factors such as memory placement, for example), and none addressed
    non-deterministic compilation. It turns out that the statistical methods
    presented in the computer systems performance evaluation literature for the
    design and summary of experiments do not readily allow this either. This
    poses a hazard to the repeatability, reproducibility and even validity of
    quantitative results. Inspired by statistical methods used in other fields
    of science, and building on results in statistics that did not make it to
    introductory textbooks, we present a statistical model that allows us both
    to quantify uncertainty in the ratio of (execution time) means and to design
    experiments with a rigorous treatment of those multiple sources of
    non-determinism that might impact measured performance. Better still, under
    our framework summaries can be as simple as "system A is faster than system
    B by 5.5% $\pm$ 2.5%, with 95% confidence", a more natural statement than
    those derived from typical current practice, which are often misinterpreted.
    November 2013
  accessed:
    - year: 2023
      month: 8
      day: 22
  author:
    - family: Kalibera
      given: Tomas
    - family: Jones
      given: Richard
  citation-key: kaliberaQuantifyingPerformanceChanges2020
  DOI: 10.48550/arXiv.2007.10899
  issued:
    - year: 2020
      month: 7
      day: 21
  number: arXiv:2007.10899
  publisher: arXiv
  source: arXiv.org
  title: Quantifying Performance Changes with Effect Size Confidence Intervals
  type: article
  URL: http://arxiv.org/abs/2007.10899

- id: kanewalaTestingScientificSoftware2014
  abstract: >-
    Context: Scientific software plays an important role in critical decision
    making, for example making weather predictions based on climate models, and
    computation of evidence for research publications. Recently, scientists have
    had to retract publications due to errors caused by software faults.
    Systematic testing can identify such faults in code. Objective: This study
    aims to identify specific challenges, proposed solutions, and unsolved
    problems faced when testing scientific software. Method: We conducted a
    systematic literature survey to identify and analyze relevant literature. We
    identified 62 studies that provided relevant information about testing
    scientific software. Results: We found that challenges faced when testing
    scientific software fall into two main categories: (1) testing challenges
    that occur due to characteristics of scientific software such as oracle
    problems and (2) testing challenges that occur due to cultural differences
    between scientists and the software engineering community such as viewing
    the code and the model that it implements as inseparable entities. In
    addition, we identified methods to potentially overcome these challenges and
    their limitations. Finally we describe unsolved challenges and how software
    engineering researchers and practitioners can help to overcome them.
    Conclusions: Scientific software presents special challenges for testing.
    Specifically, cultural differences between scientist developers and software
    engineers, along with the characteristics of the scientific software make
    testing more difficult. Existing techniques such as code clone detection can
    help to improve the testing process. Software engineers should consider
    special challenges posed by scientific software such as oracle problems when
    developing testing techniques.
  accessed:
    - year: 2023
      month: 5
      day: 6
  author:
    - family: Kanewala
      given: Upulee
    - family: Bieman
      given: James M.
  citation-key: kanewalaTestingScientificSoftware2014
  container-title: Information and Software Technology
  container-title-short: Inf. Softw. Technol.
  DOI: 10.1016/j.infsof.2014.05.006
  ISSN: 0950-5849
  issue: '10'
  issued:
    - year: 2014
      month: 10
      day: 1
  note: 'interest: 95'
  page: 1219–1232
  source: ACM Digital Library
  title: 'Testing scientific software: A systematic literature review'
  title-short: Testing scientific software
  type: article-journal
  URL: https://doi.org/10.1016/j.infsof.2014.05.006
  volume: '56'

- id: kaptur500LinesLess
  abstract: >-
    Byterun is a Python interpreter implemented in Python. Through my work on
    Byterun, I was surprised and delighted to discover that the fundamental
    structure of the Python interpreter fits easily into the 500-line size
    restriction. This chapter will walk through the structure of the interpreter
    and give you enough context to explore it further. The goal is not to
    explain everything there is to know about interpreters—like so many
    interesting areas of programming and computer science, you could devote
    years to developing a deep understanding of the topic.


    Byterun was written by Ned Batchelder and myself, building on the work of
    Paul Swartz. Its structure is similar to the primary implementation of
    Python, CPython, so understanding Byterun will help you understand
    interpreters in general and the CPython interpreter in particular. (If you
    don't know which Python you're using, it's probably CPython.) Despite its
    short length, Byterun is capable of running most simple Python programs.
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Kaptur
      given: Allison
  citation-key: kaptur500LinesLess
  title: 500 Lines or Less | A Python Interpreter Written in Python
  type: webpage
  URL: http://www.aosabook.org/en/500L/a-python-interpreter-written-in-python.html

- id: karpinskiPutThisYour
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Karpinski
      given: Stefan
  citation-key: karpinskiPutThisYour
  language: en
  note: 'interest: 63'
  title: Put This In Your Pipe
  type: post-weblog
  URL: https://julialang.org/blog/2013/04/put-this-in-your-pipe/

- id: karpinskiShellingOutSucks
  abstract: Shelling Out Sucks
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Karpinski
      given: Stefan
  citation-key: karpinskiShellingOutSucks
  language: en
  note: 'interest: 81'
  title: Shelling Out Sucks
  type: post-weblog
  URL: https://julialang.org/blog/2012/03/shelling-out-sucks/

- id: katcherPostMarkNewFile2005
  abstract: >-
    Existing file system benchmarks are deficient in portraying performance in
    the ephemeral small-file regime used by Internet software, especially:

        electronic mail;
        netnews; and
        web-based commerce. 

    PostMark is a new benchmark to measure performance for this class of
    application.


    In this paper, PostMark test results are presented and analyzed for both
    UNIX and Windows NT application servers. Network Appliance Filers (file
    server appliances) are shown to provide superior performance (via NFS or
    CIFS) compared to local disk alternatives, especially at higher loads. Such
    results are consistent with reports from ISPs (Internet Service Providers)
    who have deployed NetApp filers to support such applications on a large
    scale.
  accessed:
    - year: 2024
      month: 1
      day: 22
  author:
    - family: Katcher
      given: Jeffrey
  citation-key: katcherPostMarkNewFile2005
  collection-title: 'NetApp Library: Technical Reports'
  issued:
    - year: 2005
      month: 9
      day: 1
  number: TR3022
  title: 'PostMark: A New File System Benchmark'
  title-short: Network Appliance - PostMark
  type: report
  URL: >-
    https://web.archive.org/web/20050901112245/https://www.netapp.com/tech_library/3022.html

- id: katsaggelosSuperResolutionImages2007
  accessed:
    - year: 2022
      month: 5
      day: 2
  author:
    - family: Katsaggelos
      given: Aggelos K.
    - family: Molina
      given: Rafael
    - family: Mateos
      given: Javier
  citation-key: katsaggelosSuperResolutionImages2007
  container-title: Synthesis Lectures on Image, Video, and Multimedia Processing
  DOI: 10.2200/S00036ED1V01Y200606IVM007
  ISSN: 1559-8136
  issue: '1'
  issued:
    - year: 2007
      month: 1
  page: 1-134
  publisher: Morgan & Claypool Publishers
  source: morganclaypool.com (Atypon)
  title: Super Resolution of Images and Video
  type: article-journal
  URL: https://www.morganclaypool.com/doi/abs/10.2200/S00036ED1V01Y200606IVM007
  volume: '3'

- id: katzApplicationSkeletonsConstruction2016
  abstract: >-
    Computer scientists who work on tools and systems to support eScience (a
    variety of parallel and distributed) applications usually use actual
    applications to prove that their systems will benefit science and
    engineering (e.g., improve application performance). Accessing and building
    the applications and necessary data sets can be difficult because of policy
    or technical issues, and it can be difficult to modify the characteristics
    of the applications to understand corner cases in the system design. In this
    paper, we present the Application Skeleton, a simple yet powerful tool to
    build synthetic applications that represent real applications, with runtime
    and I/O close to those of the real applications. This allows computer
    scientists to focus on the system they are building; they can work with the
    simpler skeleton applications and be sure that their work will also be
    applicable to the real applications. In addition, skeleton applications
    support simple reproducible system experiments since they are represented by
    a compact set of parameters. Our Application Skeleton tool (available as
    open source at https://github.com/applicationskeleton/Skeleton) currently
    can create easy-to-access, easy-to-build, and easy-to-run bag-of-task,
    (iterative) map-reduce, and (iterative) multistage workflow applications.
    The tasks can be serial, parallel, or a mix of both. The parameters to
    represent the tasks can either be discovered through a manual profiling of
    the applications or through an automated method. We select three
    representative applications (Montage, BLAST, CyberShake Postprocessing),
    then describe and generate skeleton applications for each. We show that the
    skeleton applications have identical (or close) performance to that of the
    real applications. We then show examples of using skeleton applications to
    verify system optimizations such as data caching, I/O tuning, and task
    scheduling, as well as the system resilience mechanism, in some cases
    modifying the skeleton applications to emphasize some characteristic, and
    thus show that using skeleton applications simplifies the process of
    designing, implementing, and testing these optimizations.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Katz
      given: Daniel S.
    - family: Merzky
      given: Andre
    - family: Zhang
      given: Zhao
    - family: Jha
      given: Shantenu
  citation-key: katzApplicationSkeletonsConstruction2016
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2015.10.001
  ISSN: 0167-739X
  issued:
    - year: 2016
      month: 6
      day: 1
  language: en
  note: 'interest: 90'
  page: 114-124
  source: ScienceDirect
  title: 'Application skeletons: Construction and use in eScience'
  title-short: Application skeletons
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167739X15003143
  volume: '59'

- id: katzRSEExplainer2022
  abstract: >-
    RSE explainer - Daniel S. Katz, National Center for Supercomputing
    Applications (NCSA), USA
  accessed:
    - year: 2022
      month: 4
      day: 20
  author:
    - family: Katz
      given: Dan
  citation-key: katzRSEExplainer2022
  issued:
    - year: 2022
      month: 3
      day: 23
  title: RSE Explainer
  title-short: Day 1, Session 2
  type: speech
  URL: https://www.youtube.com/watch?v=L-KCAzFr8no

- id: katzSoftwareReproducibilityPossible2017
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Katz
      given: Dan
  citation-key: katzSoftwareReproducibilityPossible2017
  container-title: Daniel S. Katz's blog
  issued:
    - year: 2017
      month: 2
      day: 7
  language: en
  title: Is software reproducibility possible and practical?
  type: post-weblog
  URL: >-
    https://danielskatzblog.wordpress.com/2017/02/07/is-software-reproducibility-possible-and-practical/

- id: katzStateSustainableResearch2019
  abstract: >-
    Article: The State of Sustainable Research Software: Learning from the
    Workshop on Sustainable Software for Science: Practice and Experiences
    (WSSSPE5.1)
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Katz
      given: Daniel S.
    - family: Druskat
      given: Stephan
    - family: Haines
      given: Robert
    - family: Jay
      given: Caroline
    - family: Struck
      given: Alexander
  citation-key: katzStateSustainableResearch2019
  container-title: Journal of Open Research Software
  DOI: 10.5334/jors.242
  ISSN: 2049-9647
  issue: '1'
  issued:
    - year: 2019
      month: 4
      day: 2
  language: en
  license: >-
    Authors who publish with this journal agree to the following terms:   
    Authors retain copyright and grant the journal right of first publication
    with the work simultaneously licensed under a  Creative Commons Attribution
    License  that allows others to share the work with an acknowledgement of the
    work's authorship and initial publication in this journal.  Authors are able
    to enter into separate, additional contractual arrangements for the
    non-exclusive distribution of the journal's published version of the work
    (e.g., post it to an institutional repository or publish it in a book), with
    an acknowledgement of its initial publication in this journal.  Authors are
    permitted and encouraged to post their work online (e.g., in institutional
    repositories or on their website) prior to and during the submission
    process, as it can lead to productive exchanges, as well as earlier and
    greater citation of published work (See  The Effect of Open Access ).  All
    third-party images reproduced on this journal are shared under Educational
    Fair Use. For more information on  Educational Fair Use , please see  this
    useful checklist prepared by Columbia University Libraries .   All
    copyright  of third-party content posted here for research purposes belongs
    to its original owners.  Unless otherwise stated all references to
    characters and comic art presented on this journal are ©, ® or ™ of their
    respective owners. No challenge to any owner’s rights is intended or should
    be inferred.
  note: 'interest: 85'
  number: '1'
  page: '11'
  publisher: Ubiquity Press
  source: openresearchsoftware.metajnl.com
  title: >-
    The State of Sustainable Research Software: Learning from the Workshop on
    Sustainable Software for Science: Practice and Experiences (WSSSPE5.1)
  title-short: The State of Sustainable Research Software
  type: article-journal
  URL: http://openresearchsoftware.metajnl.com/article/10.5334/jors.242/
  volume: '7'

- id: katzTakingFreshLook2021
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Katz
      given: Daniel S.
    - family: Gruenpeter
      given: Morane
    - family: Honeyman
      given: Tom
  citation-key: katzTakingFreshLook2021
  container-title: Patterns
  container-title-short: PATTER
  DOI: 10.1016/j.patter.2021.100222
  ISSN: 2666-3899
  issue: '3'
  issued:
    - year: 2021
      month: 3
      day: 12
  language: English
  note: 'interest: 74'
  PMID: '33748799'
  publisher: Elsevier
  source: www.cell.com
  title: Taking a fresh look at FAIR for research software
  type: article-journal
  URL: https://www.cell.com/patterns/abstract/S2666-3899(21)00036-2
  volume: '2'

- id: katzTransitiveCreditJSONLD2015
  abstract: 'Article: Transitive Credit and JSON-LD'
  accessed:
    - year: 2023
      month: 2
      day: 20
  author:
    - family: Katz
      given: Daniel S.
    - family: Smith
      given: Arfon M.
  citation-key: katzTransitiveCreditJSONLD2015
  container-title: Journal of Open Research Software
  DOI: 10.5334/jors.by
  ISSN: 2049-9647
  issue: '1'
  issued:
    - year: 2015
      month: 11
      day: 5
  language: en
  license: >-
    Authors who publish with this journal agree to the following terms:   
    Authors retain copyright and grant the journal right of first publication
    with the work simultaneously licensed under a  Creative Commons Attribution
    License  that allows others to share the work with an acknowledgement of the
    work's authorship and initial publication in this journal.  Authors are able
    to enter into separate, additional contractual arrangements for the
    non-exclusive distribution of the journal's published version of the work
    (e.g., post it to an institutional repository or publish it in a book), with
    an acknowledgement of its initial publication in this journal.  Authors are
    permitted and encouraged to post their work online (e.g., in institutional
    repositories or on their website) prior to and during the submission
    process, as it can lead to productive exchanges, as well as earlier and
    greater citation of published work (See  The Effect of Open Access ).  All
    third-party images reproduced on this journal are shared under Educational
    Fair Use. For more information on  Educational Fair Use , please see  this
    useful checklist prepared by Columbia University Libraries .   All
    copyright  of third-party content posted here for research purposes belongs
    to its original owners.  Unless otherwise stated all references to
    characters and comic art presented on this journal are ©, ® or ™ of their
    respective owners. No challenge to any owner’s rights is intended or should
    be inferred.
  number: '1'
  page: e7
  publisher: Ubiquity Press
  source: openresearchsoftware.metajnl.com
  title: Transitive Credit and JSON-LD
  type: article-journal
  URL: http://openresearchsoftware.metajnl.com/article/10.5334/jors.by/
  volume: '3'

- id: katzUsingWorkflowsExpressed2019
  abstract: "I’ve\_previously written about the concept of workflows, sets of independent tasks connected by data dependencies, being expressed either as data, for example, a Pegasus DAG\_or CWL document, o…"
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Katz
      given: Daniel S.
  citation-key: katzUsingWorkflowsExpressed2019
  container-title: Daniel S. Katz's blog
  issued:
    - year: 2019
      month: 2
      day: 5
  language: en
  note: 'interest: 91'
  title: Using workflows expressed as code and workflows expressed as data together
  type: post-weblog
  URL: >-
    https://danielskatzblog.wordpress.com/2019/02/05/using-workflows-expressed-as-code-and-workflows-expressed-as-data-together/

- id: kaufmanCarbonTaxVs2016
  abstract: >-
    Experts often debate the pros and cons of a carbon tax versus a
    cap-and-trade system. But WRI research finds that if well-designed, both
    policies can effectively reduce emissions in the United States.
  accessed:
    - year: 2022
      month: 4
      day: 13
  author:
    - family: Kaufman
      given: Noah
  citation-key: kaufmanCarbonTaxVs2016
  issued:
    - year: 2016
      month: 3
      day: 1
  language: en
  source: www.wri.org
  title: 'Carbon Tax vs. Cap-and-Trade: What’s a Better Policy to Cut Emissions?'
  title-short: Carbon Tax vs. Cap-and-Trade
  type: report
  URL: >-
    https://www.wri.org/insights/carbon-tax-vs-cap-and-trade-whats-better-policy-cut-emissions

- id: kauhanenRegressionTestSelection2021
  abstract: >-
    In this paper, we present a coverage-based regression test selection (RTS)
    approach and a developed tool for Python. The tool can be used either on a
    developer's machine or on build servers. A special characteristic of the
    tool is the attention to easy integration to continuous integration and
    deployment. To evaluate the performance of the proposed approach, mutation
    testing is applied to three open-source projects, and the results of the
    execution of full test suites are compared to the execution of a set of
    tests selected by the tool. The missed fault rate of the test selection
    varies between 0-2% at file-level granularity and 16-24% at line-level
    granularity. The high missed fault rate at the line-level granularity is
    related to the selected basic mutation approach and the result could be
    improved with advanced mutation techniques. Depending on the target
    optimization metric (time or precision) in DevOps/MLOps process the error
    rate could be acceptable or further improved by using file-level granularity
    based test selection.
  author:
    - family: Kauhanen
      given: Eero
    - family: Nurminen
      given: Jukka K.
    - family: Mikkonen
      given: Tommi
    - family: Pashkovskiy
      given: Matvei
  citation-key: kauhanenRegressionTestSelection2021
  container-title: >-
    2021 IEEE International Conference on Software Analysis, Evolution and
    Reengineering (SANER)
  DOI: 10.1109/SANER50967.2021.00077
  event-title: >-
    2021 IEEE International Conference on Software Analysis, Evolution and
    Reengineering (SANER)
  ISSN: 1534-5351
  issued:
    - year: 2021
      month: 3
  note: 'interest: 94'
  page: 618-621
  source: IEEE Xplore
  title: Regression Test Selection Tool for Python in Continuous Integration Process
  type: paper-conference

- id: keaheyLessonsLearnedChameleon2020
  abstract: >-
    The Chameleon testbed is a case study in adapting the cloud paradigm for
    computer science research. In this paper, we explain how this adaptation was
    achieved, evaluate it from the perspective of supporting the most
    experiments for the most users, and make a case that utilizing mainstream
    technology in research testbeds can increase efﬁciency without compromising
    on functionality. We also highlight the opportunity inherent in the shared
    digital artifacts generated by testbeds and give an overview of the efforts
    we’ve made to develop it to foster reproducibility.
  author:
    - family: Keahey
      given: Kate
    - family: Anderson
      given: Jason
    - family: Zhen
      given: Zhuo
    - family: Riteau
      given: Pierre
    - family: Ruth
      given: Paul
    - family: Stanzione
      given: Dan
    - family: Cevik
      given: Mert
    - family: Colleran
      given: Jacob
    - family: Gunawi
      given: Haryadi
    - family: Hammock
      given: Cody
    - family: Mambretti
      given: Joe
    - family: Barnes
      given: Alexander
    - family: Halbah
      given: Francois
    - family: Rocha
      given: Alex
    - family: Stubbs
      given: joe
  citation-key: keaheyLessonsLearnedChameleon2020
  container-title: 2020 USENIX Anuual Technical Conference
  event-title: USENIX ATC
  issued:
    - year: 2020
      month: 7
      day: 15
  language: en
  publisher: USENIX
  source: Zotero
  title: Lessons Learned from the Chameleon Testbed
  type: paper-conference
  URL: https://www.usenix.org/conference/atc20/presentation/keahey

- id: kelleyFrameworkCreatingKnowledge2021
  abstract: >-
    An increasing number of researchers rely on computational methods to
    generate or manipulate the results described in their scientific
    publications. Software created to this end—scientific software—is key to
    understanding, reproducing, and reusing existing work in many disciplines,
    ranging from Geosciences to Astronomy or Artificial Intelligence. However,
    scientific software is usually challenging to find, set up, and compare to
    similar software due to its disconnected documentation (dispersed in
    manuals, readme files, websites, and code comments) and the lack of
    structured metadata to describe it. As a result, researchers have to
    manually inspect existing tools to understand their differences and
    incorporate them into their work. This approach scales poorly with the
    number of publications and tools made available every year. In this paper we
    address these issues by introducing a framework for automatically extracting
    scientific software metadata from

    its documentation (in particular, their readme files); a methodology for
    structuring the extracted metadata in a Knowledge Graph (KG) of scientific
    software; and an exploitation framework for browsing and comparing the
    contents of the generated KG. We demonstrate our approach by creating a KG
    with metadata from over 10,000 scientific software entries from public code
    repositories.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Kelley
      given: Aidan
    - family: Garijo
      given: Daniel
  citation-key: kelleyFrameworkCreatingKnowledge2021
  container-title: Quantitative Science Studies
  container-title-short: Quantitative Science Studies
  DOI: 10.1162/qss_a_00167
  ISSN: 2641-3337
  issue: '4'
  issued:
    - year: 2021
      month: 12
      day: 1
  note: 'interest: 70'
  page: 1423-1446
  source: Silverchair
  title: A framework for creating knowledge graphs of scientific software metadata
  type: article-journal
  URL: https://doi.org/10.1162/qss_a_00167
  volume: '2'

- id: kemerlisLibdftPracticalDynamic2012
  abstract: >-
    Dynamic data flow tracking (DFT) deals with tagging and tracking data of
    interest as they propagate during program execution. DFT has been repeatedly
    implemented by a variety of tools for numerous purposes, including
    protection from zero-day and cross-site scripting attacks, detection and
    prevention of information leaks, and for the analysis of legitimate and
    malicious software. We present libdft, a dynamic DFT framework that unlike
    previous work is at once fast, reusable, and works with commodity software
    and hardware. libdft provides an API for building DFT-enabled tools that
    work on unmodified binaries, running on common operating systems and
    hardware, thus facilitating research and rapid prototyping. We explore
    different approaches for implementing the low-level aspects of
    instruction-level data tracking, introduce a more efficient and 64-bit
    capable shadow memory, and identify (and avoid) the common pitfalls
    responsible for the excessive performance overhead of previous studies. We
    evaluate libdft using real applications with large codebases like the Apache
    and MySQL servers, and the Firefox web browser. We also use a series of
    benchmarks and utilities to compare libdft with similar systems. Our results
    indicate that it performs at least as fast, if not faster, than previous
    solutions, and to the best of our knowledge, we are the first to evaluate
    the performance overhead of a fast dynamic DFT implementation in such depth.
    Finally, libdft is freely available as open source software.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Kemerlis
      given: Vasileios P.
    - family: Portokalidis
      given: Georgios
    - family: Jee
      given: Kangkook
    - family: Keromytis
      given: Angelos D.
  citation-key: kemerlisLibdftPracticalDynamic2012
  collection-title: VEE '12
  container-title: >-
    Proceedings of the 8th ACM SIGPLAN/SIGOPS conference on Virtual Execution
    Environments
  DOI: 10.1145/2151024.2151042
  event-place: New York, NY, USA
  ISBN: 978-1-4503-1176-2
  issued:
    - year: 2012
      month: 3
      day: 3
  page: 121–132
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'libdft: practical dynamic data flow tracking for commodity systems'
  title-short: libdft
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2151024.2151042

- id: kemperBuildCloudHow2011
  author:
    - family: Kemper
      given: Christian
  citation-key: kemperBuildCloudHow2011
  container-title: Google Engineering Tools
  issued:
    - year: 2011
      month: 8
      day: 18
  title: 'Build in the Cloud: How the Build System works'
  type: post-weblog
  URL: >-
    https://google-engtools.blogspot.com/2011/08/build-in-cloud-how-build-system-works.html

- id: kenistonKernelProbesKprobes
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Keniston
      given: Jim
    - family: Panchamukhi
      given: Prasanna S
    - family: Hiramatsu
      given: Masami
  citation-key: kenistonKernelProbesKprobes
  container-title: The Linux Kernel documentation
  language: en_US
  title: Kernel Probes (Kprobes)
  title-short: Kernel Probes (Kprobes)
  type: webpage
  URL: https://www.kernel.org/doc/html/latest/trace/kprobes.html

- id: kerriskNamespacesOperation2013
  accessed:
    - year: 2023
      month: 12
      day: 11
  author:
    - family: Kerrisk
      given: Michael
  citation-key: kerriskNamespacesOperation2013
  container-title: Linux Web News
  genre: News blog
  issued:
    - year: 2013
      month: 1
      day: 4
  title: Namespaces in operation
  type: webpage
  URL: https://lwn.net/Articles/531114/

- id: khanGuideConvolutionalNeural2018
  accessed:
    - year: 2022
      month: 5
      day: 2
  author:
    - family: Khan
      given: Salman
    - family: Rahmani
      given: Hossein
    - family: Shah
      given: Syed Afaq Ali
    - family: Bennamoun
      given: Mohammed
  citation-key: khanGuideConvolutionalNeural2018
  container-title: Synthesis Lectures on Computer Vision
  DOI: 10.2200/S00822ED1V01Y201712COV015
  ISSN: 2153-1056
  issue: '1'
  issued:
    - year: 2018
      month: 2
      day: 13
  page: 1-207
  publisher: Morgan & Claypool Publishers
  source: morganclaypool.com (Atypon)
  title: A Guide to Convolutional Neural Networks for Computer Vision
  type: article-journal
  URL: https://www.morganclaypool.com/doi/10.2200/S00822ED1V01Y201712COV015
  volume: '8'

- id: khanSharingInteroperableWorkflow2019
  abstract: >-
    The automation of data analysis in the form of scientific workflows has
    become a widely adopted practice in many fields of research. Computationally
    driven data-intensive experiments using workflows enable automation,
    scaling, adaptation, and provenance support. However, there are still
    several challenges associated with the effective sharing, publication, and
    reproducibility of such workflows due to the incomplete capture of
    provenance and lack of interoperability between different technical
    (software) platforms.Based on best-practice recommendations identified from
    the literature on workflow design, sharing, and publishing, we define a
    hierarchical provenance framework to achieve uniformity in provenance and
    support comprehensive and fully re-executable workflows equipped with
    domain-specific information. To realize this framework, we present CWLProv,
    a standard-based format to represent any workflow-based computational
    analysis to produce workflow output artefacts that satisfy the various
    levels of provenance. We use open source community-driven standards,
    interoperable workflow definitions in Common Workflow Language (CWL),
    structured provenance representation using the W3C PROV model, and resource
    aggregation and sharing as workflow-centric research objects generated along
    with the final outputs of a given workflow enactment. We demonstrate the
    utility of this approach through a practical implementation of CWLProv and
    evaluation using real-life genomic workflows developed by independent
    groups.The underlying principles of the standards utilized by CWLProv enable
    semantically rich and executable research objects that capture computational
    workflows with retrospective provenance such that any platform supporting
    CWL will be able to understand the analysis, reuse the methods for partial
    reruns, or reproduce the analysis to validate the published findings.
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Khan
      given: Farah Zaib
    - family: Soiland-Reyes
      given: Stian
    - family: Sinnott
      given: Richard O
    - family: Lonie
      given: Andrew
    - family: Goble
      given: Carole
    - family: Crusoe
      given: Michael R
  citation-key: khanSharingInteroperableWorkflow2019
  container-title: GigaScience
  container-title-short: GigaScience
  DOI: 10.1093/gigascience/giz095
  ISSN: 2047-217X
  issue: '11'
  issued:
    - year: 2019
      month: 11
      day: 1
  language: en
  note: 'interest: 99'
  page: giz095
  source: Silverchair
  title: >-
    Sharing interoperable workflow provenance: A review of best practices and
    their practical application in CWLProv
  title-short: Sharing interoperable workflow provenance
  type: article-journal
  URL: https://doi.org/10.1093/gigascience/giz095
  volume: '8'

- id: kidwellBadgesAcknowledgeOpen2016
  abstract: >-
    Beginning January 2014, Psychological Science gave authors the opportunity
    to signal open data and materials if they qualified for badges that
    accompanied published articles. Before badges, less than 3% of Psychological
    Science articles reported open data. After badges, 23% reported open data,
    with an accelerating trend; 39% reported open data in the first half of
    2015, an increase of more than an order of magnitude from baseline. There
    was no change over time in the low rates of data sharing among comparison
    journals. Moreover, reporting openness does not guarantee openness. When
    badges were earned, reportedly available data were more likely to be
    actually available, correct, usable, and complete than when badges were not
    earned. Open materials also increased to a weaker degree, and there was more
    variability among comparison journals. Badges are simple, effective signals
    to promote open practices and improve preservation of data and materials by
    using independent repositories.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Kidwell
      given: Mallory C.
    - family: Lazarević
      given: Ljiljana B.
    - family: Baranski
      given: Erica
    - family: Hardwicke
      given: Tom E.
    - family: Piechowski
      given: Sarah
    - family: Falkenberg
      given: Lina-Sophia
    - family: Kennett
      given: Curtis
    - family: Slowik
      given: Agnieszka
    - family: Sonnleitner
      given: Carina
    - family: Hess-Holden
      given: Chelsey
    - family: Errington
      given: Timothy M.
    - family: Fiedler
      given: Susann
    - family: Nosek
      given: Brian A.
  citation-key: kidwellBadgesAcknowledgeOpen2016
  container-title: PLOS Biology
  container-title-short: PLOS Biology
  DOI: 10.1371/journal.pbio.1002456
  ISSN: 1545-7885
  issue: '5'
  issued:
    - year: 2016
      month: 5
      day: 12
  language: en
  note: |-
    interest: 83
    pubpeer: yes
  page: e1002456
  publisher: Public Library of Science
  source: PLoS Journals
  title: >-
    Badges to Acknowledge Open Practices: A Simple, Low-Cost, Effective Method
    for Increasing Transparency
  title-short: Badges to Acknowledge Open Practices
  type: article-journal
  URL: >-
    https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002456
  volume: '14'

- id: kimOptimizingUnitTest2013
  abstract: >-
    Tao is a system that optimizes the execution of unit tests in large software
    programs and reduces the programmer wait time from minutes to seconds. Tao
    is based on two key ideas: First, Tao focuses on efficiency, unlike past
    work that focused on avoiding false negatives. Tao implements simple and
    fast function-level dependency tracking that identifies tests to run on a
    code change; any false negatives missed by this dependency tracking are
    caught by running the entire test suite on a test server once the code
    change is committed. Second, to make it easy for programmers to adopt Tao,
    it incorporates the dependency information into the source code repository.
    This paper describes an early prototype of Tao and demonstrates that Tao can
    reduce unit test execution time in two large Python software projects by
    over 96% while incurring few false negatives.
  accessed:
    - year: 2022
      month: 4
      day: 6
  author:
    - family: Kim
      given: Taesoo
    - family: Chandra
      given: Ramesh
    - family: Zeldovich
      given: Nickolai
  citation-key: kimOptimizingUnitTest2013
  collection-title: APSys '13
  container-title: Proceedings of the 4th Asia-Pacific Workshop on Systems
  DOI: 10.1145/2500727.2500748
  event-place: New York, NY, USA
  ISBN: 978-1-4503-2316-1
  issued:
    - year: 2013
      month: 7
      day: 29
  note: 'interest: 95'
  page: 1–6
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Optimizing unit test execution in large software programs using dependency
    analysis
  type: paper-conference
  URL: https://doi.org/10.1145/2500727.2500748

- id: kingMetaanalysisTechnologyAcceptance2006
  abstract: >-
    A statistical meta-analysis of the technology acceptance model (TAM) as
    applied in various fields was conducted using 88 published studies that
    provided sufficient data to be credible. The results show TAM to be a valid
    and robust model that has been widely used, but which potentially has wider
    applicability. A moderator analysis involving user types and usage types was
    performed to investigate conditions under which TAM may have different
    effects. The study confirmed the value of using students as surrogates for
    professionals in some TAM studies, and perhaps more generally. It also
    revealed the power of meta-analysis as a rigorous alternative to qualitative
    and narrative literature review methods.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: King
      given: William R.
    - family: He
      given: Jun
  citation-key: kingMetaanalysisTechnologyAcceptance2006
  container-title: Information & Management
  container-title-short: Information & Management
  DOI: 10.1016/j.im.2006.05.003
  ISSN: '03787206'
  issue: '6'
  issued:
    - year: 2006
      month: 9
  language: en
  page: 740-755
  source: DOI.org (Crossref)
  title: A meta-analysis of the technology acceptance model
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0378720606000528
  volume: '43'

- id: kingParseDonValidate2019
  abstract: >-
    About a month ago, I was reflecting on Twitter about the differences I
    experienced parsing JSON in statically- and dynamically-typed languages, and
    finally, I realized what I was looking for. Now I have a single, snappy
    slogan that encapsulates what type-driven design means to me, and better
    yet, it’s only three words long:

    Parse, don’t validate.
  accessed:
    - year: 2024
      month: 1
      day: 11
  author:
    - family: King
      given: Alexis
  citation-key: kingParseDonValidate2019
  issued:
    - year: 2019
      month: 11
      day: 5
  title: Parse, don’t validate
  type: post-weblog
  URL: https://lexi-lambda.github.io/blog/2019/11/05/parse-don-t-validate/

- id: kitchenhamGuidelinesPerformingSystematic2007
  archive_location: Keele University Technical Report TR/SE-0401 ISSN:1353-7776
  author:
    - family: Kitchenham
      given: Barbara
    - family: Charters
      given: Stuart
  citation-key: kitchenhamGuidelinesPerformingSystematic2007
  genre: EBSE Technical Report
  issued:
    - year: 2007
      month: 7
      day: 9
  number: EBSE-2007-01
  publisher: >-
    Software Engineering Group, School of Computer Science and Mathematics,
    Keele University
  title: >-
    Guidelines for performing Systematic Literature Reviews in Software
    Engineering
  type: report
  URL: https://www.elsevier.com/__data/promis_misc/525444systematicreviewsguide.pdf

- id: kladovWhyNotRust2020
  abstract: >-
    I’ve recently read an article criticizing Rust, and, while it made a bunch
    of good points, I didn’t enjoy it — it was an easy to argue with piece. In
    general, I feel that I can’t recommend an article criticizing Rust. This is
    a shame — confronting drawbacks is important, and debunking low effort/miss
    informed attempts at critique sadly inoculates against actually good
    arguments.


    So, here’s my attempt to argue against Rust:
  accessed:
    - year: 2023
      month: 3
      day: 9
  author:
    - family: Kladov
      given: Alex
  citation-key: kladovWhyNotRust2020
  container-title: matklad
  issued:
    - year: 2020
      month: 9
      day: 20
  title: Why Not Rust?
  type: post-weblog
  URL: https://matklad.github.io/2020/09/20/why-not-rust.html

- id: kleenIntelProcessorTrace2015
  author:
    - family: Kleen
      given: Andi
    - family: Strong
      given: Beeman
  citation-key: kleenIntelProcessorTrace2015
  event-place: Seattle, Washington, USA
  event-title: Tracing Summit 2015
  issued:
    - year: 2015
      month: 8
      day: 10
  language: en
  publisher-place: Seattle, Washington, USA
  title: Intel® Processor Trace on Linux
  type: speech

- id: kleinRunYourResearch2012
  abstract: >-
    Formal models serve in many roles in the programming language community. In
    its primary role, a model communicates the idea of a language design; the
    architecture of a language tool; or the essence of a program analysis. No
    matter which role it plays, however, a faulty model doesn't serve its
    purpose. 


    One way to eliminate flaws from a model is to write it down in a mechanized
    formal language. It is then possible to state theorems about the model, to
    prove them, and to check the proofs. Over the past nine years, PLT has
    developed and explored a lightweight version of this approach, dubbed Redex.
    In a nutshell, Redex is a domain-specific language for semantic models that
    is embedded in the Racket programming language. The effort of creating a
    model in Redex is often no more burdensome than typesetting it with LaTeX;
    the difference is that Redex comes with tools for the semantics engineering
    life cycle.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Klein
      given: Casey
    - family: Clements
      given: John
    - family: Dimoulas
      given: Christos
    - family: Eastlund
      given: Carl
    - family: Felleisen
      given: Matthias
    - family: Flatt
      given: Matthew
    - family: McCarthy
      given: Jay A.
    - family: Rafkind
      given: Jon
    - family: Tobin-Hochstadt
      given: Sam
    - family: Findler
      given: Robert Bruce
  citation-key: kleinRunYourResearch2012
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/2103621.2103691
  ISSN: 0362-1340, 1558-1160
  issue: '1'
  issued:
    - year: 2012
      month: 1
      day: 18
  language: en
  note: 'interest: 60'
  page: 285-296
  source: DOI.org (Crossref)
  title: 'Run your research: on the effectiveness of lightweight mechanization'
  title-short: Run your research
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2103621.2103691
  volume: '47'

- id: knebeGalaxiesGoingMAD2013
  abstract: >-
    With the ever-increasing size and complexity of fully self-consistent
    simulations of galaxy formation within the framework of the cosmic web, the
    demands upon object finders for these simulations have simultaneously grown.
    To this extent we initiated the Halo-Finder Comparison Project that gathered
    together all the experts in the field and has so far led to two comparison
    papers, one for dark matter field haloes, and one for dark matter subhaloes.
    However, as state-of-the-art simulation codes are perfectly capable of not
    only following the formation and evolution of dark matter but also
    accounting for baryonic physics, i.e. gas hydrodynamics, star formation,
    stellar feedback, etc., object finders should also be capable of taking
    these additional physical processes into consideration. Here we report – for
    the first time – on a comparison of codes as applied to the Constrained
    Local UniversE Simulation (CLUES) of the formation of the Local Group which
    incorporates much of the physics relevant for galaxy formation. We compare
    both the properties of the three main galaxies in the simulation
    (representing the Milky Way, Andromeda and M33) and their satellite
    populations for a variety of halo finders ranging from phase space to
    velocity space to spherical overdensity based codes, including also a mere
    baryonic object finder. We obtain agreement amongst codes comparable to (if
    not better than) our previous comparisons – at least for the total, dark and
    stellar components of the objects. However, the diffuse gas content of the
    haloes shows great disparity, especially for low-mass satellite galaxies.
    This is primarily due to differences in the treatment of the thermal energy
    during the unbinding procedure. We acknowledge that the handling of gas in
    halo finders is something that needs to be dealt with carefully, and the
    precise treatment may depend sensitively upon the scientific problem being
    studied.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Knebe
      given: Alexander
    - family: Libeskind
      given: Noam I.
    - family: Pearce
      given: Frazer
    - family: Behroozi
      given: Peter
    - family: Casado
      given: Javier
    - family: Dolag
      given: Klaus
    - family: Dominguez-Tenreiro
      given: Rosa
    - family: Elahi
      given: Pascal
    - family: Lux
      given: Hanni
    - family: Muldrew
      given: Stuart I.
    - family: Onions
      given: Julian
  citation-key: knebeGalaxiesGoingMAD2013
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/sts173
  ISSN: 1365-2966, 0035-8711
  issue: '3'
  issued:
    - year: 2013
      month: 1
      day: 21
  language: en
  page: 2039-2052
  source: DOI.org (Crossref)
  title: 'Galaxies going MAD: the Galaxy-Finder Comparison Project'
  title-short: Galaxies going MAD
  type: article-journal
  URL: >-
    http://academic.oup.com/mnras/article/428/3/2039/1062526/Galaxies-going-MAD-the-GalaxyFinder-Comparison
  volume: '428'

- id: knebeHaloesGoneMAD2011
  abstract: >-
    We present a detailed comparison of fundamental dark matter halo properties
    retrieved by a substantial number of different halo finders. These codes
    span a wide range of techniques including friends-of-friends,
    spherical-overdensity and phase-space-based algorithms. We further introduce
    a robust (and publicly available) suite of test scenarios that allow halo
    finder developers to compare the performance of their codes against those
    presented here. This set includes mock haloes containing various levels and
    distributions of substructure at a range of resolutions as well as a
    cosmological simulation of the large-scale structure of the universe.


    All the halo-finding codes tested could successfully recover the spatial
    location of our mock haloes. They further returned lists of particles
    (potentially) belonging to the object that led to coinciding values for the
    maximum of the circular velocity profile and the radius where it is reached.
    All the finders based in configuration space struggled to recover
    substructure that was located close to the centre of the host halo, and the
    radial dependence of the mass recovered varies from finder to finder. Those
    finders based in phase space could resolve central substructure although
    they found difficulties in accurately recovering its properties. Through a
    resolution study we found that most of the finders could not reliably
    recover substructure containing fewer than 30–40 particles. However, also
    here the phase-space finders excelled by resolving substructure down to
    10–20 particles. By comparing the halo finders using a high-resolution
    cosmological volume, we found that they agree remarkably well on fundamental
    properties of astrophysical significance (e.g. mass, position, velocity and
    peak of the rotation curve).


    We further suggest to utilize the peak of the rotation curve, vmax, as a
    proxy for mass, given the arbitrariness in defining a proper halo edge.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Knebe
      given: Alexander
    - family: Knollmann
      given: Steffen R.
    - family: Muldrew
      given: Stuart I.
    - family: Pearce
      given: Frazer R.
    - family: Aragon-Calvo
      given: Miguel Angel
    - family: Ascasibar
      given: Yago
    - family: Behroozi
      given: Peter S.
    - family: Ceverino
      given: Daniel
    - family: Colombi
      given: Stephane
    - family: Diemand
      given: Juerg
    - family: Dolag
      given: Klaus
    - family: Falck
      given: Bridget L.
    - family: Fasel
      given: Patricia
    - family: Gardner
      given: Jeff
    - family: Gottlöber
      given: Stefan
    - family: Hsu
      given: Chung-Hsing
    - family: Iannuzzi
      given: Francesca
    - family: Klypin
      given: Anatoly
    - family: Lukić
      given: Zarija
    - family: Maciejewski
      given: Michal
    - family: McBride
      given: Cameron
    - family: Neyrinck
      given: Mark C.
    - family: Planelles
      given: Susana
    - family: Potter
      given: Doug
    - family: Quilis
      given: Vicent
    - family: Rasera
      given: Yann
    - family: Read
      given: Justin I.
    - family: Ricker
      given: Paul M.
    - family: Roy
      given: Fabrice
    - family: Springel
      given: Volker
    - family: Stadel
      given: Joachim
    - family: Stinson
      given: Greg
    - family: Sutter
      given: P. M.
    - family: Turchaninov
      given: Victor
    - family: Tweed
      given: Dylan
    - family: Yepes
      given: Gustavo
    - family: Zemp
      given: Marcel
  citation-key: knebeHaloesGoneMAD2011
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1111/j.1365-2966.2011.18858.x
  ISSN: '00358711'
  issue: '3'
  issued:
    - year: 2011
      month: 8
      day: 11
  language: en
  page: 2293-2318
  source: DOI.org (Crossref)
  title: >-
    Haloes gone MAD★: The Halo-Finder Comparison Project: The Halo-Finder
    Comparison Project
  title-short: Haloes gone MAD★
  type: article-journal
  URL: >-
    https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2011.18858.x
  volume: '415'

- id: knebeStructureFindingCosmological2013
  abstract: >-
    The ever increasing size and complexity of data coming from simulations of
    cosmic structure formation demand equally sophisticated tools for their
    analysis. During the past decade, the art of object finding in these
    simulations has hence developed into an important discipline itself. A
    multitude of codes based upon a huge variety of methods and techniques have
    been spawned yet the question remained as to whether or not they will
    provide the same (physical) information about the structures of interest.
    Here we summarize and extent previous work of the ‘halo finder comparison
    project’: we investigate in detail the (possible) origin of any deviations
    across finders. To this extent, we decipher and discuss differences in
    halo-finding methods, clearly separating them from the disparity in
    definitions of halo properties. We observe that different codes not only
    find different numbers of objects leading to a scatter of up to 20 per cent
    in the halo mass and Vmax function, but also that the particulars of those
    objects that are identified by all finders differ. The strength of the
    variation, however, depends on the property studied, e.g. the scatter in
    position, bulk velocity, mass and the peak value of the rotation curve is
    practically below a few per cent, whereas derived quantities such as spin
    and shape show larger deviations. Our study indicates that the prime
    contribution to differences in halo properties across codes stems from the
    distinct particle collection methods and – to a minor extent – the
    particular aspects of how the procedure for removing unbound particles is
    implemented. We close with a discussion of the relevance and implications of
    the scatter across different codes for other fields such as semi-analytical
    galaxy formation models, gravitational lensing and observables in general.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Knebe
      given: Alexander
    - family: Pearce
      given: Frazer R.
    - family: Lux
      given: Hanni
    - family: Ascasibar
      given: Yago
    - family: Behroozi
      given: Peter
    - family: Casado
      given: Javier
    - family: Moran
      given: Christine Corbett
    - family: Diemand
      given: Juerg
    - family: Dolag
      given: Klaus
    - family: Dominguez-Tenreiro
      given: Rosa
    - family: Elahi
      given: Pascal
    - family: Falck
      given: Bridget
    - family: Gottlöber
      given: Stefan
    - family: Han
      given: Jiaxin
    - family: Klypin
      given: Anatoly
    - family: Lukić
      given: Zarija
    - family: Maciejewski
      given: Michal
    - family: McBride
      given: Cameron K.
    - family: Merchán
      given: Manuel E.
    - family: Muldrew
      given: Stuart I.
    - family: Neyrinck
      given: Mark
    - family: Onions
      given: Julian
    - family: Planelles
      given: Susana
    - family: Potter
      given: Doug
    - family: Quilis
      given: Vicent
    - family: Rasera
      given: Yann
    - family: Ricker
      given: Paul M.
    - family: Roy
      given: Fabrice
    - family: Ruiz
      given: Andrés N.
    - family: Sgró
      given: Mario A.
    - family: Springel
      given: Volker
    - family: Stadel
      given: Joachim
    - family: Sutter
      given: P. M.
    - family: Tweed
      given: Dylan
    - family: Zemp
      given: Marcel
  citation-key: knebeStructureFindingCosmological2013
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/stt1403
  ISSN: 1365-2966, 0035-8711
  issue: '2'
  issued:
    - year: 2013
      month: 10
      day: 21
  language: en
  page: 1618-1658
  source: DOI.org (Crossref)
  title: 'Structure finding in cosmological simulations: the state of affairs'
  title-short: Structure finding in cosmological simulations
  type: article-journal
  URL: >-
    http://academic.oup.com/mnras/article/435/2/1618/1043579/Structure-finding-in-cosmological-simulations-the
  volume: '435'

- id: knuthLiterateProgramming1984
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Knuth
      given: D. E.
  citation-key: knuthLiterateProgramming1984
  container-title: The Computer Journal
  container-title-short: The Computer Journal
  DOI: 10.1093/comjnl/27.2.97
  ISSN: 0010-4620, 1460-2067
  issue: '2'
  issued:
    - year: 1984
      month: 2
      day: 1
  language: en
  page: 97-111
  source: DOI.org (Crossref)
  title: Literate Programming
  type: article-journal
  URL: https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/27.2.97
  volume: '27'

- id: koeblerTemplateEngines2013
  abstract: 'Template-Engines: Concepts, thoughts, comparisons and benchmarks.'
  accessed:
    - year: 2022
      month: 7
      day: 11
  author:
    - family: Koebler
      given: Roland
  citation-key: koeblerTemplateEngines2013
  container-title: simple is better
  issued:
    - year: 2013
      month: 4
      day: 3
  title: Template Engines
  type: webpage
  URL: https://www.simple-is-better.org/template/

- id: kohlerImprovingWorkflowFault2011
  abstract: >-
    Scientific workflow systems frequently are used to execute a variety of
    long-running computational pipelines prone to premature termination due to
    network failures, server outages, and other faults. Researchers have
    presented approaches for providing fault tolerance for portions of specific
    workflows, but no solution handles faults that terminate the workflow engine
    itself when executing a mix of stateless and stateful workflow components.
    Here we present a general framework for efficiently resuming workflow
    execution using information commonly captured by workflow systems to record
    data provenance. Our approach facilitates fast workflow replay using only
    such commonly recorded provenance data. We also propose a checkpoint
    extension to standard provenance models to significantly reduce the
    computation needed to reset the workflow to a consistent state, thus
    resulting in much shorter re-execution times. Our work generalizes the
    rescue-DAG approach used by DAGMan to richer workflow models that may
    contain stateless and stateful multi-invocation actors as well as workflow
    loops.
  author:
    - family: Köhler
      given: Sven
    - family: Riddle
      given: Sean
    - family: Zinn
      given: Daniel
    - family: McPhillips
      given: Timothy
    - family: Ludäscher
      given: Bertram
  citation-key: kohlerImprovingWorkflowFault2011
  collection-title: Lecture Notes in Computer Science
  container-title: Scientific and Statistical Database Management
  DOI: 10.1007/978-3-642-22351-8_12
  editor:
    - family: Bayard Cushing
      given: Judith
    - family: French
      given: James
    - family: Bowers
      given: Shawn
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-22351-8
  issued:
    - year: 2011
  language: en
  note: 'interest: 91'
  page: 207-224
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: Improving Workflow Fault Tolerance through Provenance-Based Recovery
  type: paper-conference

- id: konvesWhatIfWe2019
  abstract: Reproducible steps for identifying unwanted and malicious code
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Konves
      given: Steve
  citation-key: konvesWhatIfWe2019
  container-title: HackerNoon.com
  issued:
    - year: 2019
      month: 2
      day: 14
  language: en
  note: 'interest: 50'
  title: What if we could verify npm packages?
  type: post-weblog
  URL: >-
    https://medium.com/hackernoon/what-if-we-could-verify-npm-packages-c2a319cff758

- id: koopProvenanceBasedInfrastructureSupport2011
  abstract: >-
    As publishers establish a greater online presence as well as infrastructure
    to support the distribution of more varied information, the idea of an
    executable paper that enables greater interaction has developed. An
    executable paper provides more information for computational experiments and
    results than the text, tables, and figures of standard papers. Executable
    papers can bundle computational content that allow readers and reviewers to
    interact, validate, and explore experiments. By including such content,
    authors facilitate future discoveries by lowering the barrier to reproducing
    and extending results. We present an infrastructure for creating,
    disseminating, and maintaining executable papers. Our approach is rooted in
    provenance, the documentation of exactly how data, experiments, and results
    were generated. We seek to improve the experience for everyone involved in
    the life cycle of an executable paper. The automated capture of provenance
    information allows authors to easily integrate and update results into
    papers as they write, and also helps reviewers better evaluate approaches by
    enabling them to explore experimental results by varying parameters or data.
    With a provenance-based system, readers are able to examine exactly how a
    result was developed to better understand and extend published findings.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Koop
      given: David
    - family: Santos
      given: Emanuele
    - family: Mates
      given: Phillip
    - family: Vo
      given: Huy T.
    - family: Bonnet
      given: Philippe
    - family: Bauer
      given: Bela
    - family: Surer
      given: Brigitte
    - family: Troyer
      given: Matthias
    - family: Williams
      given: Dean N.
    - family: Tohline
      given: Joel E.
    - family: Freire
      given: Juliana
    - family: Silva
      given: Cláudio T.
  citation-key: koopProvenanceBasedInfrastructureSupport2011
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2011.04.068
  ISSN: '18770509'
  issued:
    - year: 2011
  language: en
  note: 'interest: 79'
  page: 648-657
  source: DOI.org (Crossref)
  title: >-
    A Provenance-Based Infrastructure to Support the Life Cycle of Executable
    Papers
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1877050911001268
  volume: '4'

- id: korchaginSpeedingLinuxDisk2020
  abstract: >-
    In this post, we will investigate the performance of disk encryption on
    Linux and explain how we made it at least two times faster for ourselves and
    our customers!
  accessed:
    - year: 2023
      month: 12
      day: 15
  author:
    - family: Korchagin
      given: Ignat
  citation-key: korchaginSpeedingLinuxDisk2020
  container-title: The Cloudflare Blog
  issued:
    - year: 2020
      month: 3
      day: 25
  language: en
  title: Speeding up Linux disk encryption
  type: post-weblog
  URL: https://blog.cloudflare.com/speeding-up-linux-disk-encryption

- id: kosterSnakemakeScalableBioinformatics2012
  abstract: "Summary: Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.Availability: \_http://snakemake.googlecode.com.Contact: \_johannes.koester@uni-due.de"
  accessed:
    - year: 2023
      month: 1
      day: 29
  author:
    - family: Köster
      given: Johannes
    - family: Rahmann
      given: Sven
  citation-key: kosterSnakemakeScalableBioinformatics2012
  container-title: Bioinformatics
  container-title-short: Bioinformatics
  DOI: 10.1093/bioinformatics/bts480
  ISSN: 1367-4803
  issue: '19'
  issued:
    - year: 2012
      month: 10
      day: 1
  page: 2520-2522
  source: Silverchair
  title: Snakemake—a scalable bioinformatics workflow engine
  type: article-journal
  URL: https://doi.org/10.1093/bioinformatics/bts480
  volume: '28'

- id: krafczykLearningReproducingComputational2021
  abstract: >-
    We carry out efforts to reproduce computational results for seven published
    articles and identify barriers to computational reproducibility. We then
    derive three principles to guide the practice and dissemination of
    reproducible computational research: (i) Provide transparency regarding how
    computational results are produced; (ii) When writing and releasing research
    software, aim for ease of (re-)executability; (iii) Make any code upon which
    the results rely as deterministic as possible. We then exemplify these three
    principles with 12 specific guidelines for their implementation in practice.
    We illustrate the three principles of reproducible research with a series of
    vignettes from our experimental reproducibility work. We define a novel
    Reproduction Package, a formalism that specifies a structured way to share
    computational research artifacts that implements the guidelines generated
    from our reproduction efforts to allow others to build, reproduce and extend
    computational science. We make our reproduction efforts in this paper
    publicly available as exemplar Reproduction Packages.


    This article is part of the theme issue ‘Reliability and reproducibility in
    computational science: implementing verification, validation and uncertainty
    quantification in silico’.
  accessed:
    - year: 2023
      month: 1
      day: 31
  author:
    - family: Krafczyk
      given: M. S.
    - family: Shi
      given: A.
    - family: Bhaskar
      given: A.
    - family: Marinov
      given: D.
    - family: Stodden
      given: V.
  citation-key: krafczykLearningReproducingComputational2021
  container-title: >-
    Philosophical Transactions of the Royal Society A: Mathematical, Physical
    and Engineering Sciences
  DOI: 10.1098/rsta.2020.0069
  issue: '2197'
  issued:
    - year: 2021
      month: 3
      day: 29
  page: '20200069'
  publisher: Royal Society
  source: royalsocietypublishing.org (Atypon)
  title: >-
    Learning from reproducing computational results: introducing three
    principles and the Reproduction Package
  title-short: Learning from reproducing computational results
  type: article-journal
  URL: https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069
  volume: '379'

- id: krafczykScientificTestsContinuous2019
  abstract: >-
    Continuous integration (CI) is a well-established technique in commercial
    and open-source software projects, although not routinely used in scientific
    publishing. In the scientific software context, CI can serve two functions
    to increase reproducibility of scientific results: providing an established
    platform for testing the reproducibility of these results, and demonstrating
    to other scientists how the code and data generate the published results. We
    explore scientific software testing and CI strategies using two articles
    published in the areas of applied mathematics and computational physics. We
    discuss lessons learned from reproducing these articles as well as examine
    and discuss existing tests. We introduce the notion of a "scientific test"
    as one that produces computational results from a published article. We then
    consider full result reproduction within a CI environment. If authors find
    their work too time or resource intensive to easily adapt to a CI context,
    we recommend the inclusion of results from reduced versions of their work
    (e.g., run at lower resolution, with shorter time scales, with smaller data
    sets) alongside their primary results within their article. While these
    smaller versions may be less interesting scientifically, they can serve to
    verify that published code and data are working properly. We demonstrate
    such reduction tests on the two articles studied.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Krafczyk
      given: Matthew
    - family: Shi
      given: August
    - family: Bhaskar
      given: Adhithya
    - family: Marinov
      given: Darko
    - family: Stodden
      given: Victoria
  citation-key: krafczykScientificTestsContinuous2019
  collection-title: P-RECS '19
  container-title: >-
    Proceedings of the 2nd International Workshop on Practical Reproducible
    Evaluation of Computer Systems
  DOI: 10.1145/3322790.3330595
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6756-1
  issued:
    - year: 2019
      month: 6
      day: 17
  note: 'interest: 98'
  page: 23–28
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Scientific Tests and Continuous Integration Strategies to Enhance
    Reproducibility in the Scientific Software Context
  type: paper-conference
  URL: https://doi.org/10.1145/3322790.3330595

- id: krawczykHowOpenAccess2021
  abstract: >-
    The aim of this paper is to investigate how predatory journals are
    characterized by authors who write about such journals. We emphasize the
    ways in which predatory journals have been conflated with—or distinguished
    from—open access journals. We created a list of relevant publications on
    predatory publishing using four databases: Web of Science, Scopus,
    Dimensions, and Microsoft Academic. We included 280 English-language
    publications in the review according to their contributions to the
    discussions on predatory publishing. Then, we coded and qualitatively
    analyzed these publications. The findings show the profound influence of
    Jeffrey Beall, who composed and maintained himself lists of predatory
    publishers and journals, on the whole discussion on predatory publishing.
    The major themes by which Beall has characterized predatory journals are
    widely present in non-Beall publications. Moreover, 122 papers we reviewed
    combined predatory publishing with open access using similar strategies as
    Beall. The overgeneralization of the flaws of some open access journals to
    the entire open access movement has led to unjustified prejudices among the
    academic community toward open access. This is the first large-scale study
    that systematically examines how predatory publishing is defined in the
    literature.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Krawczyk
      given: Franciszek
    - family: Kulczycki
      given: Emanuel
  citation-key: krawczykHowOpenAccess2021
  container-title: The Journal of Academic Librarianship
  container-title-short: The Journal of Academic Librarianship
  DOI: 10.1016/j.acalib.2020.102271
  ISSN: 0099-1333
  issue: '2'
  issued:
    - year: 2021
      month: 3
      day: 1
  language: en
  page: '102271'
  source: ScienceDirect
  title: >-
    How is open access accused of being predatory? The impact of Beall's lists
    of predatory journals on academic publishing
  title-short: How is open access accused of being predatory?
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0099133320301622
  volume: '47'

- id: krewinkelFormattingOpenScience2017
  abstract: >-
    The timely publication of scientific results is essential for dynamic
    advances in science. The ubiquitous availability of computers which are
    connected to a global network made the rapid and low-cost distribution of
    information through electronic channels possible. New concepts, such as Open
    Access publishing and preprint servers are currently changing the
    traditional print media business towards a community-driven peer production.
    However, the cost of scientific literature generation, which is either
    charged to readers, authors or sponsors, is still high. The main active
    participants in the authoring and evaluation of scientific manuscripts are
    volunteers, and the cost for online publishing infrastructure is close to
    negligible. A major time and cost factor is the formatting of manuscripts in
    the production stage. In this article we demonstrate the feasibility of
    writing scientific manuscripts in plain markdown (MD) text files, which can
    be easily converted into common publication formats, such as PDF, HTML or
    EPUB, using Pandoc. The simple syntax of Markdown assures the long-term
    readability of raw files and the development of software and workflows. We
    show the implementation of typical elements of scientific
    manuscripts—formulas, tables, code blocks and citations—and present tools
    for editing, collaborative writing and version control. We give an example
    on how to prepare a manuscript with distinct output formats, a DOCX file for
    submission to a journal, and a LATEX/PDF version for deposition as a PeerJ
    preprint. Further, we implemented new features for supporting ‘semantic web’
    applications, such as the ‘journal article tag suite’—JATS, and the
    ‘citation typing ontology’—CiTO standard. Reducing the work spent on
    manuscript formatting translates directly to time and cost savings for
    writers, publishers, readers and sponsors. Therefore, the adoption of the MD
    format contributes to the agile production of open science literature.
    Pandoc Scholar is freely available from 
                  https://github.com/pandoc-scholar 
                  .
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Krewinkel
      given: Albert
    - family: Winkler
      given: Robert
  citation-key: krewinkelFormattingOpenScience2017
  container-title: PeerJ Computer Science
  DOI: 10.7717/peerj-cs.112
  editor:
    - family: Ventura
      given: Sebastian
  ISSN: 2376-5992
  issued:
    - year: 2017
      month: 5
      day: 8
  language: en
  page: e112
  source: DOI.org (Crossref)
  title: >-
    Formatting Open Science: agilely creating multiple document formats for
    academic manuscripts with Pandoc Scholar
  title-short: Formatting Open Science
  type: article-journal
  URL: https://peerj.com/articles/cs-112
  volume: '3'

- id: krishnamurthiRealSoftwareCrisis2015
  abstract: >-
    Sharing experiences running artifact evaluation committees for five major
    conferences.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Krishnamurthi
      given: Shriram
    - family: Vitek
      given: Jan
  citation-key: krishnamurthiRealSoftwareCrisis2015
  container-title: Communications of the ACM
  container-title-short: Commun. ACM
  DOI: 10.1145/2658987
  ISSN: 0001-0782, 1557-7317
  issue: '3'
  issued:
    - year: 2015
      month: 2
      day: 23
  language: en
  note: 'interest: 74'
  page: 34-36
  source: DOI.org (Crossref)
  title: 'The real software crisis: repeatability as a core value'
  title-short: The real software crisis
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2658987
  volume: '58'

- id: krishnaswamiSemanticDomainGolden2022
  accessed:
    - year: 2022
      month: 11
      day: 14
  author:
    - family: Krishnaswami
      given: Neel
  citation-key: krishnaswamiSemanticDomainGolden2022
  container-title: Semantic Domain
  issued:
    - year: 2022
      month: 9
      day: 13
  note: 'interest: 85'
  title: 'Semantic Domain: The Golden Age of PL Research'
  title-short: Semantic Domain
  type: post-weblog
  URL: >-
    https://semantic-domain.blogspot.com/2022/09/the-golden-age-of-pl-research.html

- id: krollWeHaveTalk
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Kroll
      given: Rachel
  citation-key: krollWeHaveTalk
  note: 'interest: 91'
  title: We have to talk about this Python, Gunicorn, Gevent thing
  type: post-weblog
  URL: https://rachelbythebay.com/w/2020/03/07/costly/

- id: kruchtenPresentFutureSoftware2006
  abstract: >-
    It's been 10 years since David Garlan and Mary Shaw wrote their seminal book
    Software Architecture Perspective on an Emerging Discipline, since Maarten
    Boasson edited a special issue of IEEE Software on software architecture,
    and since the first International Software Architecture Workshop took place.
    What has happened over these 10 years? What have we learned? Where do we
    look for information? What's the community around this discipline? And where
    are we going from here?This article is part of a focus section on software
    architecture.
  author:
    - family: Kruchten
      given: P.
    - family: Obbink
      given: H.
    - family: Stafford
      given: J.
  citation-key: kruchtenPresentFutureSoftware2006
  container-title: IEEE Software
  DOI: 10.1109/MS.2006.59
  ISSN: 1937-4194
  issue: '2'
  issued:
    - year: 2006
      month: 3
  note: 'interest: 78'
  page: 22-30
  source: IEEE Xplore
  title: The Past, Present, and Future for Software Architecture
  type: article-journal
  volume: '23'

- id: kuhnJULEAFlexibleStorage2017
  abstract: >-
    JULEA is a flexible storage framework that allows offering arbitrary client
    interfaces to applications. To be able to rapidly prototype new approaches,
    it offers data and metadata backends that can either be client-side or
    server-side; backends for popular storage technologies such as POSIX,
    LevelDB and MongoDB have already been implemented. Additionally, JULEA
    allows dynamically adapting the I/O operations’ semantics and can thus be
    adjusted to different use-cases. It runs completely in user space, which
    eases development and debugging. Its goal is to provide a solid foundation
    for storage research and teaching.
  author:
    - family: Kuhn
      given: Michael
  citation-key: kuhnJULEAFlexibleStorage2017
  collection-title: Lecture Notes in Computer Science
  container-title: High Performance Computing
  DOI: 10.1007/978-3-319-67630-2_51
  editor:
    - family: Kunkel
      given: Julian M.
    - family: Yokota
      given: Rio
    - family: Taufer
      given: Michela
    - family: Shalf
      given: John
  event-place: Cham
  ISBN: 978-3-319-67630-2
  issued:
    - year: 2017
  language: en
  page: 712-723
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: 'JULEA: A Flexible Storage Framework for HPC'
  title-short: JULEA
  type: paper-conference

- id: kungClassFirewallTest1995
  author:
    - family: Kung
      given: David Chenho
    - family: Gao
      given: Jerry
    - family: Hsia
      given: Pei
    - family: Lin
      given: Jeremy
    - family: Toyoshima
      given: Yasufumi
  citation-key: kungClassFirewallTest1995
  container-title: JOOP
  issued:
    - year: 1995
  page: 51-65
  title: >-
    Class firewall, test order, and regression testing of object-oriented
    programs
  type: article-journal
  volume: '8.2'

- id: kurtzerSingularityScientificContainers2017
  abstract: >-
    Here we present Singularity, software developed to bring containers and
    reproducibility to scientific computing. Using Singularity containers,
    developers can work in reproducible environments of their choosing and
    design, and these complete environments can easily be copied and executed on
    other platforms. Singularity is an open source initiative that harnesses the
    expertise of system and software engineers and researchers alike, and
    integrates seamlessly into common workflows for both of these groups. As its
    primary use case, Singularity brings mobility of computing to both users and
    HPC centers, providing a secure means to capture and distribute software and
    compute environments. This ability to create and deploy reproducible
    environments across these centers, a previously unmet need, makes
    Singularity a game changing development for computational science.
  accessed:
    - year: 2023
      month: 1
      day: 29
  author:
    - family: Kurtzer
      given: Gregory M.
    - family: Sochat
      given: Vanessa
    - family: Bauer
      given: Michael W.
  citation-key: kurtzerSingularityScientificContainers2017
  container-title: PLOS ONE
  container-title-short: PLOS ONE
  DOI: 10.1371/journal.pone.0177459
  ISSN: 1932-6203
  issue: '5'
  issued:
    - year: 2017
      month: 5
      day: 11
  language: en
  page: e0177459
  publisher: Public Library of Science
  source: PLoS Journals
  title: 'Singularity: Scientific containers for mobility of compute'
  title-short: Singularity
  type: article-journal
  URL: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177459
  volume: '12'

- id: kwonLDXCausalityInference2016
  abstract: >-
    Causality inference, such as dynamic taint anslysis, has many applications
    (e.g., information leak detection). It determines whether an event e is
    causally dependent on a preceding event c during execution. We develop a new
    causality inference engine LDX. Given an execution, it spawns a slave
    execution, in which it mutates c and observes whether any change is induced
    at e. To preclude non-determinism, LDX couples the executions by sharing
    syscall outcomes. To handle path differences induced by the perturbation, we
    develop a novel on-the-fly execution alignment scheme that maintains a
    counter to reflect the progress of execution. The scheme relies on program
    analysis and compiler transformation. LDX can effectively detect information
    leak and security attacks with an average overhead of 6.08% while running
    the master and the slave concurrently on separate CPUs, much lower than
    existing systems that require instruction level monitoring. Furthermore, it
    has much better accuracy in causality inference.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Kwon
      given: Yonghwi
    - family: Kim
      given: Dohyeong
    - family: Sumner
      given: William Nick
    - family: Kim
      given: Kyungtae
    - family: Saltaformaggio
      given: Brendan
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: kwonLDXCausalityInference2016
  collection-title: ASPLOS '16
  container-title: >-
    Proceedings of the Twenty-First International Conference on Architectural
    Support for Programming Languages and Operating Systems
  DOI: 10.1145/2872362.2872395
  event-place: New York, NY, USA
  ISBN: 978-1-4503-4091-5
  issued:
    - year: 2016
      month: 3
      day: 25
  page: 503–515
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'LDX: Causality Inference by Lightweight Dual Execution'
  title-short: LDX
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2872362.2872395

- id: kwonMCIModelingbasedCausality2018
  abstract: >-
    In this paper, we develop a model based causality inference technique for
    audit logging that does not require any application instrumentation or
    kernel modiﬁcation. It leverages a recent dynamic analysis, dual execution
    (LDX), that can infer precise causality between system calls but
    unfortunately requires doubling the resource consumption such as CPU time
    and memory consumption. For each application, we use LDX to acquire precise
    causal models for a set of primitive operations. Each model is a sequence of
    system calls that have inter-dependences, some of them caused by memory
    operations and hence implicit at the system call level. These models are
    described by a language that supports various complexity such as regular,
    context-free, and even context-sensitive. In production run, a novel parser
    is deployed to parse audit logs (without any enhancement) to model instances
    and hence derive causality. Our evaluation on a set of real-world programs
    shows that the technique is highly effective. The generated models can
    recover causality with 0% false-positives (FP) and false-negatives (FN) for
    most programs and only 8.3% FP and 5.2% FN in the worst cases. The models
    also feature excellent composibility, meaning that the models derived from
    primitive operations can be composed together to describe causality for
    large and complex real world missions. Applying our technique to attack
    investigation shows that the system-wide attack causal graphs are highly
    precise and concise, having better quality than the state-of-the-art.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Kwon
      given: Yonghwi
    - family: Wang
      given: Fei
    - family: Wang
      given: Weihang
    - family: Lee
      given: Kyu Hyung
    - family: Lee
      given: Wen-Chuan
    - family: Ma
      given: Shiqing
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
    - family: Jha
      given: Somesh
    - family: Ciocarlie
      given: Gabriela
    - family: Gehani
      given: Ashish
    - family: Yegneswaran
      given: Vinod
  citation-key: kwonMCIModelingbasedCausality2018
  container-title: Proceedings 2018 Network and Distributed System Security Symposium
  DOI: 10.14722/ndss.2018.23306
  event-place: San Diego, CA
  event-title: Network and Distributed System Security Symposium
  ISBN: 978-1-891562-49-5
  issued:
    - year: 2018
  language: en
  publisher: Internet Society
  publisher-place: San Diego, CA
  source: DOI.org (Crossref)
  title: >-
    MCI : Modeling-based Causality Inference in Audit Logging for Attack
    Investigation
  title-short: MCI
  type: paper-conference
  URL: >-
    https://www.ndss-symposium.org/wp-content/uploads/2018/02/ndss2018_07B-2_Kwon_paper.pdf

- id: lakensEquivalenceTests2017
  abstract: >-
    Scientists should be able to provide support for the absence of a meaningful
    effect. Currently, researchers often incorrectly conclude an effect is
    absent based a nonsignificant result. A widely recommended approach within a
    frequentist framework is to test for equivalence. In equivalence tests, such
    as the two one-sided tests (TOST) procedure discussed in this article, an
    upper and lower equivalence bound is specified based on the smallest effect
    size of interest. The TOST procedure can be used to statistically reject the
    presence of effects large enough to be considered worthwhile. This practical
    primer with accompanying spreadsheet and R package enables psychologists to
    easily perform equivalence tests (and power analyses) by setting equivalence
    bounds based on standardized effect sizes and provides recommendations to
    prespecify equivalence bounds. Extending your statistical tool kit with
    equivalence tests is an easy way to improve your statistical and theoretical
    inferences.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Lakens
      given: Daniël
  citation-key: lakensEquivalenceTests2017
  container-title: Social Psychological and Personality Science
  container-title-short: Soc Psychol Personal Sci
  DOI: 10.1177/1948550617697177
  ISSN: 1948-5506
  issue: '4'
  issued:
    - year: 2017
      month: 5
  page: 355-362
  PMCID: PMC5502906
  PMID: '28736600'
  source: PubMed Central
  title: Equivalence Tests
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5502906/
  volume: '8'

- id: lamIDFlakiesFrameworkDetecting2019
  abstract: >-
    Regression testing is increasingly important with the wide use of continuous
    integration. A desirable requirement for regression testing is that a test
    failure reliably indicates a problem in the code under test and not a false
    alarm from the test code or the testing infrastructure. However, some test
    failures are unreliable, stemming from flaky tests that can
    nondeterministically pass or fail for the same code under test. There are
    many types of flaky tests, with order-dependent tests being a prominent
    type. To help advance research on flaky tests, we present (1) a framework,
    iDFlakies, to detect and partially classify flaky tests; (2) a dataset of
    flaky tests in open-source projects; and (3) a study with our dataset.
    iDFlakies automates experimentation with our tool for Maven-based Java
    projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5%
    order-dependent and 49.5% not. Our study of these flaky tests finds the
    prevalence of two types of flaky tests, probability of a test-suite run to
    have at least one failure due to flaky tests, and how different test
    reorderings affect the number of detected flaky tests. We envision that our
    work can spur research to alleviate the problem of flaky tests.
  author:
    - family: Lam
      given: Wing
    - family: Oei
      given: Reed
    - family: Shi
      given: August
    - family: Marinov
      given: Darko
    - family: Xie
      given: Tao
  citation-key: lamIDFlakiesFrameworkDetecting2019
  container-title: >-
    2019 12th IEEE Conference on Software Testing, Validation and Verification
    (ICST)
  DOI: 10.1109/ICST.2019.00038
  event-title: >-
    2019 12th IEEE Conference on Software Testing, Validation and Verification
    (ICST)
  ISSN: 2159-4848
  issued:
    - year: 2019
      month: 4
  page: 312-322
  source: IEEE Xplore
  title: 'iDFlakies: A Framework for Detecting and Partially Classifying Flaky Tests'
  title-short: iDFlakies
  type: paper-conference

- id: larsonTooManyPhD2014
  abstract: >-
    The academic job market has become increasingly competitive for PhD
    graduates. In this note, we ask the basic question of ‘Are we producing more
    PhDs than needed?’ We take a systems approach and offer a ‘birth rate’
    perspective: professors graduate PhDs who later become professors
    themselves, an analogue to how a population grows. We show that the
    reproduction rate in academia is very high. For example, in engineering, a
    professor in the US graduates 7.8 new PhDs during his/her whole career on
    average, and only one of these graduates can replace the professor’s
    position. This implies that in a steady state, only 12.8% of PhD graduates
    can attain academic positions in the USA. The key insight is that the system
    in many places is saturated, far beyond capacity to absorb new PhDs in
    academia at the rates that they are being produced. Based on the analysis,
    we discuss policy implications.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Larson
      given: Richard C.
    - family: Ghaffarzadegan
      given: Navid
    - family: Xue
      given: Yi
  citation-key: larsonTooManyPhD2014
  container-title: Systems research and behavioral science
  container-title-short: Syst Res Behav Sci
  DOI: 10.1002/sres.2210
  ISSN: 1092-7026
  issue: '6'
  issued:
    - year: 2014
  page: 745-750
  PMCID: PMC4309283
  PMID: '25642132'
  source: PubMed Central
  title: >-
    Too Many PhD Graduates or Too Few Academic Job Openings: The Basic
    Reproductive Number R0 in Academia
  title-short: Too Many PhD Graduates or Too Few Academic Job Openings
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4309283/
  volume: '31'

- id: lattnerLLVMCompilationFramework2004
  abstract: >-
    We describe LLVM (low level virtual machine), a compiler framework designed
    to support transparent, lifelong program analysis and transformation for
    arbitrary programs, by providing high-level information to compiler
    transformations at compile-time, link-time, run-time, and in idle time
    between runs. LLVM defines a common, low-level code representation in static
    single assignment (SSA) form, with several novel features: a simple,
    language-independent type-system that exposes the primitives commonly used
    to implement high-level language features; an instruction for typed address
    arithmetic; and a simple mechanism that can be used to implement the
    exception handling features of high-level languages (and setjmp/longjmp in
    C) uniformly and efficiently. The LLVM compiler framework and code
    representation together provide a combination of key capabilities that are
    important for practical, lifelong analysis and transformation of programs.
    To our knowledge, no existing compilation approach provides all these
    capabilities. We describe the design of the LLVM representation and compiler
    framework, and evaluate the design in three ways: (a) the size and
    effectiveness of the representation, including the type information it
    provides; (b) compiler performance for several interprocedural problems; and
    (c) illustrative examples of the benefits LLVM provides for several
    challenging compiler problems.
  author:
    - family: Lattner
      given: C.
    - family: Adve
      given: V.
  citation-key: lattnerLLVMCompilationFramework2004
  container-title: International Symposium on Code Generation and Optimization, 2004. CGO 2004.
  DOI: 10.1109/CGO.2004.1281665
  event-title: International Symposium on Code Generation and Optimization, 2004. CGO 2004.
  issued:
    - year: 2004
      month: 3
  page: 75-86
  source: IEEE Xplore
  title: 'LLVM: a compilation framework for lifelong program analysis & transformation'
  title-short: LLVM
  type: paper-conference

- id: lattnerWhatEveryProgrammer2011
  abstract: >-
    In Part 1 of our series, we discussed what undefined behavior is, and how it
    allows C and C++ compilers to produce higher performance applications than
    "safe" languages. This post talks about how "unsafe" C really is, explaining
    some of the highly surprising effects that undefined behavior can cause. In
    Part #3, we talk about what friendly compilers can do to mitigate some of
    the surprise, even if they aren't required to.
  accessed:
    - year: 2023
      month: 3
      day: 9
  author:
    - family: Lattner
      given: Chris
  citation-key: lattnerWhatEveryProgrammer2011
  container-title: The LLVM Project Blog
  issued:
    - year: 2011
      month: 5
      day: 14
  language: en
  section: posts
  title: 'What Every C Programmer Should Know About Undefined Behavior #2/3'
  type: post-weblog
  URL: https://blog.llvm.org/2011/05/what-every-c-programmer-should-know_14.html

- id: leeAdaptiveWorkflowProcessing2008
  abstract: >-
    Workflows are widely used in applications that require coordinated use of
    computational resources. Workflow definition languages typically abstract
    over some aspects of the way in which a workflow is to be executed, such as
    the level of parallelism to be used or the physical resources to be
    deployed. As a result, a workflow management system has responsibility for
    establishing how best to execute a workflow given the available resources.
    The Pegasus workflow management system compiles abstract workflows into
    concrete execution plans, and has been widely used in large-scale e-Science
    applications. This paper describes an extension to Pegasus whereby resource
    allocation decisions are revised during workflow evaluation, in the light of
    feedback on the performance of jobs at runtime. The contributions of this
    paper include: (i) a description of how adaptive processing has been
    retrofitted to an existing workflow management system; (ii) a scheduling
    algorithm that allocates resources based on runtime performance; and (iii)
    an experimental evaluation of the resulting infrastructure using grid
    middleware over clusters.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Lee
      given: Kevin
    - family: Paton
      given: Norman W.
    - family: Sakellariou
      given: Rizos
    - family: Deelman
      given: Ewa
    - family: Fernandes
      given: Alvaro A. A.
    - family: Mehta
      given: Gaurang
  citation-key: leeAdaptiveWorkflowProcessing2008
  collection-title: GPC-WORKSHOPS '08
  container-title: >-
    Proceedings of the 2008 The 3rd International Conference on Grid and
    Pervasive Computing - Workshops
  DOI: 10.1109/GPC.WORKSHOPS.2008.30
  event-place: USA
  ISBN: 978-0-7695-3177-9
  issued:
    - year: 2008
      month: 5
      day: 25
  note: 'interest: 64'
  page: 99–106
  publisher: IEEE Computer Society
  publisher-place: USA
  source: ACM Digital Library
  title: Adaptive Workflow Processing and Execution in Pegasus
  type: paper-conference
  URL: https://doi.org/10.1109/GPC.WORKSHOPS.2008.30

- id: leeHighAccuracyAttack2017
  abstract: >-
    An important aspect of cyber attack forensics is to understand the
    provenance of suspicious events, as it discloses the root cause and
    ramifications of cyber attacks. Traditionally, this is done by analyzing
    audit log. However, the presence of long running programs makes a live
    process receiving a large volume of inputs and produce many outputs and each
    output may be causally related to all the preceding inputs, leading to
    dependence explosion and making attack investigations almost infeasible. We
    observe that a long running execution can be partitioned into individual
    units by monitoring the execution of the program’s event-handling loops,
    with each iteration corresponding to the processing of an independent
    input/request. We reverse engineer such loops from application binaries. We
    also reverse engineer instructions that could cause workflows between units.
    Detecting such a workflow is critical to disclosing causality between units.
    We then perform selective logging for unit boundaries and unit dependences.
    Our experiments show that our technique, called BEEP, has negligible runtime
    overhead (< 1.4%) and low space overhead (12.28% on average). It is
    effective in capturing the minimal causal graph for every attack case we
    have studied, without any dependence explosion.
  author:
    - family: Lee
      given: Kyu Hyung
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: leeHighAccuracyAttack2017
  container-title: >-
    Proceedings of the 2017 Network and Distributed System Security (NDSS)
    Symposium
  event-title: Network and Distributed System Security (NDSS) 2017
  issued:
    - year: 2017
  language: en
  source: Zotero
  title: High Accuracy Attack Provenance via Binary-based Execution Partition
  type: paper-conference

- id: leeHowUseTwitter2019
  abstract: >-
    The social-media platform is often a tool for procrastination, says Jet-Sing
    M. Lee. But what else can it be?
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Lee
      given: Jet-Sing M.
  citation-key: leeHowUseTwitter2019
  container-title: Nature
  DOI: 10.1038/d41586-019-00535-w
  issued:
    - year: 2019
      month: 2
      day: 8
  language: en
  license: 2021 Nature
  note: |-
    Bandiera_abtest: a
    Cg_type: Career Column
    Subject_term: Communication, Lab life, Careers
    interest: 87
  publisher: Nature Publishing Group
  source: www.nature.com
  title: How to use Twitter to further your research career
  type: article-journal
  URL: https://www.nature.com/articles/d41586-019-00535-w

- id: leeLogGCGarbageCollecting2013
  abstract: >-
    System-level audit logs capture the interactions between applications and
    the runtime environment. They are highly valuable for forensic analysis that
    aims to identify the root cause of an attack, which may occur long ago, or
    to determine the ramifications of an attack for recovery from it. A key
    challenge of audit log-based forensics in practice is the sheer size of the
    log files generated, which could grow at a rate of Gigabytes per day. In
    this paper, we propose LogGC, an audit logging system with garbage
    collection (GC) capability. We identify and overcome the unique challenges
    of garbage collection in the context of computer forensic analysis, which
    makes LogGC different from traditional memory GC techniques. We also develop
    techniques that instrument user applications at a small number of selected
    places to emit additional system events so that we can substantially reduce
    the false dependences between system events to improve GC effectiveness. Our
    results show that LogGC can reduce audit log size by 14 times for regular
    user systems and 37 times for server systems, without affecting the accuracy
    of forensic analysis.
  accessed:
    - year: 2024
      month: 1
      day: 21
  author:
    - family: Lee
      given: Kyu Hyung
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: leeLogGCGarbageCollecting2013
  collection-title: CCS '13
  container-title: >-
    Proceedings of the 2013 ACM SIGSAC conference on Computer & communications
    security
  DOI: 10.1145/2508859.2516731
  event-place: New York, NY, USA
  ISBN: 978-1-4503-2477-9
  issued:
    - year: 2013
      month: 11
      day: 4
  page: 1005–1016
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'LogGC: garbage collecting audit log'
  title-short: LogGC
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2508859.2516731

- id: leeSecureProvenanceCloud2015
  abstract: >-
    Provenance information are meta-data that summarize the history of the
    creation and the actions performed on an artefact e.g. data, process etc.
    Secure provenance is essential to improve data forensics, ensure
    accountability and increase the trust in the cloud. In this paper, we survey
    the existing cloud provenance management schemes and proposed security
    solutions. We investigate the current related security challenges resulting
    from the nature of the provenance model and the characteristics of the cloud
    and we finally identify potential research directions which we feel
    necessary t should be covered in order to build a secure cloud provenance
    for the next generation.
  author:
    - family: Lee
      given: Brian
    - family: Awad
      given: Abir
    - family: Awad
      given: Mirna
  citation-key: leeSecureProvenanceCloud2015
  container-title: >-
    2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing
    (UCC)
  DOI: 10.1109/UCC.2015.102
  event-title: >-
    2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing
    (UCC)
  issued:
    - year: 2015
      month: 12
  page: 577-582
  source: IEEE Xplore
  title: 'Towards Secure Provenance in the Cloud: A Survey'
  title-short: Towards Secure Provenance in the Cloud
  type: paper-conference

- id: leeTechnologyAcceptanceModel2003
  abstract: >-
    While the technology acceptance model (TAM), introduced in 1986, continues
    to be the most widely applied theoretical model in the IS field, few
    previous efforts examined its accomplishments and limitations. This study
    traces TAM’s history, investigates its findings, and cautiously predicts its
    future trajectory. One hundred and one articles published by leading IS
    journals and conferences in the past eighteen years are examined and
    summarized. An open-ended survey of thirty-two leadi ng IS researchers
    assisted in critically examining TAM and specifying future directions.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Lee
      given: Younghwa
    - family: Kozar
      given: Kenneth A.
    - family: Larsen
      given: Kai R.T.
  citation-key: leeTechnologyAcceptanceModel2003
  container-title: Communications of the Association for Information Systems
  container-title-short: CAIS
  DOI: 10.17705/1CAIS.01250
  ISSN: '15293181'
  issued:
    - year: 2003
  language: en
  source: DOI.org (Crossref)
  title: 'The Technology Acceptance Model: Past, Present, and Future'
  title-short: The Technology Acceptance Model
  type: article-journal
  URL: https://aisel.aisnet.org/cais/vol12/iss1/50
  volume: '12'

- id: leeToxicCultureRejection
  accessed:
    - year: 2022
      month: 8
      day: 29
  author:
    - family: Lee
      given: Edward
  citation-key: leeToxicCultureRejection
  container-title: ACM SIGBED
  language: en-US
  title: The Toxic Culture of Rejection in Computer Science
  type: post-weblog
  URL: >-
    https://sigbed.org/2022/08/22/the-toxic-culture-of-rejection-in-computer-science/

- id: leisersonTherePlentyRoom2020
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Leiserson
      given: Charles E.
    - family: Thompson
      given: Neil C.
    - family: Emer
      given: Joel S.
    - family: Kuszmaul
      given: Bradley C.
    - family: Lampson
      given: Butler W.
    - family: Sanchez
      given: Daniel
    - family: Schardl
      given: Tao B.
  citation-key: leisersonTherePlentyRoom2020
  container-title: Science
  DOI: 10.1126/science.aam9744
  issue: '6495'
  issued:
    - year: 2020
      month: 6
      day: 5
  note: 'interest: 87'
  page: eaam9744
  publisher: American Association for the Advancement of Science
  source: science.org (Atypon)
  title: >-
    There’s plenty of room at the Top: What will drive computer performance
    after Moore’s law?
  title-short: There’s plenty of room at the Top
  type: article-journal
  URL: https://www.science.org/doi/full/10.1126/science.aam9744
  volume: '368'

- id: lemsonHaloGalaxyFormation2006
  abstract: >-
    The Millennium Run is the largest simulation of the formation of structure
    within the $\Lambda$CDM cosmogony so far carried out. It uses $10^{10}$
    particles to follow the dark matter distribution in a cubic region
    500$h^{-1}$Mpc on a side, and has a spatial resolution of 5 $h^{-1}$kpc.
    Application of simplified modelling techniques to the stored output of this
    calculation allows the formation and evolution of the $\sim 10^7$ galaxies
    more luminous than the Small Magellanic Cloud to be simulated for a variety
    of assumptions about the detailed physics involved. As part of the
    activities of the German Astrophysical Virtual Observatory we have used a
    relational database to store the detailed assembly histories both of all the
    haloes and subhaloes resolved by the simulation, and of all the galaxies
    that form within these structures for two independent models of the galaxy
    formation physics. We have created web applications that allow users to
    query these databases remotely using the standard Structured Query Language
    (SQL). This allows easy access to all properties of the galaxies and halos,
    as well as to the spatial and temporal relations between them and their
    environment. Information is output in table format compatible with standard
    Virtual Observatory tools and protocols. With this announcement we are
    making these structures fully accessible to all users. Interested scientists
    can learn SQL, gain familiarity with the database design and test queries on
    a small, openly accessible version of the Millennium Run (with volume 1/512
    that of the full simulation). They can then request accounts to run similar
    queries on the databases for the full simulations.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Lemson
      given: G.
    - family: Consortium
      given: the Virgo
  citation-key: lemsonHaloGalaxyFormation2006
  container-title: arXiv:astro-ph/0608019
  issued:
    - year: 2006
      month: 8
      day: 3
  note: 'interest: 70'
  source: arXiv.org
  title: >-
    Halo and Galaxy Formation Histories from the Millennium Simulation: Public
    release of a VO-oriented and SQL-queryable database for studying the
    evolution of galaxies in the LambdaCDM cosmogony
  title-short: Halo and Galaxy Formation Histories from the Millennium Simulation
  type: article-journal
  URL: http://arxiv.org/abs/astro-ph/0608019

- id: lenbergBehavioralSoftwareEngineering2015
  abstract: >-
    Throughout the history of software engineering, the human aspects have
    repeatedly been recognized as important. Even though research that
    investigates them has been growing in the past decade, these aspects should
    be more generally considered. The main objective of this study is to clarify
    the research area concerned with human aspects of software engineering and
    to create a common platform for future research. In order to meet the
    objective, we propose a definition of the research area behavioral software
    engineering (BSE) and present results from a systematic literature review
    based on the definition. The result indicates that there are knowledge gaps
    in the research area of behavioral software engineering and that earlier
    research has been focused on a few concepts, which have been applied to a
    limited number of software engineering areas. The individual studies have
    typically had a narrow perspective focusing on few concepts from a single
    unit of analysis. Further, the research has rarely been conducted in
    collaboration by researchers from both software engineering and social
    science. Altogether, this review can help put a broader set of human aspects
    higher on the agenda for future software engineering research and practice.
  accessed:
    - year: 2022
      month: 8
      day: 26
  author:
    - family: Lenberg
      given: Per
    - family: Feldt
      given: Robert
    - family: Wallgren
      given: Lars Göran
  citation-key: lenbergBehavioralSoftwareEngineering2015
  container-title: Journal of Systems and Software
  container-title-short: Journal of Systems and Software
  DOI: 10.1016/j.jss.2015.04.084
  ISSN: 0164-1212
  issued:
    - year: 2015
      month: 9
      day: 1
  language: en
  page: 15-37
  source: ScienceDirect
  title: >-
    Behavioral software engineering: A definition and systematic literature
    review
  title-short: Behavioral software engineering
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0164121215000989
  volume: '107'

- id: lerNoobTestSpack2022
  author:
    - family: Ler
      given: Jan-Patrick
  citation-key: lerNoobTestSpack2022
  event-place: Virtual event
  event-title: 7th EasyBuild User Meeting
  issued:
    - year: 2022
      month: 1
      day: 26
  publisher-place: Virtual event
  title: 'A noob test: Spack "vs" EasyBuild'
  type: speech
  URL: https://easybuild.io/eum22/009_eum22_spack_vs_easybuild.pdf

- id: leveilleWildKobalosAppears
  abstract: >-
    ESET Research has analyzed Kobalos, previously unknown and complex
    multiplatform malware targeting Linux, FreeBSD and Solaris systems. Given
    that the victims of this threat are mostly high-profile organizations, it
    seems almost certain this malware is deployed against chosen targets rather
    than opportunistically. When deployed, this malware gives access to the file
    system of the compromised host and enables access to a remote terminal,
    giving the attackers the ability to run arbitrary commands.


    The network capabilities of Kobalos make this malware quite distinctive. It
    supports acting both as a passive implant and as a bot actively connecting
    to its C&C server. Interestingly, these C&C servers are themselves
    compromised with Kobalos; the code for running such servers is present in
    all Kobalos samples.


    By performing an internet-wide scan, ESET Research was able to identify and
    notify victims of this threat.


    It is unclear how old this malware is, but the first known activity was
    confirmed by a victim who was compromised in late 2019. The group operating
    Kobalos remained active throughout 2020.


    The Linux threat landscape continues to evolve, and at times, malware
    authors invest a considerable amount of resources into their tradecraft.
    Kobalos is one of these cases.
  author:
    - family: Léveillé
      given: Marc-Etienne M.
    - family: Sanmillan
      given: Ignacio
  citation-key: leveilleWildKobalosAppears
  publisher: ESET
  title: 'A Wild Kobalos Appears: Tricksy Linux malware goes after HPCs'
  type: report
  URL: https://www.welivesecurity.com/wp-content/uploads/2021/01/ESET_Kobalos.pdf

- id: levequeReproducibleResearchScientific2012
  abstract: >-
    This article considers the obstacles involved in creating reproducible
    computational research as well as some efforts and approaches to overcome
    them.
  author:
    - family: LeVeque
      given: Randall J.
    - family: Mitchell
      given: Ian M.
    - family: Stodden
      given: Victoria
  citation-key: levequeReproducibleResearchScientific2012
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2012.38
  ISSN: 1558-366X
  issue: '4'
  issued:
    - year: 2012
      month: 7
  page: 13-17
  source: IEEE Xplore
  title: >-
    Reproducible research for scientific computing: Tools and strategies for
    changing the culture
  title-short: Reproducible research for scientific computing
  type: article-journal
  volume: '14'

- id: lewisAutocorrectErrorsExcel2021
  abstract: >-
    Despite geneticists being warned about spreadsheet problems, 30% of
    published papers contain mangled gene names in supplementary data.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Lewis
      given: Dyani
  citation-key: lewisAutocorrectErrorsExcel2021
  container-title: Nature
  DOI: 10.1038/d41586-021-02211-4
  issued:
    - year: 2021
      month: 8
      day: 13
  language: en
  license: 2021 Nature
  note: |-
    Bandiera_abtest: a
    Cg_type: News
    Subject_term: Genomics, Software, Bioinformatics, Genetics
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Autocorrect errors in Excel still creating genomics headache
  type: article-journal
  URL: https://www.nature.com/articles/d41586-021-02211-4

- id: lewisSystemUsabilityScale2018
  abstract: >-
    The System Usability Scale (SUS) is the most widely used standardized
    questionnaire for the assessment of perceived usability. This review of the
    SUS covers its early history from inception in the 1980s through recent
    research and its future prospects. From relatively inauspicious beginnings,
    when its originator described it as a “quick and dirty usability scale,” it
    has proven to be quick but not “dirty.” It is likely that the SUS will
    continue to be a popular measurement of perceived usability for the
    foreseeable future. When researchers and practitioners need a measure of
    perceived usability, they should strongly consider using the SUS.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Lewis
      given: James R.
  citation-key: lewisSystemUsabilityScale2018
  container-title: International Journal of Human–Computer Interaction
  container-title-short: International Journal of Human–Computer Interaction
  DOI: 10.1080/10447318.2018.1455307
  ISSN: 1044-7318, 1532-7590
  issue: '7'
  issued:
    - year: 2018
      month: 7
      day: 3
  language: en
  page: 577-590
  source: DOI.org (Crossref)
  title: 'The System Usability Scale: Past, Present, and Future'
  title-short: The System Usability Scale
  type: article-journal
  URL: https://www.tandfonline.com/doi/full/10.1080/10447318.2018.1455307
  volume: '34'

- id: lewsBugPredictionGoogle2011
  author:
    - family: Lews
      given: Chris
    - family: Ou
      given: Rong
  citation-key: lewsBugPredictionGoogle2011
  container-title: Google Engineering Tools
  issued:
    - year: 2011
      month: 12
      day: 14
  title: Bug Prediction at Google
  type: post-weblog
  URL: https://google-engtools.blogspot.com/2011/12/bug-prediction-at-google.html

- id: liAIassistedSuperresolutionCosmological2021
  abstract: >-
    Cosmological simulations of galaxy formation are limited by finite
    computational resources. We draw from the ongoing rapid advances in
    artificial intelligence (AI; specifically deep learning) to address this
    problem. Neural networks have been developed to learn from high-resolution
    (HR) image data and then make accurate superresolution (SR) versions of
    different low-resolution (LR) images. We apply such techniques to LR
    cosmological N-body simulations, generating SR versions. Specifically, we
    are able to enhance the simulation resolution by generating 512 times more
    particles and predicting their displacements from the initial positions.
    Therefore, our results can be viewed as simulation realizations themselves,
    rather than projections, e.g., to their density fields. Furthermore, the
    generation process is stochastic, enabling us to sample the small-scale
    modes conditioning on the large-scale environment. Our model learns from
    only 16 pairs of small-volume LR-HR simulations and is then able to generate
    SR simulations that successfully reproduce the HR matter power spectrum to
    percent level up to 16 h^−1Mpc and the HR halo mass function to within 10%
    down to 1011 M⊙. We successfully deploy the model in a box 1,000 times
    larger than the training simulation box, showing that high-resolution mock
    surveys can be generated rapidly. We conclude that AI assistance has the
    potential to revolutionize modeling of small-scale galaxy-formation physics
    in large cosmological volumes.
  accessed:
    - year: 2022
      month: 5
      day: 4
  author:
    - family: Li
      given: Yin
    - family: Ni
      given: Yueying
    - family: Croft
      given: Rupert A. C.
    - family: Di Matteo
      given: Tiziana
    - family: Bird
      given: Simeon
    - family: Feng
      given: Yu
  citation-key: liAIassistedSuperresolutionCosmological2021
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.2022038118
  issue: '19'
  issued:
    - year: 2021
      month: 5
      day: 11
  page: e2022038118
  publisher: Proceedings of the National Academy of Sciences
  source: pnas.org (Atypon)
  title: AI-assisted superresolution cosmological simulations
  type: article-journal
  URL: https://www.pnas.org/doi/10.1073/pnas.2022038118
  volume: '118'

- id: libertyRandomizedAlgorithmsLowrank2007
  abstract: >-
    We describe two recently proposed randomized algorithms for the construction
    of low-rank approximations to matrices, and demonstrate their application
    (inter alia) to the evaluation of the singular value decompositions of
    numerically low-rank matrices. Being probabilistic, the schemes described
    here have a finite probability of failure; in most cases, this probability
    is rather negligible (10−17 is a typical value). In many situations, the new
    procedures are considerably more efficient and reliable than the classical
    (deterministic) ones; they also parallelize naturally. We present several
    numerical examples to illustrate the performance of the schemes.
  accessed:
    - year: 2024
      month: 1
      day: 25
  author:
    - family: Liberty
      given: Edo
    - family: Woolfe
      given: Franco
    - family: Martinsson
      given: Per-Gunnar
    - family: Rokhlin
      given: Vladimir
    - family: Tygert
      given: Mark
  citation-key: libertyRandomizedAlgorithmsLowrank2007
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.0709640104
  issue: '51'
  issued:
    - year: 2007
      month: 12
      day: 18
  page: 20167-20172
  publisher: Proceedings of the National Academy of Sciences
  source: pnas.org (Atypon)
  title: Randomized algorithms for the low-rank approximation of matrices
  type: article-journal
  URL: https://www.pnas.org/doi/full/10.1073/pnas.0709640104
  volume: '104'

- id: liFaaSFlowEnableEfficient2022
  abstract: >-
    Serverless computing (Function-as-a-Service) provides fine-grain resource
    sharing by running functions (or Lambdas) in containers. Data-dependent
    functions are required to be invoked following a pre-defined logic, which is
    known as serverless workflows. However, our investigation shows that the
    traditional master-worker based workflow execution architecture performs
    poorly in serverless context. One significant overhead results from the
    master-side workflow schedule pattern, with which the functions are
    triggered in the master node and assigned to worker nodes for execution.
    Besides, the data movement between workers also reduces the throughput. To
    this end, we present a worker-side workflow schedule pattern for serverless
    workflow execution. Following the design, we implement FaaSFlow to enable
    efficient workflow execution in the serverless context. Besides, we propose
    an adaptive storage library FaaStore that enables fast data transfer between
    functions on the same node without through the database. Experiment results
    show that FaaSFlow effectively mitigates the workflow scheduling overhead by
    74.6% on average and data transmission overhead by 95% at most. When the
    network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput
    degradation by 23.0%, and is able to multiply the utilization of network
    bandwidth by 1.5X-4X.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Li
      given: Zijun
    - family: Liu
      given: Yushi
    - family: Guo
      given: Linsong
    - family: Chen
      given: Quan
    - family: Cheng
      given: Jiagan
    - family: Zheng
      given: Wenli
    - family: Guo
      given: Minyi
  citation-key: liFaaSFlowEnableEfficient2022
  collection-title: ASPLOS '22
  container-title: >-
    Proceedings of the 27th ACM International Conference on Architectural
    Support for Programming Languages and Operating Systems
  DOI: 10.1145/3503222.3507717
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9205-1
  issued:
    - year: 2022
      month: 2
      day: 28
  note: 'interest: 95'
  page: 782–796
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'FaaSFlow: enable efficient workflow execution for function-as-a-service'
  title-short: FaaSFlow
  type: paper-conference
  URL: https://doi.org/10.1145/3503222.3507717

- id: LifetimesCryptographicHash
  accessed:
    - year: 2023
      month: 9
      day: 7
  citation-key: LifetimesCryptographicHash
  title: Lifetimes of cryptographic hash functions
  type: webpage
  URL: https://valerieaurora.org/hash.html

- id: linuxdevelopersUser_namespacesLinuxManual2021
  accessed:
    - year: 2023
      month: 2
      day: 18
  author:
    - family: Linux Developers
      given: ''
  citation-key: linuxdevelopersUser_namespacesLinuxManual2021
  issued:
    - year: 2021
      month: 8
      day: 27
  title: user_namespaces(7) - Linux manual page
  type: webpage
  URL: https://www.man7.org/linux/man-pages/man7/user_namespaces.7.html

- id: liThreatDetectionInvestigation2021
  abstract: >-
    With the development of information technology, the border of the cyberspace
    gets much broader and thus also exposes increasingly more vulnerabilities to
    attackers. Traditional mitigation-based defence strategies are challenging
    to cope with the current complicated situation. Security practitioners
    urgently need better tools to describe and modelling attacks for defense.
    The provenance graph seems like an ideal method for threat modelling with
    powerful semantic expression ability and attacks historic correlation
    ability. In this paper, we firstly introduce the basic concepts about
    system-level provenance graph and present a typical system architecture for
    provenance graph-based threat detection and investigation. A comprehensive
    provenance graph-based threat detection system can be divided into three
    modules: data collection module, data management module, and threat
    detection modules. Each module contains several components and involves
    different research problems. We systematically taxonomize and compare the
    existing algorithms and designs involved in them. Based on these
    comparisons, we identify the strategy of technology selection for real-world
    deployment. We also provide insights and challenges about the existing work
    to guide future research in this area.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Li
      given: Zhenyuan
    - family: Chen
      given: Qi Alfred
    - family: Yang
      given: Runqing
    - family: Chen
      given: Yan
    - family: Ruan
      given: Wei
  citation-key: liThreatDetectionInvestigation2021
  container-title: Computers & Security
  container-title-short: Computers & Security
  DOI: 10.1016/j.cose.2021.102282
  ISSN: 0167-4048
  issued:
    - year: 2021
      month: 7
      day: 1
  page: '102282'
  source: ScienceDirect
  title: >-
    Threat detection and investigation with system-level provenance graphs: A
    survey
  title-short: Threat detection and investigation with system-level provenance graphs
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167404821001061
  volume: '106'

- id: liuSurveyDataIntensiveScientific2015
  abstract: >-
    Nowadays, more and more computer-based scientific experiments need to handle
    massive amounts of data. Their data processing consists of multiple
    computational steps and dependencies within them. A data-intensive
    scientific workflow is useful for modeling such process. Since the
    sequential execution of data-intensive scientific workflows may take much
    time, Scientific Workflow Management Systems (SWfMSs) should enable the
    parallel execution of data-intensive scientific workflows and exploit the
    resources distributed in different infrastructures such as grid and cloud.
    This paper provides a survey of data-intensive scientific workflow
    management in SWfMSs and their parallelization techniques. Based on a SWfMS
    functional architecture, we give a comparative analysis of the existing
    solutions. Finally, we identify research issues for improving the execution
    of data-intensive scientific workflows in a multisite cloud.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Liu
      given: Ji
    - family: Pacitti
      given: Esther
    - family: Valduriez
      given: Patrick
    - family: Mattoso
      given: Marta
  citation-key: liuSurveyDataIntensiveScientific2015
  container-title: Journal of Grid Computing
  container-title-short: J. Grid Comput.
  DOI: 10.1007/s10723-015-9329-8
  ISSN: 1570-7873
  issue: '4'
  issued:
    - year: 2015
      month: 12
      day: 1
  note: 'interest: 94'
  page: 457–493
  source: December  2015
  title: A Survey of Data-Intensive Scientific Workflow Management
  type: article-journal
  URL: https://doi.org/10.1007/s10723-015-9329-8
  volume: '13'

- id: lopezVetoBattle30
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Lopez
      given: Dan
    - family: Blanton
      given: Thomas
    - family: Fuchs
      given: Meredith
    - family: Elias
      given: Barbara
  citation-key: lopezVetoBattle30
  note: 'interest: 91'
  title: Veto Battle 30 Years Ago Set Freedom of Information Norms
  type: webpage
  URL: https://nsarchive2.gwu.edu/NSAEBB/NSAEBB142/

- id: lorenaBarbagroupReproducibilitySyllabus2016
  abstract: >-
    After my short piece, <a
    href="http://science.sciencemag.org/content/354/6308/142" target="_blank">“A
    hard road to reproducibility,”</a> appeared in <em>Science</em>, I received
    several emails and Twitter mentions asking for more specific tips — both
    about tools and documents we use in the group to train the team about
    reproducibility.
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Lorena
      given: Barbara A.
  citation-key: lorenaBarbagroupReproducibilitySyllabus2016
  container-title: HackerNoon
  issued:
    - year: 2016
      month: 10
      day: 31
  language: en
  title: Barba-group reproducibility syllabus
  type: post-weblog
  URL: https://hackernoon.com/barba-group-reproducibility-syllabus-e3757ee635cf

- id: lukPinBuildingCustomized2005
  abstract: >-
    Robust and powerful software instrumentation tools are essential for program
    analysis tasks such as profiling, performance evaluation, and bug detection.
    To meet this need, we have developed a new instrumentation system called
    Pin. Our goals are to provide easy-to-use, portable, transparent, and
    efficient instrumentation. Instrumentation tools (called Pintools) are
    written in C/C++ using Pin's rich API. Pin follows the model of ATOM,
    allowing the tool writer to analyze an application at the instruction level
    without the need for detailed knowledge of the underlying instruction set.
    The API is designed to be architecture independent whenever possible, making
    Pintools source compatible across different architectures. However, a
    Pintool can access architecture-specific details when necessary.
    Instrumentation with Pin is mostly transparent as the application and
    Pintool observe the application's original, uninstrumented behavior. Pin
    uses dynamic compilation to instrument executables while they are running.
    For efficiency, Pin uses several techniques, including inlining, register
    re-allocation, liveness analysis, and instruction scheduling to optimize
    instrumentation. This fully automated approach delivers significantly better
    instrumentation performance than similar tools. For example, Pin is 3.3x
    faster than Valgrind and 2x faster than DynamoRIO for basic-block counting.
    To illustrate Pin's versatility, we describe two Pintools in daily use to
    analyze production software. Pin is publicly available for Linux platforms
    on four architectures: IA32 (32-bit x86), EM64T (64-bit x86), Itanium®, and
    ARM. In the ten months since Pin 2 was released in July 2004, there have
    been over 3000 downloads from its website.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Luk
      given: Chi-Keung
    - family: Cohn
      given: Robert
    - family: Muth
      given: Robert
    - family: Patil
      given: Harish
    - family: Klauser
      given: Artur
    - family: Lowney
      given: Geoff
    - family: Wallace
      given: Steven
    - family: Reddi
      given: Vijay Janapa
    - family: Hazelwood
      given: Kim
  citation-key: lukPinBuildingCustomized2005
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/1064978.1065034
  ISSN: 0362-1340
  issue: '6'
  issued:
    - year: 2005
      month: 6
      day: 12
  page: 190–200
  source: ACM Digital Library
  title: 'Pin: building customized program analysis tools with dynamic instrumentation'
  title-short: Pin
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1064978.1065034
  volume: '40'

- id: luzHowScientistsCaptured
  abstract: >-
    Find out how scientists created a virtual telescope as large as Earth itself
    to capture the first image of a black hole's silhouette.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Luz
      given: Ota
  citation-key: luzHowScientistsCaptured
  container-title: NASA/JPL Edu
  note: 'interest: 75'
  title: How Scientists Captured the First Image of a Black Hole
  type: post-weblog
  URL: >-
    https://www.jpl.nasa.gov/edu/news/2019/4/19/how-scientists-captured-the-first-image-of-a-black-hole/

- id: maAccurateLowCost2015
  abstract: >-
    Audit logging is an important approach to cyber attack investigation.
    However, traditional audit logging either lacks accuracy or requires
    expensive and complex binary instrumentation. In this paper, we propose a
    Windows based audit logging technique that features accuracy and low cost.
    More importantly, it does not require instrumenting the applications, which
    is critical for commercial software with IP protection. The technique is
    build on Event Tracing for Windows (ETW). By analyzing ETW log and critical
    parts of application executables, a model can be constructed to parse ETW
    log to units representing independent sub-executions in a process. Causality
    inferred at the unit level renders much higher accuracy, allowing us to
    perform accurate attack investigation and highly effective log reduction.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Ma
      given: Shiqing
    - family: Lee
      given: Kyu Hyung
    - family: Kim
      given: Chung Hwan
    - family: Rhee
      given: Junghwan
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: maAccurateLowCost2015
  collection-title: ACSAC '15
  container-title: Proceedings of the 31st Annual Computer Security Applications Conference
  DOI: 10.1145/2818000.2818039
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3682-6
  issued:
    - year: 2015
      month: 12
      day: 7
  page: 401–410
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Accurate, Low Cost and Instrumentation-Free Security Audit Logging for
    Windows
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2818000.2818039

- id: mackoCollectingProvenanceXen2011
  abstract: |-
    The Provenance Aware Storage Systems project (PASS)
    currently collects system-level provenance by intercept-
    ing system calls in the Linux kernel and storing the
    provenance in a stackable filesystem. While this ap-
    proach is reasonably efficient, it suffers from two sig-
    nificant drawbacks: each new revision of the kernel re-
    quires reintegration of PASS changes, the stability of
    which must be continually tested; also, the use of a stack-
    able filesystem makes it difficult to collect provenance
    on root volumes, especially during early boot. In this pa-
    per we describe an approach to collecting system-level
    provenance from virtual guest machines running under
    the Xen hypervisor. We make the case that our approach
    alleviates the aforementioned difficulties and promotes
    adoption of provenance collection within cloud comput-
    ing platforms.
  author:
    - family: Macko
      given: Peter
    - family: Chiarini
      given: Marc
    - family: Seltzer
      given: Margo
  citation-key: mackoCollectingProvenanceXen2011
  container-title: Proceedings of the Theory and Practice of Provenance (TaPP)
  event-title: Theory and Practice of Provenance (TaPP)
  issued:
    - year: 2011
  publisher: USENIX
  title: Collecting Provenance via the Xen Hypervisor
  type: paper-conference
  URL: >-
    https://www.usenix.org/legacy/events/tapp11/tech/final_files/MackoChiariniSeltzer.pdf

- id: macqueenMethodsClassificationAnalysis1965
  author:
    - family: Macqueen
      given: J
  citation-key: macqueenMethodsClassificationAnalysis1965
  event-place: Los Angeles, CA
  event-title: >-
    Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and
    Probability
  issued:
    - year: 1965
      month: 6
      day: 21
  language: en
  page: '281'
  publisher: University of California Press
  publisher-place: Los Angeles, CA
  source: Zotero
  title: Some methods for classification and analysis of multivariate observatiosn
  type: paper-conference
  URL: https://www.cs.cmu.edu/~bhiksha/courses/mlsp.fall2010/class14/macqueen.pdf
  volume: '1'

- id: madabhushanaConfigureLinuxSystem2021
  abstract: >-
    Learn how to install, configure, and manage the audit daemon to track
    security-related information on your Linux systems.
  accessed:
    - year: 2024
      month: 1
      day: 21
  author:
    - family: Madabhushana
      given: Ashish Bharadwaj
  citation-key: madabhushanaConfigureLinuxSystem2021
  issued:
    - year: 2021
      month: 10
      day: 26
  language: en
  publisher: Red Hat, Inc.
  section: Enable Sysadmin
  title: Configure Linux system auditing with auditd
  type: post-weblog
  URL: https://www.redhat.com/sysadmin/configure-linux-auditing-auditd

- id: mahmoudMinotaurAdaptingSoftware2019
  abstract: >-
    With the end of conventional CMOS scaling, efficient resiliency solutions
    are needed to address the increased likelihood of hardware errors. Silent
    data corruptions (SDCs) are especially harmful because they can create
    unacceptable output without the user's knowledge. Several resiliency
    analysis techniques have been proposed to identify SDC-causing instructions,
    but they remain too slow for practical use and/or sacrifice accuracy to
    improve analysis speed. We develop Minotaur, a novel toolkit to improve the
    speed and accuracy of resiliency analysis. The key insight behind Minotaur
    is that modern resiliency analysis has many conceptual similarities to
    software testing; therefore, adapting techniques from the rich software
    testing literature can lead to principled and significant improvements in
    resiliency analysis. Minotaur identifies and adapts four concepts from
    software testing: 1) it introduces the concept of input quality criteria for
    resiliency analysis and identifies PC coverage as a simple but effective
    criterion; 2) it creates (fast) minimized inputs from (slow) standard
    benchmark inputs, using the input quality criteria to assess the goodness of
    the created input; 3) it adapts the concept of test case prioritization to
    prioritize error injections and invoke early termination for a given
    instruction to speed up error-injection campaigns; and 4) it further adapts
    test case or input prioritization to accelerate SDC discovery across
    multiple inputs. We evaluate Minotaur by applying it to Approxilyzer, a
    state-of-the-art resiliency analysis tool. Minotaur's first three techniques
    speed up Approxilyzer's resiliency analysis by 10.3X (on average) for the
    workloads studied. Moreover, they identify 96% (on average) of all
    SDC-causing instructions explored, compared to 64% identified by
    Approxilyzer alone. Minotaur's fourth technique (input prioritization)
    enables identifying all SDC-causing instructions explored across multiple
    inputs at a speed 2.3X faster (on average) than analyzing each input
    independently for our workloads.
  accessed:
    - year: 2022
      month: 4
      day: 10
  author:
    - family: Mahmoud
      given: Abdulrahman
    - family: Venkatagiri
      given: Radha
    - family: Ahmed
      given: Khalique
    - family: Misailovic
      given: Sasa
    - family: Marinov
      given: Darko
    - family: Fletcher
      given: Christopher W.
    - family: Adve
      given: Sarita V.
  citation-key: mahmoudMinotaurAdaptingSoftware2019
  collection-title: ASPLOS '19
  container-title: >-
    Proceedings of the Twenty-Fourth International Conference on Architectural
    Support for Programming Languages and Operating Systems
  DOI: 10.1145/3297858.3304050
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6240-5
  issued:
    - year: 2019
      month: 4
      day: 4
  page: 1087–1103
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Minotaur: Adapting Software Testing Techniques for Hardware Errors'
  title-short: Minotaur
  type: paper-conference
  URL: https://doi.org/10.1145/3297858.3304050

- id: mahnicUsingPlanningPoker2012
  accessed:
    - year: 2022
      month: 6
      day: 13
  author:
    - family: Mahnič
      given: Viljan
    - family: Hovelja
      given: Tomaž
  citation-key: mahnicUsingPlanningPoker2012
  container-title: Journal of Systems and Software
  container-title-short: Journal of Systems and Software
  DOI: 10.1016/j.jss.2012.04.005
  ISSN: '01641212'
  issue: '9'
  issued:
    - year: 2012
      month: 9
  language: en
  note: 'interest: 30'
  page: 2086-2095
  source: DOI.org (Crossref)
  title: On using planning poker for estimating user stories
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0164121212001021
  volume: '85'

- id: maMPIMultiplePerspective2017
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Ma
      given: Shiqing
    - family: Zhai
      given: Juan
    - family: Wang
      given: Fei
    - family: Lee
      given: Kyu Hyung
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: maMPIMultiplePerspective2017
  event-title: 26th USENIX Security Symposium (USENIX Security 17)
  ISBN: 978-1-931971-40-9
  issued:
    - year: 2017
  language: en
  page: 1111-1128
  source: www.usenix.org
  title: >-
    {MPI}: Multiple Perspective Attack Investigation with Semantic Aware
    Execution Partitioning
  title-short: '{MPI}'
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/usenixsecurity17/technical-sessions/presentation/ma

- id: mancoMyVMLighter2017
  abstract: >-
    Containers are in great demand because they are lightweight when compared to
    virtual machines. On the downside, containers offer weaker isolation than
    VMs, to the point where people run containers in virtual machines to achieve
    proper isolation. In this paper, we examine whether there is indeed a strict
    tradeoff between isolation (VMs) and efficiency (containers). We find that
    VMs can be as nimble as containers, as long as they are small and the
    toolstack is fast enough. We achieve lightweight VMs by using unikernels for
    specialized applications and with Tinyx, a tool that enables creating
    tailor-made, trimmed-down Linux virtual machines. By themselves, lightweight
    virtual machines are not enough to ensure good performance since the
    virtualization control plane (the toolstack) becomes the performance
    bottleneck. We present LightVM, a new virtualization solution based on Xen
    that is optimized to offer fast boot-times regardless of the number of
    active VMs. LightVM features a complete redesign of Xen's control plane,
    transforming its centralized operation to a distributed one where
    interactions with the hypervisor are reduced to a minimum. LightVM can boot
    a VM in 2.3ms, comparable to fork/exec on Linux (1ms), and two orders of
    magnitude faster than Docker. LightVM can pack thousands of LightVM guests
    on modest hardware with memory and CPU usage comparable to that of
    processes.
  accessed:
    - year: 2022
      month: 9
      day: 12
  author:
    - family: Manco
      given: Filipe
    - family: Lupu
      given: Costin
    - family: Schmidt
      given: Florian
    - family: Mendes
      given: Jose
    - family: Kuenzer
      given: Simon
    - family: Sati
      given: Sumit
    - family: Yasukata
      given: Kenichi
    - family: Raiciu
      given: Costin
    - family: Huici
      given: Felipe
  citation-key: mancoMyVMLighter2017
  collection-title: SOSP '17
  container-title: Proceedings of the 26th Symposium on Operating Systems Principles
  DOI: 10.1145/3132747.3132763
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5085-3
  issued:
    - year: 2017
      month: 10
      day: 14
  note: 'interest: 90'
  page: 218–233
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: My VM is Lighter (and Safer) than your Container
  type: paper-conference
  URL: https://doi.org/10.1145/3132747.3132763

- id: manianOurPointlessPursuit0000
  accessed:
    - year: 2024
      month: 2
      day: 28
  author:
    - family: Manian
      given: Divya
  citation-key: manianOurPointlessPursuit0000
  container-title: Smashing Magazine
  issued:
    - literal: 12:09:02 +0000 UTC
  language: en
  section: General
  title: Our Pointless Pursuit Of Semantic Value
  type: webpage
  URL: >-
    https://www.smashingmagazine.com/2011/11/our-pointless-pursuit-of-semantic-value/

- id: maProTracerPracticalProvenance2016
  abstract: >-
    Provenance tracing is a very important approach to Advanced Persistent
    Threat (APT) attack detection and investigation. Existing techniques either
    suffer from the dependence explosion problem or have non-trivial space and
    runtime overhead, which hinder their application in practice. We propose
    ProTracer, a lightweight provenance tracing system that alternates between
    system event logging and unit level taint propagation. The technique is
    built on an on-the-fly system event processing infrastructure that features
    a very lightweight kernel module and a sophisticated user space daemon that
    performs concurrent and out-of-order event processing. The evaluation with
    different realistic system workloads and a number of attack cases show that
    ProTracer only produces 13MB log data per day, and
    0.84GB(Server)/2.32GB(Client) in 3 months without losing any important
    information. The space consumption is only < 1.28% of the state-of-the-art,
    7 times smaller than an off-line garbage collection technique. The run-time
    overhead averages <7% for servers and <5% for regular applications. The
    generated attack causal graphs are a few times smaller than those by
    existing techniques while they are equally informative.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Ma
      given: Shiqing
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: maProTracerPracticalProvenance2016
  container-title: Proceedings 2016 Network and Distributed System Security Symposium
  DOI: 10.14722/ndss.2016.23350
  event-place: San Diego, CA
  event-title: Network and Distributed System Security Symposium
  ISBN: 978-1-891562-41-9
  issued:
    - year: 2016
  language: en
  publisher: Internet Society
  publisher-place: San Diego, CA
  source: DOI.org (Crossref)
  title: >-
    ProTracer: Towards Practical Provenance Tracing by Alternating Between
    Logging and Tainting
  title-short: ProTracer
  type: paper-conference
  URL: >-
    https://www.ndss-symposium.org/wp-content/uploads/2017/09/protracer-towards-practical-provenance-tracing-alternating-logging-tainting.pdf

- id: marangunicTechnologyAcceptanceModel2015
  abstract: >-
    With the ever-increasing development of technology and its integration into
    users’ private and professional life, a decision regarding its acceptance or
    rejection still remains an open question. A respectable amount of work
    dealing with the technology acceptance model (TAM), from its first
    appearance more than a quarter of a century ago, clearly indicates a
    popularity of the model in the field of technology acceptance. Originated in
    the psychological theory of reasoned action and theory of planned behavior,
    TAM has evolved to become a key model in understanding predictors of human
    behavior toward potential acceptance or rejection of the technology. The
    main aim of the paper is to provide an up-to-date, well-researched resource
    of past and current references to TAM-related literature and to identify
    possible directions for future TAM research. The paper presents a
    comprehensive concept-centric literature review of the TAM, from 1986
    onwards. According to a designed methodology, 85 scientific publications
    have been selected and classified according to their aim and content into
    three categories such as (i) TAM literature reviews, (ii) development and
    extension of TAM, and (iii) modification and application of TAM. Despite a
    continuous progress in revealing new factors with significant influence on
    TAM’s core variables, there are still many unexplored areas of model
    potential application that could contribute to its predictive validity.
    Consequently, four possible future directions for TAM research based on the
    conducted literature review and analysis are identified and presented.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Marangunić
      given: Nikola
    - family: Granić
      given: Andrina
  citation-key: marangunicTechnologyAcceptanceModel2015
  container-title: Universal Access in the Information Society
  container-title-short: Univ Access Inf Soc
  DOI: 10.1007/s10209-014-0348-1
  ISSN: 1615-5289, 1615-5297
  issue: '1'
  issued:
    - year: 2015
      month: 3
  language: en
  page: 81-95
  source: DOI.org (Crossref)
  title: 'Technology acceptance model: a literature review from 1986 to 2013'
  title-short: Technology acceptance model
  type: article-journal
  URL: http://link.springer.com/10.1007/s10209-014-0348-1
  volume: '14'

- id: markrussSysmonSysinternals2023
  abstract: Monitors and reports key system activity via the Windows event log.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: markruss
      given: ''
  citation-key: markrussSysmonSysinternals2023
  issued:
    - year: 2023
      month: 4
      day: 11
  language: en-us
  title: Sysmon - Sysinternals
  type: webpage
  URL: https://learn.microsoft.com/en-us/sysinternals/downloads/sysmon

- id: marowkaPythonAcceleratorsHighperformance2018
  abstract: >-
    Python became the preferred language for teaching in academia, and it is one
    of the most popular programming languages for scientific computing. This
    wide popularity occurs despite the weak performance of the language. This
    weakness is the motivation that drives the efforts devoted by the Python
    community to improve the performance of the language. In this article, we
    are following these efforts while we focus on one specific promised solution
    that aims to provide high-performance and performance portability for Python
    applications.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Marowka
      given: Ami
  citation-key: marowkaPythonAcceleratorsHighperformance2018
  container-title: The Journal of Supercomputing
  container-title-short: J Supercomput
  DOI: 10.1007/s11227-017-2213-5
  ISSN: 1573-0484
  issue: '4'
  issued:
    - year: 2018
      month: 4
      day: 1
  language: en
  note: 'interest: 80'
  page: 1449-1460
  source: Springer Link
  title: Python accelerators for high-performance computing
  type: article-journal
  URL: https://doi.org/10.1007/s11227-017-2213-5
  volume: '74'

- id: matsakisReferencecountingLeaks2015
  accessed:
    - year: 2023
      month: 12
      day: 17
  author:
    - family: Matsakis
      given: Niko
  citation-key: matsakisReferencecountingLeaks2015
  issued:
    - year: 2015
      month: 4
      day: 29
  title: On reference-counting and leaks
  type: post-weblog
  URL: >-
    https://smallcultfollowing.com/babysteps/blog/2015/04/29/on-reference-counting-and-leaks/

- id: matteisHostingQueryableHighly2014
  abstract: >-
    SPARQL endpoints suffer from low availability, and require to buy and
    configure complex servers to host them. With the advent of Linked Data
    Fragments, and more specifically Triple Pattern Fragments (TPFs), we can now
    perform complex queries on low-cost servers. Online file repositories and
    cloud hosting services, such as GitHub, Google Code, Google App Engine or
    Dropbox can be exploited to host this type of linked data for free. For this
    purpose we have developed two different proof-of-concept tools that can be
    used to publish TPFs on GitHub and Google App Engine. A generic TPF client
    can then be used to perform SPARQL queries on the freely hosted TPF servers.
  accessed:
    - year: 2023
      month: 7
      day: 18
  author:
    - family: Matteis
      given: Luca
    - family: Verborgh
      given: Ruben
  citation-key: matteisHostingQueryableHighly2014
  collection-title: ISWC-DEV'14
  container-title: Proceedings of the 2014 International Conference on Developers - Volume 1268
  event-place: Aachen, DEU
  issued:
    - year: 2014
      month: 10
      day: 19
  page: 13–18
  publisher: CEUR-WS.org
  publisher-place: Aachen, DEU
  source: ACM Digital Library
  title: Hosting queryable and highly available linked data for free
  type: paper-conference

- id: matthewsFiveRetractedStructure2007
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Matthews
      given: Brian W.
  citation-key: matthewsFiveRetractedStructure2007
  container-title: Protein Science
  DOI: 10.1110/ps.072888607
  ISSN: 1469-896X
  issue: '6'
  issued:
    - year: 2007
  language: en
  page: 1013-1016
  source: Wiley Online Library
  title: 'Five retracted structure reports: Inverted or incorrect?'
  title-short: Five retracted structure reports
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1110/ps.072888607
  volume: '16'

- id: matthewsFrameworkSoftwarePreservation2010
  abstract: "Software preservation has not had detailed consideration as a research topic or in practical application. In this paper, we present a conceptual framework to capture and organise the main notions of software preservation, which are required for a coherent and comprehensive approach.\_ This framework has three main aspects. Firstly a discussion of what it means to preserve software via a performance model which considers how a software artefact can be rebuilt from preserved components and can then be seen to be representative of the original software product. Secondly the development of a model of software artefacts, describing the basic components of all software, loosely based on the FRBR model for representing digital artefacts and their history within a library context. Finally, the definition and categorisation of the properties of software artefacts which are required to ensure that the software product has been adequately preserved. These are broken down into a number of categories and related to the concepts defined in the OAIS standard. We also discuss our experience of recording these preservation properties for a number of BADC software products, which arose from a series of case studies conducted to evaluate the software preservation framework, and also briefly describe the SPEQS toolkit, a tool to capture software preservation properties within a software development."
  accessed:
    - year: 2023
      month: 2
      day: 3
  author:
    - family: Matthews
      given: Brian
    - family: Shaon
      given: Arif
    - family: Bicarregui
      given: Juan
    - family: Jones
      given: Catherine
  citation-key: matthewsFrameworkSoftwarePreservation2010
  container-title: International Journal of Digital Curation
  DOI: 10.2218/ijdc.v5i1.145
  ISSN: 1746-8256
  issue: '1'
  issued:
    - year: 2010
      month: 7
      day: 21
  language: en
  license: Copyright (c)
  number: '1'
  page: 91-105
  source: ijdc.net
  title: A Framework for Software Preservation
  type: article-journal
  URL: http://ijdc.net/index.php/ijdc/article/view/148
  volume: '5'

- id: mcdowellShapingFutureResearch2015
  abstract: >-
    The landscape of scientific research and funding is in flux as a result of
    tight budgets, evolving models of both publishing and evaluation, and
    questions about training and workforce stability. As future leaders, junior
    scientists are uniquely poised to shape the culture and practice of science
    in response to these challenges. A group of postdocs in the Boston area who
    are invested in improving the scientific endeavor, planned a symposium held
    on October 2 nd and 3 rd , 2014, as a way to join the discussion about the
    future of US biomedical research. Here we present a report of the
    proceedings of participant-driven workshops and the organizers’ synthesis of
    the outcomes.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: McDowell
      given: Gary S.
    - family: Gunsalus
      given: Kearney T. W.
    - family: MacKellar
      given: Drew C.
    - family: Mazzilli
      given: Sarah A.
    - family: Pai
      given: Vaibhav P.
    - family: Goodwin
      given: Patricia R.
    - family: Walsh
      given: Erica M.
    - family: Robinson-Mosher
      given: Avi
    - family: Bowman
      given: Thomas A.
    - family: Kraemer
      given: James
    - family: Erb
      given: Marcella L.
    - family: Schoenfeld
      given: Eldi
    - family: Shokri
      given: Leila
    - family: Jackson
      given: Jonathan D.
    - family: Islam
      given: Ayesha
    - family: Mattozzi
      given: Matthew D.
    - family: Krukenberg
      given: Kristin A.
    - family: Polka
      given: Jessica K.
  citation-key: mcdowellShapingFutureResearch2015
  DOI: 10.12688/f1000research.5878.2
  issued:
    - year: 2015
      month: 1
      day: 9
  language: en
  license: http://creativecommons.org/licenses/by/4.0/
  number: '3:291'
  publisher: F1000Research
  source: f1000research.com
  title: 'Shaping the Future of Research: a perspective from junior scientists'
  title-short: Shaping the Future of Research
  type: article
  URL: https://f1000research.com/articles/3-291

- id: mcgradyDialingVideosRandom2023
  abstract: >-
    YouTube is one of the largest, most important communication platforms in the
    world, but while there is a great deal of research about the site, many of
    its fundamental characteristics remain unknown. To better understand YouTube
    as a whole, we created a random sample of videos using a new method. Through
    a description of the sample’s metadata, we provide answers to many essential
    questions about, for example, the distribution of views, comments, likes,
    subscribers, and categories. Our method also allows us to estimate the total
    number of publicly visible videos on YouTube and its growth over time. To
    learn more about video content, we hand-coded a subsample to answer
    questions like how many are primarily music, video games, or still images.
    Finally, we processed the videos’ audio using language detection software to
    determine the distribution of spoken languages. In providing basic
    information about YouTube as a whole, we not only learn more about an
    influential platform, but also provide baseline context against which
    samples in more focused studies can be compared.
  accessed:
    - year: 2023
      month: 12
      day: 25
  author:
    - family: McGrady
      given: Ryan
    - family: Zheng
      given: Kevin
    - family: Curran
      given: Rebecca
    - family: Baumgartner
      given: Jason
    - family: Zuckerman
      given: Ethan
  citation-key: mcgradyDialingVideosRandom2023
  container-title: 'Journal of Quantitative Description: Digital Media'
  DOI: 10.51685/jqd.2023.022
  ISSN: 2673-8813
  issued:
    - year: 2023
      month: 12
      day: 20
  language: en
  license: >-
    Copyright (c) 2023 Ryan McGrady, Kevin Zheng, Rebecca Curran, Jason
    Baumgartner, Ethan Zuckerman
  source: journalqd.org
  title: 'Dialing for Videos: A Random Sample of YouTube'
  title-short: Dialing for Videos
  type: article-journal
  URL: https://journalqd.org/article/view/4066
  volume: '3'

- id: mckinneyApacheArrow102017
  accessed:
    - year: 2024
      month: 1
      day: 15
  author:
    - family: McKinney
      given: Wes
  citation-key: mckinneyApacheArrow102017
  container-title: Wes McKinney
  issued:
    - year: 2017
      month: 9
      day: 21
  language: en
  title: Apache Arrow and the “10 Things I Hate About pandas”
  type: post-weblog
  URL: https://wesmckinney.com/blog/apache-arrow-pandas-internals/

- id: mckinneyPracticalMediumData2013
  accessed:
    - year: 2024
      month: 1
      day: 15
  author:
    - family: McKinney
      given: Wes
  citation-key: mckinneyPracticalMediumData2013
  event-place: New York, New York
  event-title: PyData
  issued:
    - year: 2013
      month: 11
      day: 9
  language: en
  publisher-place: New York, New York
  title: >-
    Practical Medium Data Analytics with Python (10 Things I Hate About pandas,
    PyData NYC 2013)
  type: speech
  URL: https://www.slideshare.net/wesm/practical-medium-data-analytics-with-python

- id: mcphillipsScientificWorkflowDesign2009
  abstract: "Recent years have seen a dramatic increase in research and development of scientific workflow systems. These systems promise to make scientists more productive by automating data-driven and compute-intensive analyses. Despite many early achievements, the long-term success of scientific workflow technology critically depends on making these systems useable by “mere mortals”, i.e.,\_scientists who have a very good idea of the analysis methods they wish to assemble, but who are neither software developers nor scripting-language experts. With these users in mind, we identify a set of desiderata for scientific workflow systems crucial for enabling scientists to model and design the workflows they wish to automate themselves. As a first step towards meeting these requirements, we also show how the collection-oriented modeling and design (comad) approach for scientific workflows, implemented within the Kepler system, can help provide these critical, design-oriented capabilities to scientists."
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: McPhillips
      given: Timothy
    - family: Bowers
      given: Shawn
    - family: Zinn
      given: Daniel
    - family: Ludäscher
      given: Bertram
  citation-key: mcphillipsScientificWorkflowDesign2009
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2008.06.013
  ISSN: 0167-739X
  issue: '5'
  issued:
    - year: 2009
      month: 5
      day: 1
  language: en
  note: 'interest: 97'
  page: 541-551
  source: ScienceDirect
  title: Scientific workflow design for mere mortals
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167739X08000873
  volume: '25'

- id: mcvoyLmbenchPortableTools1996
  abstract: >-
    lmbench is a micro-benchmark suite designed to focus attention on the basic
    building blocks of many common system applications, such as databases,
    simulations, software development, and networking. In almost all cases, the
    individual tests are the result of analysis and isolation of a customer’s
    actual performance problem. These tools can be, and currently are, used to
    compare different system implementations from different vendors. In several
    cases, the benchmarks have uncovered previously unknown bugs and design
    flaws. The results have shown a strong correlation between memory system
    performance and overall performance. lmbench includes an extensible

    database of results from systems current as of late 

    1995.
  author:
    - family: McVoy
      given: Larry
    - family: Staelin
      given: Carl
  citation-key: mcvoyLmbenchPortableTools1996
  container-title: Proceedings of the USENIX 1996 Annual Technical Conference
  event-place: San Diego, CA
  event-title: USENIX 1996 ATC (Annual Technical Conference)
  issued:
    - year: 1996
      month: 1
  language: en
  publisher: USENIX
  publisher-place: San Diego, CA
  source: Zotero
  title: 'lmbench: Portable Tools for Performance Analysis'
  type: paper-conference
  URL: >-
    https://www.usenix.org/legacy/publications/library/proceedings/sd96/full_papers/mcvoy.pdf

- id: meehlWhySummariesResearch1990
  abstract: >-
    Null hypothesis testing of correlational predictions from weak substantive
    theories in soft psychology is subject to the influence of ten obfuscating
    factors whose effects are usually (1) sizeable, (2) opposed, (3) variable,
    and (4) unknown. The net epistemic effect of these ten obfuscating
    influences is that the usual research literature review is well-nigh
    uninterpretable. Major changes in graduate education, conduct of research,
    and editorial policy are proposed.
  accessed:
    - year: 2022
      month: 9
      day: 9
  author:
    - family: Meehl
      given: Paul E.
  citation-key: meehlWhySummariesResearch1990
  container-title: Psychological Reports
  container-title-short: Psychol Rep
  DOI: 10.2466/pr0.1990.66.1.195
  ISSN: 0033-2941
  issue: '1'
  issued:
    - year: 1990
      month: 2
      day: 1
  language: en
  note: 'interest: 85'
  page: 195-244
  publisher: SAGE Publications Inc
  source: SAGE Journals
  title: >-
    Why Summaries of Research on Psychological Theories are Often
    Uninterpretable
  type: article-journal
  URL: https://doi.org/10.2466/pr0.1990.66.1.195
  volume: '66'

- id: MemoizationCheckpointing
  accessed:
    - year: 2022
      month: 5
      day: 12
  citation-key: MemoizationCheckpointing
  container-title: Parsl 1.2.0 documentation
  title: Memoization and checkpointing
  type: webpage
  URL: >-
    https://parsl.readthedocs.io/en/stable/userguide/checkpoints.html#app-equivalence

- id: memonTamingGooglescaleContinuous2017
  abstract: >-
    Growth in Google's code size and feature churn rate has seen increased
    reliance on continuous integration (CI) and testing to maintain quality.
    Even with enormous resources dedicated to testing, we are unable to
    regression test each code change individually, resulting in increased lag
    time between code check-ins and test result feedback to developers. We
    report results of a project that aims to reduce this time by: (1)
    controlling test workload without compromising quality, and (2) distilling
    test results data to inform developers, while they write code, of the impact
    of their latest changes on quality. We model, empirically understand, and
    leverage the correlations that exist between our code, test cases,
    developers, programming languages, and code-change and test-execution
    frequencies, to improve our CI and development processes. Our findings show:
    very few of our tests ever fail, but those that do are generally "closer" to
    the code they test, certain frequently modified code and certain users/tools
    cause more breakages, and code recently modified by multiple developers
    (more than 3) breaks more often.
  author:
    - family: Memon
      given: Atif
    - family: Gao
      given: Zebao
    - family: Nguyen
      given: Bao
    - family: Dhanda
      given: Sanjeev
    - family: Nickell
      given: Eric
    - family: Siemborski
      given: Rob
    - family: Micco
      given: John
  citation-key: memonTamingGooglescaleContinuous2017
  container-title: >-
    2017 IEEE/ACM 39th International Conference on Software Engineering:
    Software Engineering in Practice Track (ICSE-SEIP)
  DOI: 10.1109/ICSE-SEIP.2017.16
  event-title: >-
    2017 IEEE/ACM 39th International Conference on Software Engineering:
    Software Engineering in Practice Track (ICSE-SEIP)
  issued:
    - year: 2017
      month: 5
  note: 'interest: 85'
  page: 233-242
  source: IEEE Xplore
  title: Taming Google-scale continuous testing
  type: paper-conference

- id: mengFacilitatingReproducibilityScientific2017
  abstract: >-
    Scientific workflows are designed to solve complex scientific problems and
    accelerate scientific progress. Ideally, scientific workflows should improve
    the reproducibility of scientific applications by making it easier to share
    and reuse workflows between scientists. However, scientists often find it
    difficult to reuse others’ workflows, which is known as workflow decay. In
    this paper, we explore the challenges in reproducing scientific workflows,
    and propose a framework for facilitating the reproducibility of scientific
    workflows at the task level by giving scientists complete control over the
    execution environments of the tasks in their workflows and integrating
    execution environment specifications into scientific workflow systems. Our
    framework allows dependencies to be archived in basic units of OS image,
    software and data instead of gigantic all-in-one images. We implement a
    prototype of our framework by integrating Umbrella, an execution environment
    creator, into Makeflow, a scientific workflow system. To evaluate our
    framework, we use it to run two bioinformatics scientific workflows, BLAST
    and BWA. The execution environment of the tasks in each workflow is
    specified as an Umbrella specification file, and sent to execution nodes
    where Umbrella is used to create the specified environment for running the
    tasks. For each workflow we evaluate the size of the Umbrella specification
    file, the time and space overheads of creating execution environments using
    Umbrella, and the heterogeneity of execution nodes contributing to each
    workflow. The evaluation results show that our framework improves the
    utilization of heterogeneous computing resources, and improves the
    portability and reproducibility of scientific workflows.
  accessed:
    - year: 2023
      month: 2
      day: 20
  author:
    - family: Meng
      given: Haiyan
    - family: Thain
      given: Douglas
  citation-key: mengFacilitatingReproducibilityScientific2017
  collection-title: >-
    International Conference on Computational Science, ICCS 2017, 12-14 June
    2017, Zurich, Switzerland
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2017.05.116
  ISSN: 1877-0509
  issued:
    - year: 2017
      month: 1
      day: 1
  language: en
  page: 705-714
  source: ScienceDirect
  title: >-
    Facilitating the Reproducibility of Scientific Workflows with Execution
    Environment Specifications
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S1877050917306816
  volume: '108'

- id: mertonSociologyScienceTheoretical1974
  author:
    - family: Merton
      given: Robert K.
  citation-key: mertonSociologyScienceTheoretical1974
  edition: 4. Dr.
  event-place: Chicago
  ISBN: 978-0-226-52092-6
  issued:
    - year: 1974
  language: eng
  number-of-pages: '605'
  publisher: Univ. of Chicago Pr
  publisher-place: Chicago
  source: K10plus ISBN
  title: 'The sociology of science: theoretical and empirical investigations'
  title-short: The sociology of science
  type: book

- id: mesnardReproducibleReplicableCFD2016
  abstract: >-
    Completing a full replication study of our previously published findings on
    bluff-body aerodynamics was harder than we thought. Despite the fact that we
    have good reproducible-research practices, sharing our code and data openly.
    Here's what we learned from three years, four CFD codes and hundreds of
    runs.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Mesnard
      given: Olivier
    - family: Barba
      given: Lorena A.
  citation-key: mesnardReproducibleReplicableCFD2016
  DOI: 10.48550/arXiv.1605.04339
  issued:
    - year: 2016
      month: 10
      day: 14
  note: 'interest: 94'
  number: arXiv:1605.04339
  publisher: arXiv
  source: arXiv.org
  title: 'Reproducible and replicable CFD: it''s harder than you think'
  title-short: Reproducible and replicable CFD
  type: article
  URL: http://arxiv.org/abs/1605.04339

- id: meurerCondaCrossPlatform2014
  accessed:
    - year: 2023
      month: 4
      day: 6
  author:
    - family: Meurer
      given: Aaron
  citation-key: meurerCondaCrossPlatform2014
  event-title: SciPy 2014
  issued:
    - year: 2014
      month: 7
      day: 9
  title: 'Conda: A Cross Platform Package Manager for any Binary Distribution'
  title-short: Conda
  type: speech
  URL: https://www.youtube.com/watch?v=UaIvrDWrIWM

- id: meyerTypecheckedPythonReal2018
  abstract: >-
    You've heard about Python type annotations, but wondered if they're useful
    in the real world? Worried you've got too much code and can't afford to
    annotate it?  Type-checked Python is here, it's for real, and it can help
    you catch bugs and make your code easier to understand. Come learn from our
    experience gradually typing a million-LOC production Python application!


    Type checking solves real world problems in production Python systems. We'll
    cover the benefits, how type checking in Python works, how to introduce it
    gradually and sustainably in a production Python application, and how to
    measure success and avoid common pitfalls. We'll even demonstrate how modern
    Python typechecking goes hand-in-hand with duck-typing! Join us for a deep
    dive into type-checked Python in the real world.
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Meyer
      given: Carl
  citation-key: meyerTypecheckedPythonReal2018
  issued:
    - year: 2018
      month: 5
      day: 13
  title: Type-checked Python in the real world
  type: speech
  URL: https://www.youtube.com/watch?v=pMgmKJyWKn8

- id: meynersEquivalenceTestsReview2012
  abstract: >-
    Equivalence tests are becoming increasingly popular in many application
    areas including sensory sciences. They should be applied whenever the aim of
    the study is not to show differences, but to conclude similarity. There has
    been quite some debate about pros and cons of different approaches, reaching
    the sensory and sensometrics community in recent years. Parts of the
    sometimes heated debate are, in our opinion, due to mutual
    misunderstandings, and to different objectives and hence selection criteria
    for an “optimal” test, boiling down to the question whether power is the
    only optimality criterion to apply, or whether somehow vague criteria like
    intuition should be taken into account as well. This review intends to give
    an introduction into equivalence tests, starting with some general
    considerations on the statistical testing of hypotheses. We will
    subsequently give an overview over the most common approaches, some of which
    are shown to be inappropriate (e.g. the power approach). Some valid and
    relatively simple methods will be introduced and their correspondence to
    confidence intervals clarified, while we will skip the mathematical details
    of some more recent tests. Instead, the pros and cons of different
    approaches will be discussed and recommendations given.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Meyners
      given: Michael
  citation-key: meynersEquivalenceTestsReview2012
  container-title: Food Quality and Preference
  container-title-short: Food Quality and Preference
  DOI: 10.1016/j.foodqual.2012.05.003
  ISSN: 0950-3293
  issue: '2'
  issued:
    - year: 2012
      month: 12
      day: 1
  page: 231-245
  source: ScienceDirect
  title: Equivalence tests – A review
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0950329312000961
  volume: '26'

- id: miccoStateContinuousIntegration2017
  abstract: >-
    This presentation deck speaks to some of the key problems and challenges
    facing the internal Google software engineering stack and invites industry
    and academic collaboration to help improve the state of the art of software
    testing throughout the industry.
  author:
    - family: Micco
      given: John
  citation-key: miccoStateContinuousIntegration2017
  issued:
    - year: 2017
  note: 'score: 80'
  title: The State of Continuous Integration Testing @Google
  type: speech
  URL: >-
    https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45880.pdf

- id: michaelAskChefsOSTP2022
  abstract: >-
    Everyone has an opinion about the OSTP Policy memo! Come over and hear what
    the Chefs have to say and share your opinions with us. Part 1 of a 2 part
    post.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Michael
      given: Ann
  citation-key: michaelAskChefsOSTP2022
  container-title: The Scholarly Kitchen
  issued:
    - year: 2022
      month: 8
      day: 30
  language: en-US
  note: 'interest: 83'
  title: 'Ask The Chefs: OSTP Policy Part I'
  title-short: Ask The Chefs
  type: post-weblog
  URL: https://scholarlykitchen.sspnet.org/2022/08/30/ask-the-chefs-ostp-policy-i/

- id: mightIllustratedGuidePh
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Might
      given: Matt
  citation-key: mightIllustratedGuidePh
  title: The illustrated guide to a Ph.D.
  type: post-weblog
  URL: https://matt.might.net/articles/phd-school-in-pictures/

- id: milewiczNegativePerceptionsApplicability2021
  abstract: >-
    The U.S. Department of Energy's Office of Scientific and Technical
    Information
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Milewicz
      given: R.
    - family: Pirkelbauer
      given: P.
    - family: Soundararajan
      given: P.
    - family: Ahmed
      given: H.
    - family: Skjellum
      given: A.
  citation-key: milewiczNegativePerceptionsApplicability2021
  issued:
    - year: 2021
      month: 4
      day: 8
  language: English
  note: 'interest: 69'
  number: LLNL-CONF-821299
  publisher: Lawrence Livermore National Lab. (LLNL), Livermore, CA (United States)
  source: www.osti.gov
  title: >-
    Negative Perceptions About the Applicability of Source-to-Source Compilers
    in HPC: A Literature Review
  title-short: >-
    Negative Perceptions About the Applicability of Source-to-Source Compilers
    in HPC
  type: report
  URL: https://www.osti.gov/biblio/1806418

- id: milewiczPositionPaperUsability2019
  abstract: >-
    The modern HPC scientific software ecosystem is instrumental to the practice
    of science. However, software can only fulfill that role if it is readily
    usable. In this position paper, we discuss usability in the context of
    scientific software development, how usability engineering can be
    incorporated into current practice, and how software engineering research
    can help satisfy that objective.
  author:
    - family: Milewicz
      given: Reed
    - family: Rodeghero
      given: Paige
  citation-key: milewiczPositionPaperUsability2019
  container-title: >-
    2019 IEEE/ACM 14th International Workshop on Software Engineering for
    Science (SE4Science)
  DOI: 10.1109/SE4Science.2019.00012
  event-title: >-
    2019 IEEE/ACM 14th International Workshop on Software Engineering for
    Science (SE4Science)
  issued:
    - year: 2019
      month: 5
  note: 'interest: 87'
  page: 41-42
  source: IEEE Xplore
  title: >-
    Position Paper: Towards Usability as a First-Class Quality of HPC Scientific
    Software
  title-short: Position Paper
  type: paper-conference

- id: milewiczRequirementsElicitationTechniques
  author:
    - family: Milewicz
      given: Reed
  citation-key: milewiczRequirementsElicitationTechniques
  publisher: Sandia National Laboratories, Software Engineering and Research Department
  title: 'Requirements Elicitation Techniques: Guidelines and Recommendations'
  type: report
  URL: >-
    https://sandialabs-my.sharepoint.com/personal/rmilewi_sandia_gov/Documents/Microsoft%20Teams%20Chat%20Files/elicitation_rapid_review.pdf

- id: millerScientistNightmareSoftware2006
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Miller
      given: Greg
  citation-key: millerScientistNightmareSoftware2006
  container-title: Science
  container-title-short: Science
  DOI: 10.1126/science.314.5807.1856
  ISSN: 0036-8075, 1095-9203
  issue: '5807'
  issued:
    - year: 2006
      month: 12
      day: 22
  language: en
  page: 1856-1857
  source: DOI.org (Crossref)
  title: 'A Scientist''s Nightmare: Software Problem Leads to Five Retractions'
  title-short: A Scientist's Nightmare
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.314.5807.1856
  volume: '314'

- id: minerFormallyComparingTopic2023
  abstract: >-
    Differences between computationally generated and human-generated themes in
    unstructured text are important to understand yet difficult to assess
    formally. In this study, we bridge these approaches through two
    contributions. First, we formally compare a primarily computational
    approach, topic modeling, to a primarily human-driven approach, qualitative
    thematic coding, in an impactful context: physician mothers’ experience of
    workplace discrimination. Second, we compare our chosen topic model to a
    principled alternative topic model to make explicit study design decisions
    meriting consideration in future research. By formally contrasting
    computationally generated (i.e. topic modeling) and human-generated (i.e.
    thematic coding) knowledge, we shed light on issues of interest to several
    audiences, notably computational social scientists who wish to understand
    study design tradeoffs, and qualitative researchers who may wish to leverage
    computational methods to improve the speed and reproducibility of
    labor-intensive coding. Although useful in other domains, we highlight the
    value of fast, reproducible methods to better understand experiences of
    workplace discrimination.
  accessed:
    - year: 2023
      month: 6
      day: 9
  author:
    - family: Miner
      given: Adam S
    - family: Stewart
      given: Sheridan A
    - family: Halley
      given: Meghan C
    - family: Nelson
      given: Laura K
    - family: Linos
      given: Eleni
  citation-key: minerFormallyComparingTopic2023
  container-title: Big Data & Society
  DOI: 10.1177/20539517221149106
  ISSN: 2053-9517
  issue: '1'
  issued:
    - year: 2023
      month: 1
      day: 1
  language: en
  page: '20539517221149106'
  publisher: SAGE Publications Ltd
  source: SAGE Journals
  title: >-
    Formally comparing topic models and human-generated qualitative coding  of
    physician mothers’ experiences  of workplace discrimination
  type: article-journal
  URL: https://doi.org/10.1177/20539517221149106
  volume: '10'

- id: missierDPROVExtendingPROV2013
  abstract: >-
    This paper presents an extension to the W3C PROV provenance model, aimed at
    representing process structure. Although the modelling of process structure
    is out of the scope of the PROV speciﬁcation, it is beneﬁcial when capturing
    and analyzing the provenance of data that is produced by programs or other
    formally encoded processes. In the paper, we motivate the need for such and
    extended model in the context of an ongoing large data federation and
    preservation project, DataONE, where provenance traces of scientiﬁc workﬂow
    runs are captured and stored alongside the data products. We introduce new
    provenance relations for modelling process structure along with their usage
    patterns, and present sample queries that demonstrate their beneﬁt.
  author:
    - family: Missier
      given: Paulo
    - family: Dey
      given: Sauman
    - family: Belhajjame
      given: Khalid
    - family: Cuevas-Vicenttín
      given: Victor
    - family: Ludäscher
      given: Bertram
  citation-key: missierDPROVExtendingPROV2013
  container-title: 5th Workshop on the Theory and Practice of Provenance
  issued:
    - year: 2013
      month: 4
      day: 3
  publisher: Lombard, IL
  title: 'D-PROV: Extending the PROV Provenance Model with Workﬂow Structure'
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/tapp13/technical-sessions/presentation/missier

- id: missierFinegrainedEfficientLineage2010
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Missier
      given: Paolo
    - family: Paton
      given: Norman W.
    - family: Belhajjame
      given: Khalid
  citation-key: missierFinegrainedEfficientLineage2010
  container-title: >-
    Proceedings of the 13th International Conference on Extending Database
    Technology - EDBT '10
  DOI: 10.1145/1739041.1739079
  event-place: Lausanne, Switzerland
  event-title: the 13th International Conference
  ISBN: 978-1-60558-945-9
  issued:
    - year: 2010
  language: en
  page: '299'
  publisher: ACM Press
  publisher-place: Lausanne, Switzerland
  source: DOI.org (Crossref)
  title: >-
    Fine-grained and efficient lineage querying of collection-based workflow
    provenance
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=1739041.1739079

- id: missierW3CPROVFamily2013
  abstract: >-
    Provenance, a form of structured metadata designed to record the origin or
    source of information, can be instrumental in deciding whether information
    is to be trusted, how it can be integrated with other diverse information
    sources, and how to establish attribution of information to authors
    throughout its history. The PROV set of specifications, produced by the
    World Wide Web Consortium (W3C), is designed to promote the publication of
    provenance information on the Web, and offers a basis for interoperability
    across diverse provenance management systems. The PROV provenance model is
    deliberately generic and domain-agnostic, but extension mechanisms are
    available and can be exploited for modelling specific domains. This tutorial
    provides an account of these specifications. Starting from intuitive and
    informal examples that present idiomatic provenance patterns, it
    progressively introduces the relational model of provenance along with the
    constraints model for validation of provenance documents, and concludes with
    example applications that show the extension points in use.
  accessed:
    - year: 2022
      month: 7
      day: 11
  author:
    - family: Missier
      given: Paolo
    - family: Belhajjame
      given: Khalid
    - family: Cheney
      given: James
  citation-key: missierW3CPROVFamily2013
  container-title: >-
    Proceedings of the 16th International Conference on Extending Database
    Technology - EDBT '13
  DOI: 10.1145/2452376.2452478
  event-place: Genoa, Italy
  event-title: the 16th International Conference
  ISBN: 978-1-4503-1597-5
  issued:
    - year: 2013
  language: en
  page: '773'
  publisher: ACM Press
  publisher-place: Genoa, Italy
  source: DOI.org (Crossref)
  title: The W3C PROV family of specifications for modelling provenance metadata
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?doid=2452376.2452478

- id: miyaokaEmergentCodingTopic2023
  abstract: >-
    More than ever in the past, researchers have access to broad, educationally
    relevant text data from sources such as literature databases (e.g., ERIC),
    an open-ended response from online courses/surveys, online discussion
    forums, digital essays, and social media. These advances in data
    availability can dramatically increase the possibilities for discovering new
    patterns in the data and testing new theories through processing texts with
    emerging analytic techniques. In our study, we extended the application of
    Topic Modeling (TM) to data collected from focus groups within the context
    of a larger study. Specifically, we compared the results of emergent
    qualitative coding and TM. We found a high level of agreement between TM and
    emergent qualitative coding, suggesting TM is a viable method for coding
    focus group data when augmenting and validating manual qualitative coding.
    We also found that TM was ineffective in capturing more nuanced information
    than the qualitative coding was able to identify. This can be explained by
    two factors: (1) the word level tokenization we used in the study, and (2)
    variations in the terminology teachers used to identify the different
    technologies. Recommendations include additional data cleaning steps
    researchers should take and specifications within the topic modeling code
    when using topic modeling to analyze focus group data.
  accessed:
    - year: 2023
      month: 6
      day: 14
  author:
    - family: Miyaoka
      given: Atsushi
    - family: Decker-Woodrow
      given: Lauren
    - family: Hartman
      given: Nancy
    - family: Booker
      given: Barbara
    - family: Ottmar
      given: Erin
  citation-key: miyaokaEmergentCodingTopic2023
  container-title: International Journal of Qualitative Methods
  DOI: 10.1177/16094069231165950
  ISSN: 1609-4069
  issued:
    - year: 2023
      month: 1
      day: 1
  language: en
  page: '16094069231165950'
  publisher: SAGE Publications Inc
  source: SAGE Journals
  title: >-
    Emergent Coding and Topic Modeling: A Comparison of Two Qualitative Analysis
    Methods on Teacher Focus Group Data
  title-short: Emergent Coding and Topic Modeling
  type: article-journal
  URL: https://doi.org/10.1177/16094069231165950
  volume: '22'

- id: mokhovBuildSystemsCarte2018
  abstract: >-
    Build systems are awesome, terrifying -- and unloved. They are used by every
    developer around the world, but are rarely the object of study. In this
    paper we offer a systematic, and executable, framework for developing and
    comparing build systems, viewing them as related points in landscape rather
    than as isolated phenomena. By teasing apart existing build systems, we can
    recombine their components, allowing us to prototype new build systems with
    desired properties.
  accessed:
    - year: 2023
      month: 5
      day: 6
  author:
    - family: Mokhov
      given: Andrey
    - family: Mitchell
      given: Neil
    - family: Peyton Jones
      given: Simon
  citation-key: mokhovBuildSystemsCarte2018
  container-title: Proceedings of the ACM on Programming Languages
  container-title-short: Proc. ACM Program. Lang.
  DOI: 10.1145/3236774
  ISSN: 2475-1421
  issue: ICFP
  issued:
    - year: 2018
      month: 7
      day: 30
  language: en
  page: 1-29
  source: DOI.org (Crossref)
  title: Build systems à la carte
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/3236774
  volume: '2'

- id: molderSustainableDataAnalysis2021
  abstract: >-
    Data analysis often entails a multitude of heterogeneous steps, from the
    application of various command line tools to the usage of scripting
    languages like R or Python for the generation of plots and tables. It is
    widely recognized that data analyses should ideally be conducted in a
    reproducible way.&nbsp;Reproducibility enables technical validation and
    regeneration of results on the original or even new data. However,
    reproducibility alone is by no means sufficient to deliver an analysis that
    is of lasting impact (i.e., sustainable) for the field, or even just one
    research group. We postulate that it is equally important to ensure
    adaptability and transparency. The former describes the ability to modify
    the analysis to answer extended or slightly different research questions.
    The latter describes the ability to understand the analysis in order to
    judge whether it is not only technically, but methodologically valid. Here,
    we analyze the properties needed for a data analysis to become reproducible,
    adaptable, and transparent. We show how the popular workflow management
    system Snakemake can be used to guarantee this, and how it enables an
    ergonomic, combined, unified representation of all steps involved in data
    analysis, ranging from raw data processing, to quality control and
    fine-grained, interactive exploration and plotting of final results.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Mölder
      given: Felix
    - family: Jablonski
      given: Kim Philipp
    - family: Letcher
      given: Brice
    - family: Hall
      given: Michael B.
    - family: Tomkins-Tinch
      given: Christopher H.
    - family: Sochat
      given: Vanessa
    - family: Forster
      given: Jan
    - family: Lee
      given: Soohyun
    - family: Twardziok
      given: Sven O.
    - family: Kanitz
      given: Alexander
    - family: Wilm
      given: Andreas
    - family: Holtgrewe
      given: Manuel
    - family: Rahmann
      given: Sven
    - family: Nahnsen
      given: Sven
    - family: Köster
      given: Johannes
  citation-key: molderSustainableDataAnalysis2021
  DOI: 10.12688/f1000research.29032.2
  issued:
    - year: 2021
      month: 4
      day: 19
  language: en
  license: http://creativecommons.org/licenses/by/4.0/
  number: '10:33'
  publisher: F1000Research
  source: f1000research.com
  title: Sustainable data analysis with Snakemake
  type: article
  URL: https://f1000research.com/articles/10-33

- id: momotSevenTurretsBabel2016
  abstract: >-
    Input-handling bugs share two common patterns: insufficient recognition,
    where input-checking logic is unfit to validate a program's assumptions
    about inputs, %leading to the code acting on invalid inputs, and parser
    differentials, wherein two or more components of a system fail to interpret
    input equivalently. We argue that these patterns are artifacts of avoidable
    weaknesses in the development process and explore these patterns both in
    general and via recent CVE instances. We break ground on defining the
    input-handling code weaknesses that should be actionable findings and
    propose a refactoring of existing CWEs to accommodate them. We propose a set
    of new CWEs to name such weaknesses that will help code auditors and
    penetration testers precisely express their findings of likely vulnerable
    code structures.
  accessed:
    - year: 2024
      month: 1
      day: 11
  author:
    - family: Momot
      given: Falcon
    - family: Bratus
      given: Sergey
    - family: Hallberg
      given: Sven M.
    - family: Patterson
      given: Meredith L.
  citation-key: momotSevenTurretsBabel2016
  container-title: 2016 IEEE Cybersecurity Development (SecDev)
  DOI: 10.1109/SecDev.2016.019
  event-title: 2016 IEEE Cybersecurity Development (SecDev)
  issued:
    - year: 2016
      month: 11
  page: 45-52
  source: IEEE Xplore
  title: >-
    The Seven Turrets of Babel: A Taxonomy of LangSec Errors and How to Expunge
    Them
  title-short: The Seven Turrets of Babel
  type: paper-conference
  URL: https://ieeexplore.ieee.org/abstract/document/7839788

- id: mondelliCapturingSemanticallyDescribing2021
  abstract: >-
    Scientific workflows are commonly used to model and execute large-scale
    scientific experiments. They represent key resources for scientists and are
    enacted and managed by Scientific Workflow Management Systems (SWfMS). Each
    SWfMS has its particular approach to execute workflows and to capture and
    manage their provenance data. Due to the large scale of experiments, it may
    be unviable to analyze provenance data only after the end of the execution.
    A single experiment may demand weeks to run, even in high performance
    computing environments. Thus scientists need to monitor the experiment
    during its execution, and this can be done through provenance data. Runtime
    provenance analysis allows for scientists to monitor workflow execution and
    to take actions before the end of it (i.e. workflow steering). This
    provenance data can also be used to fine-tune the parallel execution of the
    workflow dynamically. We use the PROV data model as a basic framework for
    modeling and providing runtime provenance as a database that can be queried
    even during the execution. This database is agnostic of SWfMS and workflow
    engine. We show the benefits of representing and sharing runtime provenance
    data for improving the experiment management as well as the analysis of the
    scientific data.
  accessed:
    - year: 2022
      month: 7
      day: 26
  author:
    - family: Mondelli
      given: Maria Luiza
    - family: Samuel
      given: Sheeba
    - family: Konig-Ries
      given: Birgitta
    - family: Gadelha
      given: Luiz M. R.
  citation-key: mondelliCapturingSemanticallyDescribing2021
  container-title: 2021 IEEE 17th International Conference on eScience (eScience)
  DOI: 10.1109/eScience51609.2021.00057
  event-place: Innsbruck, Austria
  event-title: 2021 IEEE 17th International Conference on eScience (eScience)
  ISBN: 978-1-66540-361-0
  issued:
    - year: 2021
      month: 9
  page: 283-288
  publisher: IEEE
  publisher-place: Innsbruck, Austria
  source: DOI.org (Crossref)
  title: >-
    Capturing and Semantically Describing Provenance to Tell the Story of R
    Scripts
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/9582412/

- id: mooreDevelopmentInstrumentMeasure1991
  abstract: >-
    This paper reports on the development of an instrument designed to measure
    the various perceptions that an individual may have of adopting an
    information technology (IT) innovation. This instrument is intended to be a
    tool for the study of the initial adoption and eventual diffusion of IT
    innovations within organizations. While the adoption of information
    technologies by individuals and organizations has been an area of
    substantial research interest since the early days of computerization,
    research efforts to date have led to mixed and inconclusive outcomes. The
    lack of a theoretical foundation for such research and inadequate definition
    and measurement of constructs have been identified as major causes for such
    outcomes. In a recent study examining the diffusion of new end-user IT, we
    decided to focus on measuring the potential adopters' perceptions of the
    technology. Measuring such perceptions has been termed a “classic issue” in
    the innovation diffusion literature, and a key to integrating the various
    findings of diffusion research. The perceptions of adopting were initially
    based on the five characteristics of innovations derived by Rogers (1983)
    from the diffusion of innovations literature, plus two developed
    specifically within this study. Of the existing scales for measuring these
    characteristics, very few had the requisite levels of validity and
    reliability. For this study, both newly created and existing items were
    placed in a common pool and subjected to four rounds of sorting by judges to
    establish which items should be in the various scales. The objective was to
    verify the convergent and discriminant validity of the scales by examining
    how the items were sorted into various construct categories. Analysis of
    inter-judge agreement about item placement identified both bad items as well
    as weaknesses in some of the constructs' original definitions. These were
    subsequently redefined. Scales for the resulting constructs were subjected
    to three separate field tests. Following the final test, the scales all
    demonstrated acceptable levels of reliability. Their validity was further
    checked using factor analysis, as well as conducting discriminant analysis
    comparing responses between adopters and nonadopters of the innovation. The
    result is a parsimonious, 38-item instrument comprising eight scales which
    provides a useful tool for the study of the initial adoption and diffusion
    of innovations. A short, 25 item, version of the instrument is also
    suggested.
  accessed:
    - year: 2022
      month: 6
      day: 2
  author:
    - family: Moore
      given: Gary C.
    - family: Benbasat
      given: Izak
  citation-key: mooreDevelopmentInstrumentMeasure1991
  container-title: Information Systems Research
  container-title-short: Information Systems Research
  DOI: 10.1287/isre.2.3.192
  ISSN: 1047-7047, 1526-5536
  issue: '3'
  issued:
    - year: 1991
      month: 9
  language: en
  page: 192-222
  source: DOI.org (Crossref)
  title: >-
    Development of an Instrument to Measure the Perceptions of Adopting an
    Information Technology Innovation
  type: article-journal
  URL: http://pubsonline.informs.org/doi/abs/10.1287/isre.2.3.192
  volume: '2'

- id: moreauSpecialIssueFirst2008
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Moreau
      given: Luc
    - family: Ludäscher
      given: Bertram
    - family: Altintas
      given: Ilkay
    - family: Barga
      given: Roger S.
    - family: Bowers
      given: Shawn
    - family: Callahan
      given: Steven
    - family: Chin
      given: George
    - family: Clifford
      given: Ben
    - family: Cohen
      given: Shirley
    - family: Cohen-Boulakia
      given: Sarah
    - family: Davidson
      given: Susan
    - family: Deelman
      given: Ewa
    - family: Digiampietri
      given: Luciano
    - family: Foster
      given: Ian
    - family: Freire
      given: Juliana
    - family: Frew
      given: James
    - family: Futrelle
      given: Joe
    - family: Gibson
      given: Tara
    - family: Gil
      given: Yolanda
    - family: Goble
      given: Carole
    - family: Golbeck
      given: Jennifer
    - family: Groth
      given: Paul
    - family: Holland
      given: David A.
    - family: Jiang
      given: Sheng
    - family: Kim
      given: Jihie
    - family: Koop
      given: David
    - family: Krenek
      given: Ales
    - family: McPhillips
      given: Timothy
    - family: Mehta
      given: Gaurang
    - family: Miles
      given: Simon
    - family: Metzger
      given: Dominic
    - family: Munroe
      given: Steve
    - family: Myers
      given: Jim
    - family: Plale
      given: Beth
    - family: Podhorszki
      given: Norbert
    - family: Ratnakar
      given: Varun
    - family: Santos
      given: Emanuele
    - family: Scheidegger
      given: Carlos
    - family: Schuchardt
      given: Karen
    - family: Seltzer
      given: Margo
    - family: Simmhan
      given: Yogesh L.
    - family: Silva
      given: Claudio
    - family: Slaughter
      given: Peter
    - family: Stephan
      given: Eric
    - family: Stevens
      given: Robert
    - family: Turi
      given: Daniele
    - family: Vo
      given: Huy
    - family: Wilde
      given: Mike
    - family: Zhao
      given: Jun
    - family: Zhao
      given: Yong
  citation-key: moreauSpecialIssueFirst2008
  container-title: 'Concurrency and Computation: Practice and Experience'
  container-title-short: 'Concurrency Computat.: Pract. Exper.'
  DOI: 10.1002/cpe.1233
  ISSN: 15320626, 15320634
  issue: '5'
  issued:
    - year: 2008
      month: 4
      day: 10
  language: en
  note: 'interest: 87'
  page: 409-418
  source: DOI.org (Crossref)
  title: 'Special Issue: The First Provenance Challenge'
  title-short: Special Issue
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/10.1002/cpe.1233
  volume: '20'

- id: morenoSpeedingLargeScaleFinancial2014
  abstract: >-
    Quantitative financial analysis requires repeated computations of the same
    functions with the same arguments when prototyping trading strategies; many
    of these functions involve resource intensive operations on large matrices.
    Reducing the number of repeated computations either within a program or
    across runs of the same program would allow analysts to build and debug
    trading strategies more quickly. We built a disk memoization library that
    caches function computations to files to avoid recomputation. Anymemoization
    solution should be easy to use, minimizing the need for users to think about
    whether caching is appropriate, while at the same time giving them control
    over speed, accuracy, and space used for caching. Guo and Engler proposed a
    similar tool that does automatic memoization by modifying the python
    interpreter, while the packages Jug and Joblib are distributed computing
    tools that have memoization options. Our library attempts to maintain the
    ease of use of the above packages for memoization, but at the same time give
    a higher degree of control of how caching is done for users who need it. We
    provide the same basic features as these other libraries, but allow control
    of how hashing is done, space usage for individual functions and all
    memoization, refreshing memoization for a specific function, and accuracy
    checking. This should lead to both increased productivity and speed
    increases for recomputation. We show that for several financial
    calculations, including Markowitz Optimization, Fama French, and the
    Singular Value Decomposition, memoization greatly speeds up recomputation,
    often by over 99%. We also show that by using xxhash, a non-cryptographic
    hash function, instead of md5, and avoiding equality checks, our package
    greatly outperforms joblib, the best current package.
  author:
    - family: Moreno
      given: Alexander
    - family: Balch
      given: Tucker
  citation-key: morenoSpeedingLargeScaleFinancial2014
  container-title: 2014 Seventh Workshop on High Performance Computational Finance
  DOI: 10.1109/WHPCF.2014.9
  event-title: 2014 Seventh Workshop on High Performance Computational Finance
  issued:
    - year: 2014
      month: 11
  page: 17-22
  source: IEEE Xplore
  title: Speeding up Large-Scale Financial Recomputation with Memoization
  type: paper-conference

- id: mukherjeeFixingDependencyErrors2021
  abstract: >-
    Software reproducibility is important for re-usability and the cumulative
    progress of research. An important manifestation of unreproducible software
    is the changed outcome of software builds over time. While enhancing code
    reuse, the use of open-source dependency packages hosted on centralized
    repositories such as PyPI can have adverse effects on build reproducibility.
    Frequent updates to these packages often cause their latest versions to have
    breaking changes for applications using them. Large Python applications risk
    their historical builds becoming unreproducible due to the widespread usage
    of Python dependencies, and the lack of uniform practices for dependency
    version specification. Manually fixing dependency errors requires expensive
    developer time and effort, while automated approaches face challenges of
    parsing unstructured build logs, finding transitive dependencies, and
    exploring an exponential search space of dependency versions. In this paper,
    we investigate how open-source Python projects specify dependency versions,
    and how their reproducibility is impacted by dependency packages. We propose
    a tool PyDFix to detect and fix unreproducibility in Python builds caused by
    dependency errors. PyDFix is evaluated on two bug datasets BugSwarm and
    BugsInPy, both of which are built from real-world open-source projects.
    PyDFix analyzes a total of 2,702 builds, identifying 1,921 (71.1%) of them
    to be unreproducible due to dependency errors. From these, PyDFix provides a
    complete fix for 859 (44.7%) builds, and partial fixes for an additional 632
    (32.9%) builds.
  accessed:
    - year: 2023
      month: 7
      day: 3
  author:
    - family: Mukherjee
      given: Suchita
    - family: Almanza
      given: Abigail
    - family: Rubio-González
      given: Cindy
  citation-key: mukherjeeFixingDependencyErrors2021
  collection-title: ISSTA 2021
  container-title: >-
    Proceedings of the 30th ACM SIGSOFT International Symposium on Software
    Testing and Analysis
  DOI: 10.1145/3460319.3464797
  event-place: New York, NY, USA
  ISBN: 978-1-4503-8459-9
  issued:
    - year: 2021
      month: 7
      day: 11
  note: 'interest: 98'
  page: 439–451
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Fixing dependency errors for Python build reproducibility
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3460319.3464797

- id: mulderCanSanctionsStop2022
  abstract: >-
    Nicholas Mulder, the author of a new book on the history of sanctions,
    explains the West’s use of the “economic weapon.”
  accessed:
    - year: 2022
      month: 4
      day: 13
  author:
    - family: Mulder
      given: Nicholas
  citation-key: mulderCanSanctionsStop2022
  container-title: The Atlantic
  contributor:
    - family: Lowrey
      given: Annie
  issued:
    - year: 2022
      month: 3
      day: 10
  language: en
  section: Ideas
  title: Can Sanctions Stop Russia?
  type: article-magazine
  URL: >-
    https://www.theatlantic.com/ideas/archive/2022/03/russia-sanctions-economic-policy-effects/627009/

- id: munafoManifestoReproducibleScience2017
  abstract: >-
    Improving the reliability and efficiency of scientific research will
    increase the credibility of the published scientific literature and
    accelerate discovery. Here we argue for the adoption of measures to optimize
    key elements of the scientific process: methods, reporting and
    dissemination, reproducibility, evaluation and incentives. There is some
    evidence from both simulations and empirical studies supporting the likely
    effectiveness of these measures, but their broad adoption by researchers,
    institutions, funders and journals will require iterative evaluation and
    improvement. We discuss the goals of these measures, and how they can be
    implemented, in the hope that this will facilitate action toward improving
    the transparency, reproducibility and efficiency of scientific research.
  accessed:
    - year: 2022
      month: 9
      day: 27
  author:
    - family: Munafò
      given: Marcus R.
    - family: Nosek
      given: Brian A.
    - family: Bishop
      given: Dorothy V. M.
    - family: Button
      given: Katherine S.
    - family: Chambers
      given: Christopher D.
    - family: Percie du Sert
      given: Nathalie
    - family: Simonsohn
      given: Uri
    - family: Wagenmakers
      given: Eric-Jan
    - family: Ware
      given: Jennifer J.
    - family: Ioannidis
      given: John P. A.
  citation-key: munafoManifestoReproducibleScience2017
  container-title: Nature Human Behaviour
  container-title-short: Nat Hum Behav
  DOI: 10.1038/s41562-016-0021
  ISSN: 2397-3374
  issue: '1'
  issued:
    - year: 2017
      month: 1
      day: 10
  language: en
  license: 2017 Macmillan Publishers Limited
  note: 'interest: 89'
  number: '1'
  page: 1-9
  publisher: Nature Publishing Group
  source: www.nature.com
  title: A manifesto for reproducible science
  type: article-journal
  URL: https://www.nature.com/articles/s41562-016-0021
  volume: '1'

- id: munatLowercaseSemanticWeb
  abstract: >-
    Despite the enthusiasm of certain technological utopians, building the
    Semantic Web is a hard problem. Capturing true, incontrovertible meaning of
    natural language remains an elusive goal. Also, the widespread adoption of
    complex data modeling languages like RDF or OWL is unlikely to happen
    overnight. A more feasible approach would be to focus first on specific
    domains and use the simpler technologies. XML and XHTML are two relatively
    simple technologies already on hand and can be used to lend a little more
    semantic value to the Web's existing content.
  author:
    - family: Munat
      given: Ben
  citation-key: munatLowercaseSemanticWeb
  language: en
  source: Zotero
  title: The Lowercase Semantic Web
  type: article-journal

- id: muniswamy-reddyLayeringProvenanceSystems2009
  abstract: >-
    Digital provenance describes the ancestry or history of a digital object.
    Most existing provenance systems, however, operate at only one level of
    abstraction: the system call layer, a workflow specification, or the
    high-level constructs of a particular application. The provenance
    collectable in each of these layers is different, and all of it can be
    important. Single-layer systems fail to account for the different levels of
    abstraction at which users need to reason about their data and processes.
    These systems cannot integrate data provenance across layers and cannot
    answer questions that require an integrated view of the provenance. We have
    designed a provenance collection structure facilitating the integration of
    provenance across multiple levels of abstraction, including a workflow
    engine, a web browser, and an initial runtime Python provenance tracking
    wrapper. We layer these components atop provenance-aware network storage
    (NFS) that builds upon a Provenance-Aware Storage System (PASS). We discuss
    the challenges of building systems that integrate provenance across multiple
    layers of abstraction, present how we augmented systems in each layer to
    integrate provenance, and present use cases that demonstrate how provenance
    spanning multiple layers provides functionality not available in existing
    systems. Our evaluation shows that the overheads imposed by layering
    provenance systems are reasonable.
  accessed:
    - year: 2023
      month: 7
      day: 18
  author:
    - family: Muniswamy-Reddy
      given: Kiran-Kumar
    - family: Braun
      given: Uri
    - family: Holland
      given: David A.
    - family: Macko
      given: Peter
    - family: Maclean
      given: Diana
    - family: Margo
      given: Daniel
    - family: Seltzer
      given: Margo
    - family: Smogor
      given: Robin
  citation-key: muniswamy-reddyLayeringProvenanceSystems2009
  collection-title: USENIX'09
  container-title: Proceedings of the 2009 conference on USENIX Annual technical conference
  DOI: 10.5555/1855807.1855817
  event-place: USA
  issued:
    - year: 2009
      month: 6
      day: 14
  language: en
  page: '10'
  publisher: USENIX Association
  publisher-place: USA
  source: ACM Digital Library
  title: Layering in provenance systems
  type: paper-conference

- id: muniswamy-reddyProvenanceAwareStorageSystems2006
  abstract: >-
    A Provenance-Aware Storage System (PASS) is a storage system that
    automatically collects and maintains provenance or lineage, the complete
    history or ancestry of an item. We discuss the advantages of treating
    provenance as meta-data collected and maintained by the storage system,
    rather than as manual annotations stored in a separately administered
    database. We describe a PASS implementation, discussing the challenges it
    presents, performance cost it incurs, and the new functionality it enables.
    We show that with reasonable overhead, we can provide useful functionality
    not available in today’s ﬁle systems or provenance management systems.
  author:
    - family: Muniswamy-Reddy
      given: Kiran-Kumar
    - family: Holland
      given: David A
    - family: Braun
      given: Uri
    - family: Seltzer
      given: Margo
  citation-key: muniswamy-reddyProvenanceAwareStorageSystems2006
  container-title: 2006 USENIX Annual Technical Conference
  event-title: 2006 USENIX Annual Technical Conference
  issued:
    - year: 2006
  language: en_US
  license: open
  note: 'Accepted: 2015-12-07T19:07:43Z'
  source: dash.harvard.edu
  title: Provenance-Aware Storage Systems
  type: paper-conference
  URL: https://dash.harvard.edu/handle/1/23853812

- id: murphy-hillWhatPredictsSoftware2019
  abstract: >-
    Organizations have a variety of options to help their software developers
    become their most productive selves, from modifying office layouts, to
    investing in better tools, to cleaning up the source code. But which options
    will have the biggest impact? Drawing from the literature in software
    engineering and industrial/organizational psychology to identify factors
    that correlate with productivity, we designed a survey that asked 622
    developers across 3 companies about these productivity factors and about
    self-rated productivity. Our results suggest that the factors that most
    strongly correlate with self-rated productivity were non-technical factors,
    such as job enthusiasm, peer support for new ideas, and receiving useful
    feedback about job performance. Compared to other knowledge workers, our
    results also suggest that software developers' self-rated productivity is
    more strongly related to task variety and ability to work remotely.
  author:
    - family: Murphy-Hill
      given: Emerson
    - family: Jaspan
      given: Ciera
    - family: Sadowski
      given: Caitlin
    - family: Shepherd
      given: David
    - family: Phillips
      given: Michael
    - family: Winter
      given: Collin
    - family: Knight
      given: Andrea
    - family: Smith
      given: Edward
    - family: Jorde
      given: Matthew
  citation-key: murphy-hillWhatPredictsSoftware2019
  container-title: IEEE Transactions on Software Engineering
  DOI: 10.1109/TSE.2019.2900308
  ISSN: 1939-3520
  issue: '3'
  issued:
    - year: 2019
      month: 2
      day: 19
  note: 'score: 90'
  page: 582-594
  source: IEEE Xplore
  title: What Predicts Software Developers' Productivity?
  type: article-journal
  volume: '47'

- id: murphy31NoobyHabits2021
  abstract: Up your C++ skill by recognizing and ditching these nooby C++ habits.
  accessed:
    - year: 2022
      month: 4
      day: 10
  author:
    - family: Murphy
      given: James
  citation-key: murphy31NoobyHabits2021
  issued:
    - year: 2021
      month: 12
      day: 13
  title: 31 nooby C++ habits you need to ditch
  type: speech
  URL: https://www.youtube.com/watch?v=i_wDa2AS_8w

- id: murtaNoWorkflowCapturingAnalyzing2015
  abstract: >-
    We propose noWorkflow, a tool that transparently captures provenance of
    scripts and enables reproducibility. Unlike existing approaches, noWorkflow
    is non-intrusive and does not require users to change the way they work –
    users need not wrap their experiments in scientific workflow systems,
    install version control systems, or instrument their scripts. The tool
    leverages Software Engineering techniques, such as abstract syntax tree
    analysis, reflection, and profiling, to collect different types of
    provenance, including detailed information about the underlying libraries.
    We describe how noWorkflow captures multiple kinds of provenance and the
    different classes of analyses it supports: graph-based visualization;
    differencing over provenance trails; and inference queries.
  author:
    - family: Murta
      given: Leonardo
    - family: Braganholo
      given: Vanessa
    - family: Chirigati
      given: Fernando
    - family: Koop
      given: David
    - family: Freire
      given: Juliana
  citation-key: murtaNoWorkflowCapturingAnalyzing2015
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-319-16462-5_6
  editor:
    - family: Ludäscher
      given: Bertram
    - family: Plale
      given: Beth
  event-place: Cham
  ISBN: 978-3-319-16462-5
  issued:
    - year: 2015
  language: en
  page: 71-83
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: 'noWorkflow: Capturing and Analyzing Provenance of Scripts'
  title-short: noWorkflow
  type: paper-conference

- id: mytkowiczProducingWrongData2009
  abstract: >-
    This paper presents a surprising result: changing a seemingly innocuous
    aspect of an experimental setup can cause a systems researcher to draw wrong
    conclusions from an experiment. What appears to be an innocuous aspect in
    the experimental setup may in fact introduce a significant bias in an
    evaluation. This phenomenon is called measurement bias in the natural and
    social sciences. Our results demonstrate that measurement bias is
    significant and commonplace in computer system evaluation. By significant we
    mean that measurement bias can lead to a performance analysis that either
    over-states an effect or even yields an incorrect conclusion. By commonplace
    we mean that measurement bias occurs in all architectures that we tried
    (Pentium 4, Core 2, and m5 O3CPU), both compilers that we tried (gcc and
    Intel's C compiler), and most of the SPEC CPU2006 C programs. Thus, we
    cannot ignore measurement bias. Nevertheless, in a literature survey of 133
    recent papers from ASPLOS, PACT, PLDI, and CGO, we determined that none of
    the papers with experimental results adequately consider measurement bias.
    Inspired by similar problems and their solutions in other sciences, we
    describe and demonstrate two methods, one for detecting (causal analysis)
    and one for avoiding (setup randomization) measurement bias.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Mytkowicz
      given: Todd
    - family: Diwan
      given: Amer
    - family: Hauswirth
      given: Matthias
    - family: Sweeney
      given: Peter F.
  citation-key: mytkowiczProducingWrongData2009
  container-title: ACM SIGARCH Computer Architecture News
  container-title-short: SIGARCH Comput. Archit. News
  DOI: 10.1145/2528521.1508275
  ISSN: 0163-5964
  issue: '1'
  issued:
    - year: 2009
      month: 3
      day: 7
  note: |-
    score: 95
    interest: 70
  page: 265–276
  source: March 2009
  title: Producing wrong data without doing anything obviously wrong!
  type: article-journal
  URL: https://doi.org/10.1145/2528521.1508275
  volume: '37'

- id: nakamuraProvenancebasedWorkflowDiagnostics2022
  abstract: >-
    Workflow management systems (WMS) help automate and coordinate scientific
    modules and monitor their execution. WMSes are also used to repeat a
    workflow application with different inputs to test sensitivity and
    reproducibility of runs. However, when differences arise in outputs across
    runs, current WMSes do not audit sufficient provenance metadata to determine
    where the execution first differed. This increases diagnostic time and leads
    to poor quality diagnostic results. In this paper, we use program
    specification to precisely determine locations where workflow execution
    differs. We use existing provenance audited to isolate modules where
    execution differs. We show that using program specification comes at some
    increased storage overhead due to mapping of provenance data flows onto
    program specification, but leads to better quality diagnostics in terms of
    the number of differences found and their location relative to comparing
    provenance metadata audited within current WMSes.
  accessed:
    - year: 2023
      month: 11
      day: 14
  author:
    - family: Nakamura
      given: Yuta
    - family: Malik
      given: Tanu
    - family: Kanj
      given: Iyad
    - family: Gehani
      given: Ashish
  citation-key: nakamuraProvenancebasedWorkflowDiagnostics2022
  container-title: >-
    2022 IEEE 29th International Conference on High Performance Computing, Data,
    and Analytics (HiPC)
  DOI: 10.1109/HiPC56025.2022.00046
  event-title: >-
    2022 IEEE 29th International Conference on High Performance Computing, Data,
    and Analytics (HiPC)
  ISSN: 2640-0316
  issued:
    - year: 2022
      month: 12
  page: 292-301
  source: IEEE Xplore
  title: Provenance-based Workflow Diagnostics Using Program Specification
  type: paper-conference
  URL: >-
    https://ieeexplore.ieee.org/abstract/document/10106315?casa_token=WExXw0DqtskAAAAA:Es6Z26rbhv-4-J6_3TEOcEIFNw_F8FwLKAeP7CB_XUI8Ey5Lwnke6oUroqQR7WOlqF7VHn6x

- id: namikiMethodConstructingResearch2023
  abstract: >-
    Research must be reproducible to be verifiable. Provenance, which describes
    how data was produced, is one of the metadata that can improve
    reproducibility. In this paper, we propose a method to construct the
    provenance of research data produced in high-performance computing (HPC)
    systems. Our method can construct a high-level and user-perspective
    provenance by integrating information available in HPC systems, such as a
    workload manager, with low-level data about running programs' behavior
    captured in an operating system kernel. The method enables users of HPC
    systems to collect the provenance without modifying assets such as programs
    and scripts.
  accessed:
    - year: 2024
      month: 2
      day: 14
  author:
    - family: Namiki
      given: Yuta
    - family: Hosomi
      given: Takeo
    - family: Tanushi
      given: Hideyuki
    - family: Yamashita
      given: Akihiro
    - family: Date
      given: Susumu
  citation-key: namikiMethodConstructingResearch2023
  container-title: 2023 IEEE 19th International Conference on e-Science (e-Science)
  DOI: 10.1109/e-Science58273.2023.10254932
  event-title: 2023 IEEE 19th International Conference on e-Science (e-Science)
  ISSN: 2325-3703
  issued:
    - year: 2023
      month: 10
  page: 1-10
  source: IEEE Xplore
  title: >-
    A Method for Constructing Research Data Provenance in High-Performance
    Computing Systems
  type: paper-conference
  URL: >-
    https://ieeexplore.ieee.org/abstract/document/10254932?casa_token=pKmzTDmrxt0AAAAA:4_U3R82DbSFp5YSbx1tUWr86cMposZ_A-T4vbGHui1UR2Z9YRiFKeil9wQUDGhX3rlb3fb-h

- id: nathanyorkToolsContinuousIntegration2010
  abstract: >-
    Software engineers rarely invoke compilers and lower-level tools directly.
    Instead they interact with a build system which analyzes dependency
    information and then orchestrates the overall build process. Yet build
    systems are often overlooked by industry and academia. This presents a
    challenge for large organizations as their code base grows and engineering
    processes struggle to keep up. This talk covers the key insights and
    technical design elements that enable us to scale the word's largest
    continuously integrated code base to thousands of engineers worldwide.
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - literal: Nathan York
  citation-key: nathanyorkToolsContinuousIntegration2010
  issued:
    - year: 2010
      month: 10
      day: 27
  note: 'score: 50'
  title: Tools for Continuous Integration at Google Scale
  type: speech
  URL: https://www.youtube.com/watch?v=b52aXZ2yi08

- id: naugleGroundTruthProgram2022
  abstract: >-
    Social systems are uniquely complex and difficult to study, but
    understanding them is vital to solving the world’s problems. The Ground
    Truth program developed a new way of testing the research methods that
    attempt to understand and leverage the Human Domain and its associated
    complexities. The program developed simulations of social systems as virtual
    world test beds. Not only were these simulations able to produce data on
    future states of the system under various circumstances and scenarios, but
    their causal ground truth was also explicitly known. Research teams studied
    these virtual worlds, facilitating deep validation of causal inference,
    prediction, and prescription methods. The Ground Truth program model
    provides a way to test and validate research methods to an extent previously
    impossible, and to study the intricacies and interactions of different
    components of research.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Naugle
      given: Asmeret
    - family: Russell
      given: Adam
    - family: Lakkaraju
      given: Kiran
    - family: Swiler
      given: Laura
    - family: Verzi
      given: Stephen
    - family: Romero
      given: Vicente
  citation-key: naugleGroundTruthProgram2022
  container-title: Computational and Mathematical Organization Theory
  container-title-short: Comput Math Organ Theory
  DOI: 10.1007/s10588-021-09346-9
  ISSN: 1381-298X, 1572-9346
  issued:
    - year: 2022
      month: 4
      day: 18
  language: en
  note: 'interest: 50'
  source: DOI.org (Crossref)
  title: >-
    The Ground Truth program: simulations as test beds for social science
    research methods
  title-short: The Ground Truth program
  type: article-journal
  URL: https://link.springer.com/10.1007/s10588-021-09346-9

- id: naugleWhatCanSimulation2022
  abstract: >-
    The ground truth program used simulations as test beds for social science
    research methods. The simulations had known ground truth and were capable of
    producing large amounts of data. This allowed research teams to run
    experiments and ask questions of these simulations similar to social
    scientists studying real-world systems, and enabled robust evaluation of
    their causal inference, prediction, and prescription capabilities. We tested
    three hypotheses about research effectiveness using data from the ground
    truth program, specifically looking at the influence of complexity, causal
    understanding, and data collection on performance. We found some evidence
    that system complexity and causal understanding influenced research
    performance, but no evidence that data availability contributed. The ground
    truth program may be the first robust coupling of simulation test beds with
    an experimental framework capable of teasing out factors that determine the
    success of social science research.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Naugle
      given: Asmeret
    - family: Krofcheck
      given: Daniel
    - family: Warrender
      given: Christina
    - family: Lakkaraju
      given: Kiran
    - family: Swiler
      given: Laura
    - family: Verzi
      given: Stephen
    - family: Emery
      given: Ben
    - family: Murdock
      given: Jaimie
    - family: Bernard
      given: Michael
    - family: Romero
      given: Vicente
  citation-key: naugleWhatCanSimulation2022
  container-title: Computational and Mathematical Organization Theory
  container-title-short: Comput Math Organ Theory
  DOI: 10.1007/s10588-021-09349-6
  ISSN: 1381-298X, 1572-9346
  issued:
    - year: 2022
      month: 4
      day: 30
  language: en
  note: 'interest: 50'
  source: DOI.org (Crossref)
  title: >-
    What can simulation test beds teach us about social science? Results of the
    ground truth program
  title-short: What can simulation test beds teach us about social science?
  type: article-journal
  URL: https://link.springer.com/10.1007/s10588-021-09349-6

- id: navarroleijaReproducibleContainers2020
  abstract: >-
    We describe the design and implementation of DetTrace, a reproducible
    container abstraction for Linux implemented in user space. All computation
    that occurs inside a DetTrace container is a pure function of the initial
    filesystem state of the container. Reproducible containers can be used for a
    variety of purposes, including replication for fault-tolerance, reproducible
    software builds and reproducible data analytics. We use DetTrace to achieve,
    in an automatic fashion, reproducibility for 12,130 Debian package builds,
    containing over 800 million lines of code, as well as bioinformatics and
    machine learning workflows. We show that, while software in each of these
    domains is initially irreproducible, DetTrace brings reproducibility without
    requiring any hardware, OS or application changes. DetTrace's performance is
    dictated by the frequency of system calls: IO-intensive software builds have
    an average overhead of 3.49x, while a compute-bound bioinformatics workflow
    is under 2%.
  accessed:
    - year: 2023
      month: 1
      day: 4
  author:
    - family: Navarro Leija
      given: Omar S.
    - family: Shiptoski
      given: Kelly
    - family: Scott
      given: Ryan G.
    - family: Wang
      given: Baojun
    - family: Renner
      given: Nicholas
    - family: Newton
      given: Ryan R.
    - family: Devietti
      given: Joseph
  citation-key: navarroleijaReproducibleContainers2020
  collection-title: ASPLOS '20
  container-title: >-
    Proceedings of the Twenty-Fifth International Conference on Architectural
    Support for Programming Languages and Operating Systems
  DOI: 10.1145/3373376.3378519
  event-place: New York, NY, USA
  ISBN: 978-1-4503-7102-5
  issued:
    - year: 2020
      month: 3
      day: 13
  page: 167–182
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Reproducible Containers
  type: paper-conference
  URL: https://doi.org/10.1145/3373376.3378519

- id: nelsonEnsuringFreeImmediate2022
  author:
    - family: Nelson
      given: Alondra
  citation-key: nelsonEnsuringFreeImmediate2022
  issued:
    - year: 2022
      month: 8
      day: 25
  publisher: Executive Office of the President, Office of Science and Technology Policy
  title: Ensuring Free, Immediate, and Equitable Access to Federally Funded Research
  type: document

- id: nelsonIllustrisSimulationPublic2015
  abstract: >-
    We present the full public release of all data from the Illustris simulation
    project. Illustris is a suite of large volume, cosmological hydrodynamical
    simulations run with the moving-mesh code Arepo and including a
    comprehensive set of physical models critical for following the formation
    and evolution of galaxies across cosmic time. Each simulates a volume of
    (106.5 Mpc)3 and self-consistently evolves five different types of
    resolution elements from a starting redshift of z=127 to the present day,
    z=0. These components are: dark matter particles, gas cells, passive gas
    tracers, stars and stellar wind particles, and supermassive black holes.
    This data release includes the snapshots at all 136 available redshifts,
    halo and subhalo catalogs at each snapshot, and two distinct merger trees.
    Six primary realizations of the Illustris volume are released, including the
    flagship Illustris-1 run. These include three resolution levels with the
    fiducial “full” baryonic physics model, and a dark matter only analog for
    each. In addition, we provide four distinct, high time resolution, smaller
    volume “subboxes”. The total data volume is ∼265 TB, including ∼800 full
    volume snapshots and ∼30,000 subbox snapshots. We describe the released data
    products as well as tools we have developed for their analysis. All data may
    be directly downloaded in its native HDF5 format. Additionally, we release a
    comprehensive, web-based API which allows programmatic access to search and
    data processing tasks. In both cases we provide example scripts and a
    getting-started guide in several languages: currently, IDL, Python, and
    Matlab. This paper addresses scientific issues relevant for the
    interpretation of the simulations, serves as a pointer to published and
    on-line documentation of the project, describes planned future additional
    data releases, and discusses technical aspects of the release.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Nelson
      given: D.
    - family: Pillepich
      given: A.
    - family: Genel
      given: S.
    - family: Vogelsberger
      given: M.
    - family: Springel
      given: V.
    - family: Torrey
      given: P.
    - family: Rodriguez-Gomez
      given: V.
    - family: Sijacki
      given: D.
    - family: Snyder
      given: G. F.
    - family: Griffen
      given: B.
    - family: Marinacci
      given: F.
    - family: Blecha
      given: L.
    - family: Sales
      given: L.
    - family: Xu
      given: D.
    - family: Hernquist
      given: L.
  citation-key: nelsonIllustrisSimulationPublic2015
  container-title: Astronomy and Computing
  container-title-short: Astronomy and Computing
  DOI: 10.1016/j.ascom.2015.09.003
  ISSN: 2213-1337
  issued:
    - year: 2015
      month: 11
      day: 1
  language: en
  note: 'interest: 70'
  page: 12-37
  source: ScienceDirect
  title: 'The illustris simulation: Public data release'
  title-short: The illustris simulation
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S2213133715000864
  volume: '13'

- id: nelsonLetPublishFewer2012
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Nelson
      given: Leif D.
    - family: Simmons
      given: Joseph P.
    - family: Simonsohn
      given: Uri
  citation-key: nelsonLetPublishFewer2012
  container-title: Psychological Inquiry
  DOI: 10.1080/1047840X.2012.705245
  ISSN: 1047-840X
  issue: '3'
  issued:
    - year: 2012
      month: 7
      day: 1
  note: 'interest: 86'
  page: 291-293
  publisher: Routledge
  source: Taylor and Francis+NEJM
  title: Let's Publish Fewer Papers
  type: article-journal
  URL: https://doi.org/10.1080/1047840X.2012.705245
  volume: '23'

- id: neupaneCharacterizationLeptazolinesPolar2019
  abstract: >-
    The bioactivity-guided examination of a Leptolyngbya sp. led to the
    isolation of leptazolines A–D (1–4), from the culture media, along with two
    degradation products (5 and 6). Density functional theory nuclear magnetic
    resonance calculations established the relative configurations of 1 and 2
    and revealed that the calculated shifts depended on the operating system
    when using the “Willoughby–Hoye” Python scripts to streamline the processing
    of the output files, a previously unrecognized flaw that could lead to
    incorrect conclusions.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Neupane
      given: Jayanti Bhandari
    - family: Neupane
      given: Ram P.
    - family: Luo
      given: Yuheng
    - family: Yoshida
      given: Wesley Y.
    - family: Sun
      given: Rui
    - family: Williams
      given: Philip G.
  citation-key: neupaneCharacterizationLeptazolinesPolar2019
  container-title: Organic Letters
  container-title-short: Org. Lett.
  DOI: 10.1021/acs.orglett.9b03216
  ISSN: 1523-7060, 1523-7052
  issue: '20'
  issued:
    - year: 2019
      month: 10
      day: 18
  language: en
  page: 8449-8453
  source: DOI.org (Crossref)
  title: >-
    Characterization of Leptazolines A–D, Polar Oxazolines from the
    Cyanobacterium Leptolyngbya sp., Reveals a Glitch with the “Willoughby–Hoye”
    Scripts for Calculating NMR Chemical Shifts
  type: article-journal
  URL: https://pubs.acs.org/doi/10.1021/acs.orglett.9b03216
  volume: '21'

- id: newmanMyExperimentOntologyEResearch2009
  abstract: >-
    myExperiment describes itself as a "Social Virtual Research Environment"
    that provides the ability to share Research Objects (ROs) over a social
    infrastructure to facilitate actioning of research. The myExperiment
    Ontology is a logical representation of the data model used by this
    environment, allowing its data to be published in a standard RDF format,
    whilst providing a generic extensible framework that can be reused by
    similar projects. ROs are data structures designed to semantically enhance
    research publications by capturing and preserving the research method so
    that it can be reproduced in the future. This paper provides some motivation
    for an RO specification and briefly considers how existing domain-specifific
    ontologies might be integrated. It concludes by discussing the future
    direction of the myExperiment Ontology and how it will best support these
    ROs.
  author:
    - family: Newman
      given: David
    - family: Bechhofer
      given: Sean
    - family: De Roure
      given: David
  citation-key: newmanMyExperimentOntologyEResearch2009
  container-title: Semantic Web Applications in Scientific Discourse
  event-place: Washington, D.C., United States
  event-title: Semantic Web Applications in Scientific Discourse
  issued:
    - year: 2009
      month: 10
      day: 24
  publisher: Web & Internet Science
  publisher-place: Washington, D.C., United States
  title: 'myExperiment: An ontology for e-Research'
  type: paper-conference
  URL: https://eprints.soton.ac.uk/267787/

- id: nigrovicLymeVaccineCautionary2007
  abstract: >-
    People living in endemic areas acquire Lyme disease from the bite of an
    infected tick. This infection, when diagnosed and treated early in its
    course, usually responds well to antibiotic therapy. A minority of patients
    develops more serious disease, particularly after a delay in diagnosis or
    therapy, and sometimes chronic neurological, cardiac, or rheumatological
    manifestations. In 1998, the FDA approved a new recombinant Lyme vaccine,
    LYMErix™, which reduced new infections in vaccinated adults by nearly 80%.
    Just 3 years later, the manufacturer voluntarily withdrew its product from
    the market amidst media coverage, fears of vaccine side-effects, and
    declining sales. This paper reviews these events in detail and focuses on
    the public communication of risks and benefits of the Lyme vaccine and
    important lessons learned.
  accessed:
    - year: 2022
      month: 8
      day: 26
  author:
    - family: Nigrovic
      given: L. E.
    - family: Thompson
      given: K. M.
  citation-key: nigrovicLymeVaccineCautionary2007
  container-title: Epidemiology and Infection
  container-title-short: Epidemiol Infect
  DOI: 10.1017/S0950268806007096
  ISSN: 0950-2688
  issue: '1'
  issued:
    - year: 2007
      month: 1
  page: 1-8
  PMCID: PMC2870557
  PMID: '16893489'
  source: PubMed Central
  title: 'The Lyme vaccine: a cautionary tale'
  title-short: The Lyme vaccine
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2870557/
  volume: '135'

- id: nikolicMinimalRecomputationExploratory2018
  abstract: >-
    We present a technique to automatically minimise the re-computation when a
    data analysis program is iteratively changed, or added to, as is often the
    case in exploratory data analysis in astronomy. A typical example is
    flagging and calibration of demanding or unusual observations where visual
    inspection suggests improvement to the processing strategy. The technique is
    based on memoization and referentially transparent tasks. We describe the
    implementation of this technique for the CASA radio astronomy data reduction
    package. We also propose a technique for optimising efficiency of storage of
    memoized intermediate data products using copy-on-write and block level
    de-duplication and measure their practical efficiency. We find that the
    minimal recomputation technique improves the efficiency of data analysis
    while reducing the possibility for user error and improving the
    reproducibility of the final result. It also aids exploratory data analysis
    on batch-schedule cluster computer systems.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Nikolic
      given: B.
    - family: Small
      given: D.
    - family: Kettenis
      given: M.
  citation-key: nikolicMinimalRecomputationExploratory2018
  container-title: Astronomy and Computing
  container-title-short: Astronomy and Computing
  DOI: 10.1016/j.ascom.2018.09.003
  ISSN: 2213-1337
  issued:
    - year: 2018
      month: 10
      day: 1
  language: en
  note: 'interest: 95'
  page: 133-138
  source: ScienceDirect
  title: Minimal re-computation for exploratory data analysis in astronomy
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S2213133718300246
  volume: '25'

- id: nissenWhatBadJulia2023
  abstract: >-
    Julia is my favorite programming language. More than that actually, perhaps
    I'm a bit of a fanboy. Sometimes, though, the ceaseless celebration of Julia
    by fans like me can be a bit too much. It papers over legitimate problems in
    the language, hindering progress. And from an outsider perspective, it's not
    only insufferable (I would guess), but also obfuscates the true pros and
    cons of the language. Learning why you may not want to choose to use a tool
    is just as important as learning why you may.


    This post is about all the major disadvantages of Julia. Some of it will
    just be rants about things I particularly don't like - hopefully they will
    be informative, too. A post like this is necessarily subjective. For
    example, some people believe Julia's lack of a Java-esque OOP is a design
    mistake. I don't, so the post won't go into that.
  accessed:
    - year: 2023
      month: 3
      day: 9
  author:
    - family: Nissen
      given: Jakob Nybo
  citation-key: nissenWhatBadJulia2023
  container-title: virtualinstruction
  issued:
    - year: 2023
      month: 2
      day: 24
  title: What's bad about Julia?
  type: post-weblog
  URL: https://viralinstruction.com/posts/badjulia/

- id: nissenWhatGreatJulia2023
  abstract: >-
    The first post on this blog was "What's bad about Julia" - a collection of
    the worst things about my favourite language, which turned out to be quite
    the Hacker News bait. The most common responses I got was along the lines
    of: "If Julia has all these flaws, why not just use another language?". At
    the time, I just said that despite its flaws, Julia was still amazing, that
    it would take another 4,000 word post to elaborate on why, and then I left
    it at that.


    Recently I've been thinking a lot about one of Julia's major drawbacks, and
    have been drafting up a post that goes in depth about the subject. But
    honestly, posting another verbose criticism of Julia would risk giving a
    misleadingly bad impression of my experience with the lovely language, even
    if I bracket a wall of criticism with a quick endorsement. After all, I've
    chosen to use the language for my daily work about two years ago, and I
    don't regret that choice in the slightest.


    Now is the right time for that 4,000 word post on the best parts of Julia.
  accessed:
    - year: 2023
      month: 3
      day: 9
  author:
    - family: Nissen
      given: Jakob Nybo
  citation-key: nissenWhatGreatJulia2023
  container-title: virtualinstruction
  issued:
    - year: 2023
      month: 2
      day: 24
  title: What's great about Julia?
  type: post-weblog
  URL: https://viralinstruction.com/posts/goodjulia/

- id: noonanGhostsDepartedProofs2018
  abstract: >-
    Library authors often are faced with a design choice: should a function with
    preconditions be implemented as a partial function, or by returning a
    failure condition on incorrect use? Neither option is ideal. Partial
    functions lead to frustrating run-time errors. Failure conditions must be
    checked at the use-site, placing an unfair tax on the users who have ensured
    that the function's preconditions were correctly met. In this paper, we
    introduce an API design concept called ``ghosts of departed proofs'' based
    on the following observation: sophisticated preconditions can be encoded in
    Haskell's type system with no run-time overhead, by using proofs that
    inhabit phantom type parameters attached to newtype wrappers. The user
    expresses correctness arguments by constructing proofs to inhabit these
    phantom types. Critically, this technique allows the library user to decide
    when and how to validate that the API's preconditions are met. The ``ghosts
    of departed proofs'' approach to API design can achieve many of the benefits
    of dependent types and refinement types, yet only requires some minor and
    well-understood extensions to Haskell 2010. We demonstrate the utility of
    this approach through a series of case studies, showing how to enforce novel
    invariants for lists, maps, graphs, shared memory regions, and more.
  accessed:
    - year: 2024
      month: 1
      day: 11
  author:
    - family: Noonan
      given: Matt
  citation-key: noonanGhostsDepartedProofs2018
  collection-title: Haskell 2018
  container-title: Proceedings of the 11th ACM SIGPLAN International Symposium on Haskell
  DOI: 10.1145/3242744.3242755
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5835-4
  issued:
    - year: 2018
      month: 9
      day: 17
  page: 119–131
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Ghosts of departed proofs (functional pearl)
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3242744.3242755

- id: norgaardWhatUsabilityEvaluators2006
  abstract: >-
    Think-aloud testing is a widely employed usability evaluation method, yet
    its use in practice is rarely studied. We report an explorative study of 14
    think-aloud sessions, the audio recordings of which were examined in detail.
    The study shows that immediate analysis of observations made in the
    think-aloud sessions is done only sporadically, if at all. When testing,
    evaluators seem to seek confirmation of problems that they are already aware
    of. During testing, evaluators often ask users about their expectations and
    about hypothetical situations, rather than about experienced problems. In
    addition, evaluators learn much about the usability of the tested system but
    little about its utility. The study shows how practical realities rarely
    discussed in the literature on usability evaluation influence sessions. We
    discuss implications for usability researchers and professionals, including
    techniques for fast-paced analysis and tools for capturing observations
    during sessions.
  accessed:
    - year: 2022
      month: 6
      day: 13
  author:
    - family: Nørgaard
      given: Mie
    - family: Hornbæk
      given: Kasper
  citation-key: norgaardWhatUsabilityEvaluators2006
  container-title: >-
    Proceedings of the 6th ACM conference on Designing Interactive systems  -
    DIS '06
  DOI: 10.1145/1142405.1142439
  event-place: University Park, PA, USA
  event-title: the 6th ACM conference
  ISBN: 978-1-59593-367-6
  issued:
    - year: 2006
  language: en
  note: 'interest: 80'
  page: '209'
  publisher: ACM Press
  publisher-place: University Park, PA, USA
  source: DOI.org (Crossref)
  title: >-
    What do usability evaluators do in practice?: an explorative study of
    think-aloud testing
  title-short: What do usability evaluators do in practice?
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=1142405.1142439

- id: nosekStrategyCultureChange
  abstract: Strategy for Culture Change
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Nosek
      given: Brian
  citation-key: nosekStrategyCultureChange
  language: en
  note: 'interest: 89'
  title: Strategy for Culture Change
  type: post-weblog
  URL: https://www.cos.io/blog/strategy-for-culture-change

- id: nowakowskiCollageAuthoringEnvironment2011
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Nowakowski
      given: Piotr
    - family: Ciepiela
      given: Eryk
    - family: Harężlak
      given: Daniel
    - family: Kocot
      given: Joanna
    - family: Kasztelnik
      given: Marek
    - family: Bartyński
      given: Tomasz
    - family: Meizner
      given: Jan
    - family: Dyk
      given: Grzegorz
    - family: Malawski
      given: Maciej
  citation-key: nowakowskiCollageAuthoringEnvironment2011
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2011.04.064
  ISSN: '18770509'
  issued:
    - year: 2011
  language: en
  license: https://www.elsevier.com/tdm/userlicense/1.0/
  page: 608-617
  source: DOI.org (Crossref)
  title: The Collage Authoring Environment
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1877050911001220
  volume: '4'

- id: nuzzoHowScientistsFool2015
  abstract: >-
    Humans are remarkably good at self-deception. But growing concern about
    reproducibility is driving many researchers to seek ways to fight their own
    worst instincts.
  accessed:
    - year: 2022
      month: 9
      day: 27
  author:
    - family: Nuzzo
      given: Regina
  citation-key: nuzzoHowScientistsFool2015
  container-title: Nature
  DOI: 10.1038/526182a
  ISSN: 1476-4687
  issue: '7572'
  issued:
    - year: 2015
      month: 10
      day: 1
  language: en
  license: 2015 Nature Publishing Group
  note: 'interest: 74'
  number: '7572'
  page: 182-185
  publisher: Nature Publishing Group
  source: www.nature.com
  title: How scientists fool themselves – and how they can stop
  type: article-journal
  URL: https://www.nature.com/articles/526182a
  volume: '526'

- id: nuzzoScientificMethodStatistical2014
  abstract: >-
    P values, the 'gold standard' of statistical validity, are not as reliable
    as many scientists assume.
  accessed:
    - year: 2022
      month: 7
      day: 28
  author:
    - family: Nuzzo
      given: Regina
  citation-key: nuzzoScientificMethodStatistical2014
  container-title: Nature
  container-title-short: Nature
  DOI: 10.1038/506150a
  ISSN: 0028-0836, 1476-4687
  issue: '7487'
  issued:
    - year: 2014
      month: 2
      day: 13
  language: en
  license: 2014 Nature Publishing Group
  note: 'interest: 82'
  page: 150-152
  source: www.nature.com
  title: 'Scientific method: Statistical errors'
  title-short: Scientific method
  type: article-journal
  URL: https://www.nature.com/articles/506150a
  volume: '506'

- id: ocallahanEngineeringRecordReplay2017
  abstract: >-
    The ability to record and replay program executions with low overhead
    enables many applications, such as reverse-execution debugging, debugging of
    hard-to-reproduce test failures, and "black box" forensic analysis of
    failures in deployed systems. Existing record-and-replay approaches limit
    deployability by recording an entire virtual machine (heavyweight),
    modifying the OS kernel (adding deployment and maintenance costs), requiring
    pervasive code instrumentation (imposing significant performance and
    complexity overhead), or modifying compilers and runtime systems (limiting
    generality). We investigated whether it is possible to build a practical
    record-and-replay system avoiding all these issues. The answer turns out to
    be yes - if the CPU and operating system meet certain non-obvious
    constraints. Fortunately modern Intel CPUs, Linux kernels and user-space
    frameworks do meet these constraints, although this has only become true
    recently. With some novel optimizations, our system 'rr' records and replays
    real-world low-parallelism workloads with low overhead, with an entirely
    user-space implementation, using stock hardware, compilers, runtimes and
    operating systems. "rr" forms the basis of an open-source reverse-execution
    debugger seeing significant use in practice. We present the design and
    implementation of 'rr', describe its performance on a variety of workloads,
    and identify constraints on hardware and operating system design required to
    support our approach.
  accessed:
    - year: 2024
      month: 1
      day: 26
  author:
    - family: O'Callahan
      given: Robert
    - family: Jones
      given: Chris
    - family: Froyd
      given: Nathan
    - family: Huey
      given: Kyle
    - family: Noll
      given: Albert
    - family: Partush
      given: Nimrod
  citation-key: ocallahanEngineeringRecordReplay2017
  DOI: 10.48550/arXiv.1705.05937
  issued:
    - year: 2017
      month: 5
      day: 16
  number: arXiv:1705.05937
  publisher: arXiv
  source: arXiv.org
  title: 'Engineering Record And Replay For Deployability: Extended Technical Report'
  title-short: Engineering Record And Replay For Deployability
  type: article
  URL: http://arxiv.org/abs/1705.05937

- id: ocallahanRrdebuggerRelatedWork
  author:
    - family: O'Callahan
      given: Robert
  citation-key: ocallahanRrdebuggerRelatedWork
  title: 'rr-debugger: Related work'
  type: webpage
  URL: https://github.com/rr-debugger/rr/wiki/Related-work

- id: oconnorDockstoreEnablingModular2017
  abstract: >-
    As genomic datasets continue to grow, the feasibility of downloading data to
    a local organization and running analysis on a traditional compute
    environment is becoming increasingly problematic. Current large-scale
    projects, such as the ICGC PanCancer Analysis of Whole Genomes (PCAWG), the
    Data Platform for the U.S. Precision Medicine Initiative, and the NIH Big
    Data to Knowledge Center for Translational Genomics, are using cloud-based
    infrastructure to both host and perform analysis across large data sets. In
    PCAWG, over 5,800 whole human genomes were aligned and variant called across
    14 cloud and HPC environments; the processed data was then made available on
    the cloud for further analysis and sharing. If run locally, an operation at
    this scale would have monopolized a typical academic data centre for many
    months, and would have presented major challenges for data storage and
    distribution. However, this scale is increasingly typical for genomics
    projects and necessitates a rethink of how analytical tools are packaged and
    moved to the data. For PCAWG, we embraced the use of highly portable Docker
    images for encapsulating and sharing complex alignment and variant calling
    workflows across highly variable environments. While successful, this
    endeavor revealed a limitation in Docker containers, namely the lack of a
    standardized way to describe and execute the tools encapsulated inside the
    container. As a result, we created the Dockstore (
                  https://dockstore.org
                  ), a project that brings together Docker images with standardized, machine-readable ways of describing and running the tools contained within. This service greatly improves the sharing and reuse of genomics tools and promotes interoperability with similar projects through emerging web service standards developed by the Global Alliance for Genomics and Health (GA4GH).
  accessed:
    - year: 2022
      month: 10
      day: 31
  author:
    - family: O'Connor
      given: Brian D.
    - family: Yuen
      given: Denis
    - family: Chung
      given: Vincent
    - family: Duncan
      given: Andrew G.
    - family: Liu
      given: Xiang Kun
    - family: Patricia
      given: Janice
    - family: Paten
      given: Benedict
    - family: Stein
      given: Lincoln
    - family: Ferretti
      given: Vincent
  citation-key: oconnorDockstoreEnablingModular2017
  container-title: F1000Research
  container-title-short: F1000Res
  DOI: 10.12688/f1000research.10137.1
  ISSN: 2046-1402
  issued:
    - year: 2017
      month: 1
      day: 18
  language: en
  page: '52'
  source: DOI.org (Crossref)
  title: >-
    The Dockstore: enabling modular, community-focused sharing of Docker-based
    genomics tools and workflows
  title-short: The Dockstore
  type: article-journal
  URL: https://f1000research.com/articles/6-52/v1
  volume: '6'

- id: odenLessonsLearnedComparing2020
  abstract: >-
    Python as programming language is increasingly gaining importance,
    especially in data science, scientific, and parallel programming. It is
    faster and easier to learn than classical programming languages such as C.
    However, usability often comes at the cost of performance and applications
    written in Python are considered to be much slower than applications written
    in C or FORTRAN. Further, it does not allow the usage of GPUs-besides of
    pre-compiled libraries.However, the Numba package promises performance
    similar to C code for compute intensive parts of a Python application and it
    supports CUDA, which allows the use of GPUs inside a Python application.In
    this paper we compare the performance of Numba-CUDA and C -CUDA for
    different kinds of applications. For compute intensive benchmarks, the
    performance of the Numba version only reaches between 50% and 85%
    performance of the CCUDA version, despite the reduction operation, where the
    Numba version outperforms CUDA. Analyzing the PTX code and CUDA performance
    counters revealed that index-calculation is one limiting factor in Numba.
    Another problem is the type interference for single precision computations,
    as some values are computed in double precision. By optimizing this within
    the Numba package, the performance of Numba improves. However, C-CUDA
    applications still outperform the Numba versions. Further analysis with the
    CloverLeav Mini App shows that Numba performance further decreases for
    applications with multiple different compute kernels. The non-GPU part slows
    down these applications, due to the slow Python interpreter. This leads to a
    worse GPU utilization.Today Python is widely used in industry and academia
    and has been the first choice of coding languages among software programmers
    in the last years. Currently, according to the TIOBE index [5], it is the
    3rd most popular programming language and the number one in IEEE Spectrum's
    fifth annual interactive ranking of the top programming languages [4]. One
    reason for this is that is easier to learn than classical programming
    languages like C. However, the other reason is the increasing popularity of
    Data Science, where Python is the most used language. A collection of
    libraries such as NumPy [22], and Matplotlib [1] or Scipy [8] provide a rich
    set of functions for scientific computing [16]. Packages like Dask [19],
    PyCompss [21] and MPI for Python [6] allow running Python applications on
    large, parallel machines, promising high performance. However, the
    performance of Python is considered slow compared to compiled languages such
    as C, C++, and FORTRAN, especially for heavy computations. In recent years,
    more and more tools have been developed to counter this prejudice. Numpy
    [22], for example, uses C-like arrays to store data and offers fast
    functions implemented in C to speed up calculations. The CuPy [14] package
    provides a similar set of functions, but these functions are implemented for
    GPUs using CUDA. The SciPy library is based on NumPy and provides a rich set
    on functionalities for scientific computing. Still, the high performance of
    these libraries is provided by the underling C-implementations. Internally,
    they use libraries like OpenBlas or IntelMKL to reach high performance and
    therefore, they are limited by the functions which are provided by theses
    libraries. Therefore, a performance problem always arises when the required
    functionality is not implemented within these libraries. In this case, the
    application falls back to the Python interpreter. Compared to "bare metal"
    code, interpreted code is slow. In addition, in Python it is not possible to
    use GPUs or other accelerators directly, as the Python interpreter cannot
    execute code on these machines. Therefore, the usage is only possible with
    precompiled libraries. To overcome this limitation, different approaches
    where developed to mix C, CUDA or OpenCL with Python. Cython [2] allows
    integrating C-code in Python applications to improve performance of critical
    sections. It also allows an easy development of wrappers for C-libraries.
    Similar, packages such as PyCuda and PyOpenCL [9] support wrappers for CUDA
    or OpenCL code within a Python script. Both approaches require the mixture
    of different programming languages.Numba [10] follows a different approach.
    Instead of merging C/CUDA code with Python, it allows the development of
    efficient applications for both, CPUs and GPUs in Python style. When a
    Python script using Numba is executed, marked functions are compiled
    just-in-time (JIT) using the LLVM framework. Using Python for GPU
    programming can mean a considerable simplification in the development of
    parallel applications.But often a simplification of comes at the expense of
    performance, and one expects a performance loss from Python compared to pure
    C code. In this paper, we want to understand the differences between native
    C-CUDA code and CUDA-code written in Python with Numba. We also want to
    share some basic tips how to improve the performance of applications written
    in Numba.We will first analyse a few micro benchmarks in detail. We are
    using these simple benchmarks, as it is easier to understand the differences
    with small code examples. We will use the collected information to derive
    some optimization for Numba. Finally, we evaluate and compare the
    performance of a more application like mini-app, written in C-CUDA and Numba
    accelerated Python. We will evaluate if our insights from the
    microbenchmarks to real applications.
  author:
    - family: Oden
      given: Lena
  citation-key: odenLessonsLearnedComparing2020
  container-title: >-
    2020 28th Euromicro International Conference on Parallel, Distributed and
    Network-Based Processing (PDP)
  DOI: 10.1109/PDP50117.2020.00041
  event-title: >-
    2020 28th Euromicro International Conference on Parallel, Distributed and
    Network-Based Processing (PDP)
  ISSN: 2377-5750
  issued:
    - year: 2020
      month: 3
  note: 'interest: 71'
  page: 216-223
  source: IEEE Xplore
  title: Lessons learned from comparing C-CUDA and Python-Numba for GPU-Computing
  type: paper-conference

- id: oliphantPythonScientificComputing2007
  abstract: >-
    Python is an excellent "steering" language for scientific codes written in
    other languages. However, with additional basic tools, Python transforms
    into a high-level language suited for scientific and engineering code that's
    often fast enough to be immediately useful but also flexible enough to be
    sped up with additional extensions.
  accessed:
    - year: 2024
      month: 2
      day: 23
  author:
    - family: Oliphant
      given: Travis E.
  citation-key: oliphantPythonScientificComputing2007
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2007.58
  ISSN: 1558-366X
  issue: '3'
  issued:
    - year: 2007
      month: 5
  page: 10-20
  source: IEEE Xplore
  title: Python for Scientific Computing
  type: article-journal
  URL: https://ieeexplore.ieee.org/abstract/document/4160250
  volume: '9'

- id: oliveiraProvenanceAnalyticsWorkflowBased2018
  abstract: >-
    Until not long ago, manually capturing and storing provenance from
    scientific experiments were constant concerns for scientists. With the
    advent of computational experiments (modeled as scientific workflows) and
    Scientific Workflow Management Systems, produced and consumed data, as well
    as the provenance of a given experiment, are automatically managed, so
    provenance capturing and storing in such a context is no longer a major
    concern. Similarly to several existing big data problems, the bottom line is
    now on how to analyze the large amounts of provenance data generated by
    workflow executions and how to be able to extract useful knowledge of this
    data. In this context, this article surveys the current state of the art on
    provenance analytics by presenting the key initiatives that have been taken
    to support provenance data analysis. We also contribute by proposing a
    taxonomy to classify elements related to provenance analytics.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Oliveira
      given: Wellington
    - family: Oliveira
      given: Daniel De
    - family: Braganholo
      given: Vanessa
  citation-key: oliveiraProvenanceAnalyticsWorkflowBased2018
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/3184900
  ISSN: 0360-0300
  issue: '3'
  issued:
    - year: 2018
      month: 5
      day: 23
  note: 'interest: 95'
  page: 53:1–53:25
  source: May 2019
  title: 'Provenance Analytics for Workflow-Based Computational Experiments: A Survey'
  title-short: Provenance Analytics for Workflow-Based Computational Experiments
  type: article-journal
  URL: https://doi.org/10.1145/3184900
  volume: '51'

- id: olmsted-hawalaThinkaloudProtocolsComparison2010
  accessed:
    - year: 2022
      month: 6
      day: 13
  author:
    - family: Olmsted-Hawala
      given: Erica L.
    - family: Murphy
      given: Elizabeth D.
    - family: Hawala
      given: Sam
    - family: Ashenfelter
      given: Kathleen T.
  citation-key: olmsted-hawalaThinkaloudProtocolsComparison2010
  container-title: >-
    Proceedings of the 28th international conference on Human factors in
    computing systems - CHI '10
  DOI: 10.1145/1753326.1753685
  event-place: Atlanta, Georgia, USA
  event-title: the 28th international conference
  ISBN: 978-1-60558-929-9
  issued:
    - year: 2010
  language: en
  note: 'interest: 75'
  page: '2381'
  publisher: ACM Press
  publisher-place: Atlanta, Georgia, USA
  source: DOI.org (Crossref)
  title: >-
    Think-aloud protocols: a comparison of three think-aloud protocols for use
    in testing data-dissemination web sites for usability
  title-short: Think-aloud protocols
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=1753326.1753685

- id: omorainWhyWeRe2015
  accessed:
    - year: 2024
      month: 1
      day: 3
  author:
    - family: O'Morain
      given: Marc
  citation-key: omorainWhyWeRe2015
  container-title: The Circle Blog
  issued:
    - year: 2015
      month: 9
      day: 24
  title: Why we’re no longer using Core.typed
  type: post-weblog
  URL: https://archive.is/KiBXb

- id: onionsSubhaloesGoingNotts2012
  abstract: >-
    We present a detailed comparison of the substructure properties of a single
    Milky Way sized dark matter halo from the Aquarius suite at five different
    resolutions, as identified by a variety of different (sub)halo finders for
    simulations of cosmic structure formation. These finders span a wide range
    of techniques and methodologies to extract and quantify substructures within
    a larger non-homogeneous background density (e.g. a host halo). This
    includes real-space-, phase-space-, velocity-space- and time-space-based
    finders, as well as finders employing a Voronoi tessellation,
    Friends-of-Friends techniques or refined meshes as the starting point for
    locating substructure. A common post-processing pipeline was used to
    uniformly analyse the particle lists provided by each finder. We extract
    quantitative and comparable measures for the subhaloes, primarily focusing
    on mass and the peak of the rotation curve for this particular study. We
    find that all of the finders agree extremely well in the presence and
    location of substructure and even for properties relating to the inner part
    of the subhalo (e.g. the maximum value of the rotation curve). For
    properties that rely on particles near the outer edge of the subhalo the
    agreement is at around the 20 per cent level. We find that the basic
    properties (mass and maximum circular velocity) of a subhalo can be reliably
    recovered if the subhalo contains more than 100 particles although its
    presence can be reliably inferred for a lower particle number limit of 20.


    We finally note that the logarithmic slope of the subhalo cumulative number
    count is remarkably consistent and <1 for all the finders that reached high
    resolution. If correct, this would indicate that the larger and more
    massive, respectively, substructures are the most dynamically interesting
    and that higher levels of the (sub)subhalo hierarchy become progressively
    less important.
  accessed:
    - year: 2022
      month: 7
      day: 22
  author:
    - family: Onions
      given: Julian
    - family: Knebe
      given: Alexander
    - family: Pearce
      given: Frazer R.
    - family: Muldrew
      given: Stuart I.
    - family: Lux
      given: Hanni
    - family: Knollmann
      given: Steffen R.
    - family: Ascasibar
      given: Yago
    - family: Behroozi
      given: Peter
    - family: Elahi
      given: Pascal
    - family: Han
      given: Jiaxin
    - family: Maciejewski
      given: Michal
    - family: Merchán
      given: Manuel E.
    - family: Neyrinck
      given: Mark
    - family: Ruiz
      given: Andrés N.
    - family: Sgró
      given: Mario A.
    - family: Springel
      given: Volker
    - family: Tweed
      given: Dylan
  citation-key: onionsSubhaloesGoingNotts2012
  container-title: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1111/j.1365-2966.2012.20947.x
  ISSN: '00358711'
  issue: '2'
  issued:
    - year: 2012
      month: 6
      day: 21
  language: en
  page: 1200-1214
  source: DOI.org (Crossref)
  title: >-
    Subhaloes going Notts: the subhalo-finder comparison project: Subhalo-finder
    comparison
  title-short: Subhaloes going Notts
  type: article-journal
  URL: >-
    https://academic.oup.com/mnras/article-lookup/doi/10.1111/j.1365-2966.2012.20947.x
  volume: '423'

- id: ORESpecificationAbstract2008
  abstract: >-
    Open Archives Initiative Object Reuse and Exchange (OAI-ORE) defines
    standards for the description and exchange of aggregations of Web resources.
    This document describes the abstract data model that is the foundation for
    these standards. This model is conformant with the Architecture of the World
    Wide Web [Web Architecture] and leverages concepts from the Semantic Web
    including RDF descriptions [RDF Concepts] and Linked Data [Linked Data
    Tutorial].


    This specification is one of several documents comprising the OAI-ORE
    specifications and user guides. The intended audience for this document is
    implementers that have an understanding of Semantic Web Concepts. Readers
    that want a high-level understanding of the motivation for ORE, and of the
    solution it provides should read the ORE Primer.
  citation-key: ORESpecificationAbstract2008
  issued:
    - year: 2008
      month: 10
      day: 17
  title: ORE Specification - Abstract Data Model
  type: webpage
  URL: https://www.openarchives.org/ore/1.0/datamodel

- id: osheaIntroducingEnzoAMR2004
  abstract: >-
    In this paper we introduce Enzo, a 3D MPI-parallel Eulerian block-structured
    adaptive mesh refinement cosmology code. Enzo is designed to simulate
    cosmological structure formation, but can also be used to simulate a wide
    range of astrophysical situations. Enzo solves dark matter N-body dynamics
    using the particle-mesh technique. The Poisson equation is solved using a
    combination of fast fourier transform (on a periodic root grid) and
    multigrid techniques (on non-periodic subgrids). Euler's equations of
    hydrodynamics are solved using a modified version of the piecewise parabolic
    method. Several additional physics packages are implemented in the code,
    including several varieties of radiative cooling, a metagalactic ultraviolet
    background, and prescriptions for star formation and feedback. We also show
    results illustrating properties of the adaptive mesh portion of the code.
    Enzo is publicly available and can be downloaded at
    http://cosmos.ucsd.edu/enzo/ .
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: O'Shea
      given: Brian W.
    - family: Bryan
      given: Greg
    - family: Bordner
      given: James
    - family: Norman
      given: Michael L.
    - family: Abel
      given: Tom
    - family: Harkness
      given: Robert
    - family: Kritsuk
      given: Alexei
  citation-key: osheaIntroducingEnzoAMR2004
  container-title: arXiv e-prints
  issued:
    - year: 2004
      month: 3
  language: en
  page: astro-ph/0403044
  source: ui.adsabs.harvard.edu
  title: Introducing Enzo, an AMR Cosmology Application
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/2004astro.ph..3044O/abstract

- id: oswaldLargeInequalityInternational2020
  abstract: >-
    Inequality in energy consumption, both direct and indirect, affects the
    distribution of benefits that result from energy use. Detailed measures of
    this inequality are required to ensure an equitable and just energy
    transition. Here we calculate final energy footprints; that is, the energy
    embodied in goods and services across income classes in 86 countries, both
    highly industrialized and developing. We analyse the energy intensity of
    goods and services used by different income groups, as well as their income
    elasticity of demand. We find that inequality in the distribution of energy
    footprints varies across different goods and services. Energy-intensive
    goods tend to be more elastic, leading to higher energy footprints of
    high-income individuals. Our results consequently expose large inequality in
    international energy footprints: the consumption share of the bottom half of
    the population is less than 20% of final energy footprints, which in turn is
    less than what the top 5% consume.
  accessed:
    - year: 2024
      month: 9
      day: 10
  author:
    - family: Oswald
      given: Yannick
    - family: Owen
      given: Anne
    - family: Steinberger
      given: Julia K.
  citation-key: oswaldLargeInequalityInternational2020
  container-title: Nature Energy
  container-title-short: Nat Energy
  DOI: 10.1038/s41560-020-0579-8
  ISSN: 2058-7546
  issue: '3'
  issued:
    - year: 2020
      month: 3
  language: en
  license: 2020 The Author(s), under exclusive licence to Springer Nature Limited
  page: 231-239
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Large inequality in international and intranational energy footprints
    between income groups and across consumption categories
  type: article-journal
  URL: https://www.nature.com/articles/s41560-020-0579-8
  volume: '5'

- id: otterRoadmapComputationPersistent2017
  abstract: >-
    Persistent homology (PH) is a method used in topological data analysis (TDA)
    to study qualitative features of data that persist across multiple scales.
    It is robust to perturbations of input data, independent of dimensions and
    coordinates, and provides a compact representation of the qualitative
    features of the input. The computation of PH is an open area with numerous
    important and fascinating challenges. The field of PH computation is
    evolving rapidly, and new algorithms and software implementations are being
    updated and released at a rapid pace. The purposes of our article are to (1)
    introduce theory and computational methods for PH to a broad range of
    computational scientists and (2) provide benchmarks of state-of-the-art
    implementations for the computation of PH. We give a friendly introduction
    to PH, navigate the pipeline for the computation of PH with an eye towards
    applications, and use a range of synthetic and real-world data sets to
    evaluate currently available open-source implementations for the computation
    of PH. Based on our benchmarking, we indicate which algorithms and
    implementations are best suited to different types of data sets. In an
    accompanying tutorial, we provide guidelines for the computation of PH. We
    make publicly available all scripts that we wrote for the tutorial, and we
    make available the processed version of the data sets used in the
    benchmarking.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Otter
      given: Nina
    - family: Porter
      given: Mason A
    - family: Tillmann
      given: Ulrike
    - family: Grindrod
      given: Peter
    - family: Harrington
      given: Heather A
  citation-key: otterRoadmapComputationPersistent2017
  container-title: Epj Data Science
  container-title-short: EPJ Data Sci
  DOI: 10.1140/epjds/s13688-017-0109-5
  ISSN: 2193-1127
  issue: '1'
  issued:
    - year: 2017
  note: 'interest: 61'
  page: '17'
  PMCID: PMC6979512
  PMID: '32025466'
  source: PubMed Central
  title: A roadmap for the computation of persistent homology
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6979512/
  volume: '6'

- id: paineWhoHasPlots2017
  abstract: >-
    Software is an integral element of the work of science yet it is not
    commonly an object of inquiry in studies of scientific infrastructures. This
    paper presents findings from an ethnographic study of a cosmology group's
    collaborative scientific software production. We demonstrate how these
    cosmologists use plots to simultaneously test their software and analyze
    data while interrogating multiple layers of infrastructural components. We
    broaden perspectives on scientific software development using a
    sociotechnical, software studies lens to examine this work of scientific
    discovery as a creative and embodied, yet exacting and methodical, activity
    that requires a 'human in the loop'. We offer a new reading of scientific
    software practices to convey how creating scientific software is often
    really the act of doing science itself--an intervention we believe is
    necessary to more successfully support scientific software sharing and
    infrastructure production.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Paine
      given: Drew
    - family: Lee
      given: Charlotte P.
  citation-key: paineWhoHasPlots2017
  container-title: Proceedings of the ACM on Human-Computer Interaction
  container-title-short: Proc. ACM Hum.-Comput. Interact.
  DOI: 10.1145/3134720
  issue: CSCW
  issued:
    - year: 2017
      month: 12
      day: 6
  note: 'interest: 82'
  page: 85:1–85:21
  source: November 2017
  title: >-
    "Who Has Plots?": Contextualizing Scientific Software, Practice, and
    Visualizations
  title-short: '"Who Has Plots?'
  type: article-journal
  URL: https://doi.org/10.1145/3134720
  volume: '1'

- id: parisMerkleHashGrids2020
  abstract: >-
    Merkle grids are a new data organization that replicates the functionality
    of Merkle trees while reducing their transmission and storage costs by up to
    50 percent. All Merkle grids organize the objects whose conformity they
    monitor in a square array. They add row and column hashes to it such that
    (a) all row hashes contain the hash of the concatenation of the hashes of
    all the objects in their respective row and (b) all column hashes contain
    the hash of the concatenation of the hashes of all the objects in their
    respective column. In addition, a single signed master hash contains the
    hash of the concatenation of all row and column hashes. Extended Merkle
    grids add two auxiliary Merkle trees to speed up searches among both row
    hashes and column hashes. While both basic and extended Merkle grids perform
    authentication of all blocks better than Merkle trees, only extended Merkle
    grids can locate individual non-conforming objects or authenticate a single
    non-conforming object as fast as Merkle trees.
  author:
    - family: Pâris
      given: Jehan-François
    - family: Schwarz
      given: Thomas
  citation-key: parisMerkleHashGrids2020
  container-title: >-
    2020 28th International Symposium on Modeling, Analysis, and Simulation of
    Computer and Telecommunication Systems (MASCOTS)
  DOI: 10.1109/MASCOTS50786.2020.9285942
  event-title: >-
    2020 28th International Symposium on Modeling, Analysis, and Simulation of
    Computer and Telecommunication Systems (MASCOTS)
  ISSN: 2375-0227
  issued:
    - year: 2020
      month: 11
  page: 1-8
  source: IEEE Xplore
  title: Merkle Hash Grids Instead of Merkle Trees
  type: paper-conference

- id: parrEnforcingStrictModelview2004
  abstract: >-
    The mantra of every experienced web application developer is the same: thou
    shalt separate business logic from display. Ironically, almost all template
    engines allow violation of this separation principle, which is the very
    impetus for HTML template engine development. This situation is due mostly
    to a lack of formal definition of separation and fear that enforcing
    separation emasculates a template's power. I show that not only is strict
    separation a worthy design principle, but that we can enforce separation
    while providing a potent template engine. I demonstrate my StringTemplate
    engine, used to build jGuru.com and other commercial sites, at work solving
    some nontrivial generational tasks.


    My goal is to formalize the study of template engines, thus, providing a
    common nomenclature, a means of classifying template generational power, and
    a way to leverage interesting results from formal language theory. I
    classify three types of restricted templates analogous to Chomsky's type
    1..3 grammar classes and formally define separation including the rules that
    embody separation.


    Because this paper provides a clear definition of model-view separation,
    template engine designers may no longer blindly claim enforcement of
    separation. Moreover, given theoretical arguments and empirical evidence,
    programmers no longer have an excuse to entangle model and view.
  accessed:
    - year: 2022
      month: 7
      day: 11
  author:
    - family: Parr
      given: Terence John
  citation-key: parrEnforcingStrictModelview2004
  container-title: Proceedings of the 13th conference on World Wide Web  - WWW '04
  DOI: 10.1145/988672.988703
  event-place: New York, NY, USA
  event-title: the 13th conference
  ISBN: 978-1-58113-844-3
  issued:
    - year: 2004
  language: en
  note: 'interest: 45'
  page: '224'
  publisher: ACM Press
  publisher-place: New York, NY, USA
  source: DOI.org (Crossref)
  title: Enforcing strict model-view separation in template engines
  type: paper-conference
  URL: http://portal.acm.org/citation.cfm?doid=988672.988703

- id: parrLLFoundationANTLR2011
  abstract: >-
    Despite the power of Parser Expression Grammars (PEGs) and GLR, parsing is
    not a solved problem. Adding nondeterminism (parser speculation) to
    traditional LL and LR parsers can lead to unexpected parse-time behavior and
    introduces practical issues with error handling, single-step debugging, and
    side-effecting embedded grammar actions. This paper introduces the LL(*)
    parsing strategy and an associated grammar analysis algorithm that
    constructs LL(*) parsing decisions from ANTLR grammars. At parse-time,
    decisions gracefully throttle up from conventional fixed k>=1 lookahead to
    arbitrary lookahead and, finally, fail over to backtracking depending on the
    complexity of the parsing decision and the input symbols. LL(*) parsing
    strength reaches into the context-sensitive languages, in some cases beyond
    what GLR and PEGs can express. By statically removing as much speculation as
    possible, LL(*) provides the expressivity of PEGs while retaining LL's good
    error handling and unrestricted grammar actions. Widespread use of ANTLR
    (over 70,000 downloads/year) shows that it is effective for a wide variety
    of applications.
  accessed:
    - year: 2022
      month: 9
      day: 21
  author:
    - family: Parr
      given: Terence
    - family: Fisher
      given: Kathleen
  citation-key: parrLLFoundationANTLR2011
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/1993316.1993548
  ISSN: 0362-1340
  issue: '6'
  issued:
    - year: 2011
      month: 6
      day: 4
  page: 425–436
  source: June 2011
  title: 'LL(*): the foundation of the ANTLR parser generator'
  title-short: LL(*)
  type: article-journal
  URL: https://doi.org/10.1145/1993316.1993548
  volume: '46'

- id: pasquierPracticalWholesystemProvenance2017
  abstract: >-
    Data provenance describes how data came to be in its present form. It
    includes data sources and the transformations that have been applied to
    them. Data provenance has many uses, from forensics and security to aiding
    the reproducibility of scientific experiments. We present CamFlow, a
    whole-system provenance capture mechanism that integrates easily into a PaaS
    offering. While there have been several prior whole-system provenance
    systems that captured a comprehensive, systemic and ubiquitous record of a
    system's behavior, none have been widely adopted. They either A) impose too
    much overhead, B) are designed for long-outdated kernel releases and are
    hard to port to current systems, C) generate too much data, or D) are
    designed for a single system. CamFlow addresses these shortcoming by: 1)
    leveraging the latest kernel design advances to achieve efficiency; 2) using
    a self-contained, easily maintainable implementation relying on a Linux
    Security Module, NetFilter, and other existing kernel facilities; 3)
    providing a mechanism to tailor the captured provenance data to the needs of
    the application; and 4) making it easy to integrate provenance across
    distributed systems. The provenance we capture is streamed and consumed by
    tenant-built auditor applications. We illustrate the usability of our
    implementation by describing three such applications: demonstrating
    compliance with data regulations; performing fault/intrusion detection; and
    implementing data loss prevention. We also show how CamFlow can be leveraged
    to capture meaningful provenance without modifying existing applications.
  accessed:
    - year: 2023
      month: 7
      day: 7
  author:
    - family: Pasquier
      given: Thomas
    - family: Han
      given: Xueyuan
    - family: Goldstein
      given: Mark
    - family: Moyer
      given: Thomas
    - family: Eyers
      given: David
    - family: Seltzer
      given: Margo
    - family: Bacon
      given: Jean
  citation-key: pasquierPracticalWholesystemProvenance2017
  collection-title: SoCC '17
  container-title: Proceedings of the 2017 Symposium on Cloud Computing
  DOI: 10.1145/3127479.3129249
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5028-0
  issued:
    - year: 2017
      month: 9
      day: 24
  page: 405–418
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Practical whole-system provenance capture
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3127479.3129249

- id: patilStatisticalDefinitionReproducibility2016
  abstract: |-
    Abstract
              Everyone agrees that reproducibility and replicability are fundamental characteristics of scientific studies. These topics are attracting increasing attention, scrutiny, and debate both in the popular press and the scientific literature. But there are no formal statistical definitions for these concepts, which leads to confusion since the same words are used for different concepts by different people in different fields. We provide formal and informal definitions of scientific studies, reproducibility, and replicability that can be used to clarify discussions around these concepts in the scientific and popular press.
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Patil
      given: Prasad
    - family: Peng
      given: Roger D.
    - family: Leek
      given: Jeffrey T.
  citation-key: patilStatisticalDefinitionReproducibility2016
  DOI: 10.1101/066803
  issued:
    - year: 2016
      month: 7
      day: 29
  language: en
  license: http://creativecommons.org/licenses/by/4.0/
  source: DOI.org (Crossref)
  title: A statistical definition for reproducibility and replicability
  type: article
  URL: http://biorxiv.org/lookup/doi/10.1101/066803

- id: pattersonLinkingTypesMultiLanguage2017
  abstract: >-
    Software developers compose systems from components written in many
    different languages. A business-logic component may be written in Java or
    OCaml, a resource-intensive component in C or Rust, and a high-assurance
    component in Coq. In this multi-language world, program execution sends
    values from one linguistic context to another. This boundary-crossing
    exposes values to contexts with unforeseen behavior---that is, behavior that
    could not arise in the source language of the value. For example, a Rust
    function may end up being applied in an ML context that violates the memory
    usage policy enforced by Rust's type system. This leads to the question of
    how developers ought to reason about code in such a multi-language world
    where behavior inexpressible in one language is easily realized in another.
    This paper proposes the novel idea of linking types to address the problem
    of reasoning about single-language components in a multi-lingual setting.
    Specifically, linking types allow programmers to annotate where in a program
    they can link with components inexpressible in their unadulterated language.
    This enables developers to reason about (behavioral) equality using only
    their own language and the annotations, even though their code may be linked
    with code written in a language with more expressive power.
  accessed:
    - year: 2023
      month: 3
      day: 10
  author:
    - family: Patterson
      given: Daniel
    - family: Ahmed
      given: Amal
  citation-key: pattersonLinkingTypesMultiLanguage2017
  DOI: 10.4230/LIPIcs.SNAPL.2017.12
  issued:
    - year: 2017
  note: 'interest: 75'
  page: 15 pages
  source: arXiv.org
  title: 'Linking Types for Multi-Language Software: Have Your Cake and Eat It Too'
  title-short: Linking Types for Multi-Language Software
  type: article-journal
  URL: http://arxiv.org/abs/1711.04559

- id: pearlCausality2009
  author:
    - family: Pearl
      given: Judea
  citation-key: pearlCausality2009
  ISBN: 978-0-521-89560-6
  issued:
    - year: 2009
  language: eng
  note: |-
    Open Library ID: OL27305967M
    interest: 80
  publisher: Cambridge University Press
  source: The Open Library
  title: Causality
  type: book

- id: pearlUnderstandingSimpsonParadox2013
  abstract: >-
    Simpson's paradox is often presented as a compelling demonstration of why we
    need statistics education in our schools. It is a reminder of how easy it is
    to fall into a web of paradoxical conclusions when relying solely on
    intuition, unaided by rigorous statistical methods. In recent years,
    ironically, the paradox assumed an added dimension when educators began
    using it to demonstrate the limits of statistical methods, and why causal,
    rather than statistical considerations are necessary to avoid those
    paradoxical conclusions (Arah, 2008; Pearl, 2009, pp. 173-182; Wasserman,
    2004).
  accessed:
    - year: 2022
      month: 9
      day: 9
  author:
    - family: Pearl
      given: Judea
  citation-key: pearlUnderstandingSimpsonParadox2013
  DOI: 10.2139/ssrn.2343788
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2013
      month: 9
      day: 19
  language: en
  note: 'interest: 91'
  number: '2343788'
  publisher-place: Rochester, NY
  source: Social Science Research Network
  title: Understanding Simpson's Paradox
  type: article
  URL: https://papers.ssrn.com/abstract=2343788

- id: pediaditisExplicitProvenanceManagement2009
  abstract: >-
    The notion of RDF Named Graphs has been proposed in order to assign
    provenance information to data described using RDF triples. In this paper,
    we argue that named graphs alone cannot capture provenance information in
    the presence of RDFS reasoning and updates. In order to address this
    problem, we introduce the notion of RDF/S Graphsets: a graphset is
    associated with a set of RDF named graphs and contain the triples that are
    jointly owned by the named graphs that constitute the graphset. We formalize
    the notions of RDF named graphs and RDF/S graphsets and propose query and
    update languages that can be used to handle provenance information for RDF/S
    graphs taking into account RDFS semantics.
  author:
    - family: Pediaditis
      given: P
    - family: Flouris
      given: G
    - family: Fundulaki
      given: I
    - family: Christophides
      given: V
  citation-key: pediaditisExplicitProvenanceManagement2009
  container-title: First Workshop on the Theory and Practice of Provenance (TaPP '09)
  event-place: San Francisco, CA
  issued:
    - year: 2009
      month: 2
      day: 23
  language: en
  publisher: USENIX
  publisher-place: San Francisco, CA
  source: Zotero
  title: On Explicit Provenance Management in RDF/S Graphs
  type: paper-conference

- id: pengReproducibleResearchComputational2011
  abstract: >-
    Computational science has led to exciting new developments, but the nature
    of the work has exposed limitations in our ability to evaluate published
    findings. Reproducibility has the potential to serve as a minimum standard
    for judging scientific claims when full independent replication of a study
    is not possible.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Peng
      given: Roger D.
  citation-key: pengReproducibleResearchComputational2011
  container-title: Science
  container-title-short: Science
  DOI: 10.1126/science.1213847
  ISSN: 0036-8075, 1095-9203
  issue: '6060'
  issued:
    - year: 2011
      month: 12
      day: 2
  language: en
  note: 'interest: 95'
  page: 1226-1227
  source: DOI.org (Crossref)
  title: Reproducible Research in Computational Science
  type: article-journal
  URL: https://www.science.org/doi/10.1126/science.1213847
  volume: '334'

- id: pennarunMtimeComparisonConsidered
  abstract: >-
    tl;dr: Rebuilding a target because its mtime is older than the

    mtimes of its dependencies, like `make` does, is very error prone.  redo
    does...
  accessed:
    - year: 2024
      month: 2
      day: 11
  author:
    - family: Pennarun
      given: Avery
  citation-key: pennarunMtimeComparisonConsidered
  title: mtime comparison considered harmful
  type: post-weblog
  URL: https://apenwarr.ca/log/20181113

- id: pereraImpactConsideringHuman2021
  abstract: >-
    Human values, or what people hold important in their life, such as freedom,
    fairness, and social responsibility, often remain unnoticed and unattended
    during software development. Ignoring values can lead to values violations
    in software that can result in financial losses, reputation damage, and
    widespread social and legal implications. However, embedding human values in
    software is not only non-trivial but also generally an unclear process.
    Commencing as early as during the Requirements Engineering (RE) activities
    promises to ensure fit-for-purpose and quality software products that adhere
    to human values. But what is the impact of considering human values
    explicitly during early RE activities? To answer this question, we conducted
    a scenario-based survey where 56 software practitioners contextualised
    requirements analysis towards a proposed mobile application for the homeless
    and suggested values-laden software features accordingly. The suggested
    features were qualitatively analysed. Results show that explicit
    considerations of values can help practitioners identify applicable values,
    associate purpose with the features they develop, think outside-the-box, and
    build connections between software features and human values. Finally,
    drawing from the results and experiences of this study, we propose a
    scenario-based values elicitation process -- a simple four-step takeaway as
    a practical implication of this study.
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Perera
      given: Harsha
    - family: Hoda
      given: Rashina
    - family: Shams
      given: Rifat Ara
    - family: Nurwidyantoro
      given: Arif
    - family: Shahin
      given: Mojtaba
    - family: Hussain
      given: Waqar
    - family: Whittle
      given: Jon
  citation-key: pereraImpactConsideringHuman2021
  container-title: arXiv:2111.15293 [cs]
  issued:
    - year: 2021
      month: 11
      day: 30
  note: 'interest: 80'
  source: arXiv.org
  title: >-
    The Impact of Considering Human Values during Requirements Engineering
    Activities
  type: article-journal
  URL: http://arxiv.org/abs/2111.15293

- id: perezderossoWhatWrongGit2013
  abstract: >-
    It is commonly asserted that the success of a software development project,
    and the usability of the final product, depend on the quality of the
    concepts that underlie its design. Yet this hypothesis has not been
    systematically explored by researchers, and conceptual design has not played
    the central role in the research and teaching of software engineering that
    one might expect. As part of a new research project to explore conceptual
    design, we are engaging in a series of case studies. This paper reports on
    the early stages of our first study, on the Git version control system.
    Despite its widespread adoption, Git puzzles even experienced developers and
    is not regarded as easy to use. In an attempt to understand the root causes
    of its complexity, we analyze its conceptual model and identify some
    undesirable properties; we then propose a reworking of the conceptual model
    that forms the basis of (the first version of) Gitless, an ongoing effort to
    redesign Git and experiment with the effects of conceptual simplifications.
  accessed:
    - year: 2023
      month: 7
      day: 6
  author:
    - family: Perez De Rosso
      given: Santiago
    - family: Jackson
      given: Daniel
  citation-key: perezderossoWhatWrongGit2013
  collection-title: Onward! 2013
  container-title: >-
    Proceedings of the 2013 ACM international symposium on New ideas, new
    paradigms, and reflections on programming & software
  DOI: 10.1145/2509578.2509584
  event-place: New York, NY, USA
  ISBN: 978-1-4503-2472-4
  issued:
    - year: 2013
      month: 10
      day: 29
  page: 37–52
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: What's wrong with git? a conceptual design analysis
  title-short: What's wrong with git?
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2509578.2509584

- id: perezgonzalezFisherNeymanPearsonNHST2015
  abstract: >-
    Despite frequent calls for the overhaul of null hypothesis significance
    testing (NHST), this controversial procedure remains ubiquitous in
    behavioral, social and biomedical teaching and research. Little change seems
    possible once the procedure becomes well ingrained in the minds and current
    practice of researchers; thus, the optimal opportunity for such change is at
    the time the procedure is taught, be this at undergraduate or at
    postgraduate levels. This paper presents a tutorial for the teaching of data
    testing procedures, often referred to as hypothesis testing theories. The
    first procedure introduced is Fisher's approach to data testing—tests of
    significance; the second is Neyman-Pearson's approach—tests of acceptance;
    the final procedure is the incongruent combination of the previous two
    theories into the current approach—NSHT. For those researchers sticking with
    the latter, two compromise solutions on how to improve NHST conclude the
    tutorial.
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Perezgonzalez
      given: Jose D.
  citation-key: perezgonzalezFisherNeymanPearsonNHST2015
  container-title: Frontiers in Psychology
  ISSN: 1664-1078
  issued:
    - year: 2015
  source: Frontiers
  title: Fisher, Neyman-Pearson or NHST? A tutorial for teaching data testing
  title-short: Fisher, Neyman-Pearson or NHST?
  type: article-journal
  URL: https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00223
  volume: '6'

- id: perezPythonEcosystemScientific2011
  abstract: >-
    As the relationship between research and computing evolves, new tools are
    required to not only treat numerical problems, but also to solve various
    problems that involve large datasets in different formats, new algorithms,
    and computational systems such as databases and Internet servers. Python can
    help develop these computational research tools by providing a balance of
    clarity and flexibility without sacrificing performance.
  accessed:
    - year: 2024
      month: 2
      day: 23
  author:
    - family: Pérez
      given: Fernando
    - family: Granger
      given: Brian E.
    - family: Hunter
      given: John D.
  citation-key: perezPythonEcosystemScientific2011
  container-title: Computing in Science & Engineering
  DOI: 10.1109/MCSE.2010.119
  ISSN: 1558-366X
  issue: '2'
  issued:
    - year: 2011
      month: 3
  page: 13-21
  source: IEEE Xplore
  title: 'Python: An Ecosystem for Scientific Computing'
  title-short: Python
  type: article-journal
  URL: https://ieeexplore.ieee.org/abstract/document/5582063
  volume: '13'

- id: perkelSixTipsBetter2022
  abstract: >-
    Microsoft Excel and Google Sheets are powerful and widely used. But there’s
    a right way and a wrong way to use them, data scientists say.
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Perkel
      given: Jeffrey M.
  citation-key: perkelSixTipsBetter2022
  container-title: Nature
  DOI: 10.1038/d41586-022-02076-1
  issue: '7921'
  issued:
    - year: 2022
      month: 8
      day: 2
  language: en
  license: 2022 Springer Nature Limited
  note: >-
    Bandiera_abtest: a

    Cg_type: Technology Feature

    Subject_term: Software, Computational biology and bioinformatics, Research
    data
  number: '7921'
  page: 229-230
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Six tips for better spreadsheets
  type: article-journal
  URL: https://www.nature.com/articles/d41586-022-02076-1
  volume: '608'

- id: perryCrossValidationUnsupervisedLearning2009
  abstract: >-
    Cross-validation (CV) is a popular method for model-selection.
    Unfortunately, it is not immediately obvious how to apply CV to unsupervised
    or exploratory contexts. This thesis discusses some extensions of
    cross-validation to unsupervised learning, specifically focusing on the
    problem of choosing how many principal components to keep. We introduce the
    latent factor model, define an objective criterion, and show how CV can be
    used to estimate the intrinsic dimensionality of a data set. Through both
    simulation and theory, we demonstrate that cross-validation is a valuable
    tool for unsupervised learning.
  accessed:
    - year: 2024
      month: 2
      day: 10
  author:
    - family: Perry
      given: Patrick O.
  citation-key: perryCrossValidationUnsupervisedLearning2009
  issued:
    - year: 2009
      month: 9
      day: 16
  language: en
  number: arXiv:0909.3052
  publisher: arXiv
  source: arXiv.org
  title: Cross-Validation for Unsupervised Learning
  type: article
  URL: http://arxiv.org/abs/0909.3052

- id: petersohnScalableDataframeSystems2020
  abstract: >-
    Dataframes are a popular abstraction to represent, prepare, and analyze
    data. Despite the remarkable success of dataframe libraries in Rand Python,
    dataframes face performance issues even on moderately large datasets.
    Moreover, there is significant ambiguity regarding dataframe semantics. In
    this paper we lay out a vision and roadmap for scalable dataframe systems.
    To demonstrate the potential in this area, we report on our experience
    building MODIN, a scaled-up implementation of the most widely-used and
    complex dataframe API today, Python's pandas. With pandas as a reference, we
    propose a simple data model and algebra for dataframes to ground discussion
    in the field. Given this foundation, we lay out an agenda of open research
    opportunities where the distinct features of dataframes will require
    extending the state of the art in many dimensions of data management. We
    discuss the implications of signature data-frame features including flexible
    schemas, ordering, row/column equivalence, and data/metadata fluidity, as
    well as the piecemeal, trial-and-error-based approach to interacting with
    dataframes.
  accessed:
    - year: 2022
      month: 5
      day: 25
  author:
    - family: Petersohn
      given: Devin
    - family: Macke
      given: Stephen
    - family: Xin
      given: Doris
    - family: Ma
      given: William
    - family: Lee
      given: Doris
    - family: Mo
      given: Xiangxi
    - family: Gonzalez
      given: Joseph E.
    - family: Hellerstein
      given: Joseph M.
    - family: Joseph
      given: Anthony D.
    - family: Parameswaran
      given: Aditya
  citation-key: petersohnScalableDataframeSystems2020
  container-title: arXiv:2001.00888 [cs]
  issued:
    - year: 2020
      month: 6
      day: 2
  note: 'interest: 71'
  source: arXiv.org
  title: Towards Scalable Dataframe Systems
  type: article-journal
  URL: http://arxiv.org/abs/2001.00888

- id: petersPhenoMeNalProcessingAnalysis2019
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Peters
      given: Kristian
    - family: Bradbury
      given: James
    - family: Bergmann
      given: Sven
    - family: Capuccini
      given: Marco
    - family: Cascante
      given: Marta
    - family: "de\_Atauri"
      given: Pedro
    - family: Ebbels
      given: Timothy M D
    - family: Foguet
      given: Carles
    - family: Glen
      given: Robert
    - family: Gonzalez-Beltran
      given: Alejandra
    - family: Günther
      given: Ulrich L
    - family: Handakas
      given: Evangelos
    - family: Hankemeier
      given: Thomas
    - family: Haug
      given: Kenneth
    - family: Herman
      given: Stephanie
    - family: Holub
      given: Petr
    - family: Izzo
      given: Massimiliano
    - family: Jacob
      given: Daniel
    - family: Johnson
      given: David
    - family: Jourdan
      given: Fabien
    - family: Kale
      given: Namrata
    - family: Karaman
      given: Ibrahim
    - family: Khalili
      given: Bita
    - family: "Emami\_Khonsari"
      given: Payam
    - family: Kultima
      given: Kim
    - family: Lampa
      given: Samuel
    - family: Larsson
      given: Anders
    - family: Ludwig
      given: Christian
    - family: Moreno
      given: Pablo
    - family: Neumann
      given: Steffen
    - family: Novella
      given: Jon Ander
    - family: O'Donovan
      given: Claire
    - family: Pearce
      given: Jake T M
    - family: Peluso
      given: Alina
    - family: Piras
      given: Marco Enrico
    - family: Pireddu
      given: Luca
    - family: Reed
      given: Michelle A C
    - family: Rocca-Serra
      given: Philippe
    - family: Roger
      given: Pierrick
    - family: Rosato
      given: Antonio
    - family: Rueedi
      given: Rico
    - family: Ruttkies
      given: Christoph
    - family: Sadawi
      given: Noureddin
    - family: Salek
      given: Reza M
    - family: Sansone
      given: Susanna-Assunta
    - family: Selivanov
      given: Vitaly
    - family: Spjuth
      given: Ola
    - family: Schober
      given: Daniel
    - family: Thévenot
      given: Etienne A
    - family: Tomasoni
      given: Mattia
    - family: "van\_Rijswijk"
      given: Merlijn
    - family: "van\_Vliet"
      given: Michael
    - family: Viant
      given: Mark R
    - family: Weber
      given: Ralf J M
    - family: Zanetti
      given: Gianluigi
    - family: Steinbeck
      given: Christoph
  citation-key: petersPhenoMeNalProcessingAnalysis2019
  container-title: GigaScience
  DOI: 10.1093/gigascience/giy149
  ISSN: 2047-217X
  issue: '2'
  issued:
    - year: 2019
      month: 2
      day: 1
  language: en
  license: http://creativecommons.org/licenses/by/4.0/
  source: DOI.org (Crossref)
  title: 'PhenoMeNal: processing and analysis of metabolomics data in the cloud'
  title-short: PhenoMeNal
  type: article-journal
  URL: >-
    https://academic.oup.com/gigascience/article/doi/10.1093/gigascience/giy149/5232984
  volume: '8'

- id: phamUsingProvenanceRepeatability2013
  accessed:
    - year: 2024
      month: 2
      day: 14
  author:
    - family: Pham
      given: Quan
    - family: Malik
      given: Tanu
    - family: Foster
      given: Ian
  citation-key: phamUsingProvenanceRepeatability2013
  event-title: 5th USENIX Workshop on the Theory and Practice of Provenance (TaPP 13)
  issued:
    - year: 2013
  language: en
  source: www.usenix.org
  title: Using Provenance for Repeatability
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/tapp13/technical-sessions/presentation/pham

- id: phungNotAllTasks2021
  abstract: >-
    Users running dynamic workflows in distributed systems usually have
    inadequate expertise to correctly size the allocation of resources (cores,
    memory, disk) to each task due to the difficulty in uncovering the obscure
    yet important correlation between tasks and their resource consumption.
    Thus, users typically pay little attention to this problem of allocation
    sizing and either simply apply an error-prone upper bound of resource
    allocation to all tasks, or delegate this responsibility to underlying
    distributed systems, resulting in substantial waste from allocated yet
    unused resources. In this paper, we will first show that tasks performing
    different work may have significantly different resource consumption. We
    will then show that exploiting the heterogeneity of tasks is a desirable way
    to reveal and predict the relationship between tasks and their resource
    consumption, reduce waste from resource misallocation, increase tasks'
    consumption efficiency, and incentivize users' cooperation. We have
    developed two info-aware allocation strategies capitalizing on this
    characteristic and will show their effectiveness through simulations on two
    modern applications with dynamic workflows and five synthetic datasets of
    resource consumption. Our results show that info-aware strategies can cut
    down up to 98.7% of the total waste incurred by a best-effort strategy, and
    increase the efficiency in resource consumption of each task on average
    anywhere up to 93.9%.
  author:
    - family: Phung
      given: Thanh Son
    - family: Ward
      given: Logan
    - family: Chard
      given: Kyle
    - family: Thain
      given: Douglas
  citation-key: phungNotAllTasks2021
  container-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  DOI: 10.1109/WORKS54523.2021.00008
  event-title: 2021 IEEE Workshop on Workflows in Support of Large-Scale Science (WORKS)
  issued:
    - year: 2021
      month: 11
  note: 'interest: 71'
  page: 17-24
  source: IEEE Xplore
  title: >-
    Not All Tasks Are Created Equal: Adaptive Resource Allocation for
    Heterogeneous Tasks in Dynamic Workflows
  title-short: Not All Tasks Are Created Equal
  type: paper-conference

- id: pillerPatientsEndangeredLaw2015
  abstract: >-
    A STAT analysis has found that some of the top research institutions are not
    reporting clinical trial results as required by law.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Piller
      given: Charles
  citation-key: pillerPatientsEndangeredLaw2015
  container-title: STAT
  issued:
    - year: 2015
      month: 12
      day: 14
  language: en-US
  title: Patients endangered as law is ignored
  type: post-weblog
  URL: https://www.statnews.com/2015/12/13/clinical-trials-investigation/

- id: pimentelLargeScaleStudyQuality2019
  abstract: >-
    Jupyter Notebooks have been widely adopted by many different communities,
    both in science and industry. They support the creation of literate
    programming documents that combine code, text, and execution results with
    visualizations and all sorts of rich media. The self-documenting aspects and
    the ability to reproduce results have been touted as significant benefits of
    notebooks. At the same time, there has been growing criticism that the way
    notebooks are being used leads to unexpected behavior, encourage poor coding
    practices, and that their results can be hard to reproduce. To understand
    good and bad practices used in the development of real notebooks, we studied
    1.4 million notebooks from GitHub. We present a detailed analysis of their
    characteristics that impact reproducibility. We also propose a set of best
    practices that can improve the rate of reproducibility and discuss open
    challenges that require further research and development.
  author:
    - family: Pimentel
      given: João Felipe
    - family: Murta
      given: Leonardo
    - family: Braganholo
      given: Vanessa
    - family: Freire
      given: Juliana
  citation-key: pimentelLargeScaleStudyQuality2019
  collection-title: MSR '19
  container-title: >-
    Proceedings of the 16th International Conference on Mining Software
    Repositories
  DOI: 10.1109/MSR.2019.00077
  event-place: Montreal, Quebec, Canada
  event-title: >-
    2019 IEEE/ACM 16th International Conference on Mining Software Repositories
    (MSR)
  ISSN: 2574-3864
  issued:
    - year: 2019
      month: 5
      day: 26
  page: 507-517
  publisher: IEEE Press
  publisher-place: Montreal, Quebec, Canada
  source: ACM Digital Library
  title: A Large-Scale Study About Quality and Reproducibility of Jupyter Notebooks
  type: paper-conference
  URL: https://doi.org/10.1109/MSR.2019.00077

- id: pimentelSurveyCollectingManaging2019
  abstract: >-
    Scripts are widely used to design and run scientific experiments. Scripting
    languages are easy to learn and use, and they allow complex tasks to be
    specified and executed in fewer steps than with traditional programming
    languages. However, they also have important limitations for reproducibility
    and data management. As experiments are iteratively refined, it is
    challenging to reason about each experiment run (or trial), to keep track of
    the association between trials and experiment instances as well as the
    differences across trials, and to connect results to specific input data and
    parameters. Approaches have been proposed that address these limitations by
    collecting, managing, and analyzing the provenance of scripts. In this
    article, we survey the state of the art in provenance for scripts. We have
    identified the approaches by following an exhaustive protocol of forward and
    backward literature snowballing. Based on a detailed study, we propose a
    taxonomy and classify the approaches using this taxonomy.
  accessed:
    - year: 2024
      month: 1
      day: 20
  author:
    - family: Pimentel
      given: João Felipe
    - family: Freire
      given: Juliana
    - family: Murta
      given: Leonardo
    - family: Braganholo
      given: Vanessa
  citation-key: pimentelSurveyCollectingManaging2019
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/3311955
  ISSN: 0360-0300
  issue: '3'
  issued:
    - year: 2019
      month: 6
      day: 18
  page: 47:1–47:38
  source: ACM Digital Library
  title: A Survey on Collecting, Managing, and Analyzing Provenance from Scripts
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/3311955
  volume: '52'

- id: pintoHowScientistsDevelop2018
  abstract: >-
    Although the goal of scientists is to do science, not to develop software,
    many scientists have extended their roles to include software development to
    their skills. However, since scientists have different background, it
    remains unclear how do they perceive software engineering practices or how
    do they acquire software engineering knowledge. In this paper we conducted
    an external replication of one influential 10 years paper about how
    scientists develop and use scientific software. In particular, we employed
    the same method (an on-line questionnaire) in a different population (R
    developers). When analyzing the more than 1,574 responses received, enriched
    with data gathered from their GitHub repositories, we correlated our
    findings with the original study. We found that the results were consistent
    in many ways, including: (1) scientists that develop software work mostly
    alone, (2) they decide themselves what they want to work on next, and (3)
    most of what they learnt came from self-study, rather than a formal
    education. However, we also uncover new facts, such as: some of the "pain
    points" regarding software development are not related to technical
    activities (e.g., interruptions, lack of collaborators, and lack of a reward
    system play a role). Our replication can help researchers, practitioners,
    and educators to better focus their efforts on topics that are important to
    the scientific community that develops software.
  author:
    - family: Pinto
      given: Gustavo
    - family: Wiese
      given: Igor
    - family: Dias
      given: Luiz Felipe
  citation-key: pintoHowScientistsDevelop2018
  container-title: >-
    2018 IEEE 25th International Conference on Software Analysis, Evolution and
    Reengineering (SANER)
  DOI: 10.1109/SANER.2018.8330263
  event-title: >-
    2018 IEEE 25th International Conference on Software Analysis, Evolution and
    Reengineering (SANER)
  issued:
    - year: 2018
      month: 3
  note: 'interest: 90'
  page: 582-591
  source: IEEE Xplore
  title: How do scientists develop scientific software? An external replication
  title-short: How do scientists develop scientific software?
  type: paper-conference

- id: plankensteinerIWIRLanguageEnabling2011
  accessed:
    - year: 2022
      month: 8
      day: 3
  author:
    - family: Plankensteiner
      given: Kassian
    - family: Montagnat
      given: Johan
    - family: Prodan
      given: Radu
  citation-key: plankensteinerIWIRLanguageEnabling2011
  container-title: >-
    Proceedings of the 6th workshop on Workflows in support of large-scale
    science - WORKS '11
  DOI: 10.1145/2110497.2110509
  event-place: Seattle, Washington, USA
  event-title: the 6th workshop
  ISBN: 978-1-4503-1100-7
  issued:
    - year: 2011
  language: en
  page: '97'
  publisher: ACM Press
  publisher-place: Seattle, Washington, USA
  source: DOI.org (Crossref)
  title: 'IWIR: a language enabling portability across grid workflow systems'
  title-short: IWIR
  type: paper-conference
  URL: http://dl.acm.org/citation.cfm?doid=2110497.2110509

- id: plenzHowTradeServer2019
  abstract: >-
    When running large scale systems, we strive to deliver both low tail latency
    and high utilization of servers. However, these two dimenions are at odds:
    increasing the average utilization of a system will have a detrimental
    impact on the tail latency.


    This talk provides a light-weight walkthrough of the important basics of
    queueing theory (avoiding unnecessary formalism), illustrates graphically
    several typical outcomes of this analysis, and closes with a few basic rules
    on how to think about utilization and tail latency.
  accessed:
    - year: 2023
      month: 12
      day: 15
  author:
    - family: Plenz
      given: Julius
  citation-key: plenzHowTradeServer2019
  container-title: SRE Con
  issued:
    - year: 2019
  language: en
  source: www.usenix.org
  title: How to Trade off Server Utilization and Tail Latency
  type: article-journal
  URL: https://www.usenix.org/conference/srecon19asia/presentation/plenz

- id: plesserReproducibilityVsReplicability2018
  accessed:
    - year: 2022
      month: 10
      day: 11
  author:
    - family: Plesser
      given: Hans E.
  citation-key: plesserReproducibilityVsReplicability2018
  container-title: Frontiers in Neuroinformatics
  ISSN: 1662-5196
  issued:
    - year: 2018
  source: Frontiers
  title: 'Reproducibility vs. Replicability: A Brief History of a Confused Terminology'
  title-short: Reproducibility vs. Replicability
  type: article-journal
  URL: https://www.frontiersin.org/articles/10.3389/fninf.2017.00076
  volume: '11'

- id: plouffeResearchReportRichness2001
  accessed:
    - year: 2022
      month: 6
      day: 2
  author:
    - family: Plouffe
      given: Christopher R.
    - family: Hulland
      given: John S.
    - family: Vandenbosch
      given: Mark
  citation-key: plouffeResearchReportRichness2001
  container-title: Information Systems Research
  container-title-short: Information Systems Research
  DOI: 10.1287/isre.12.2.208.9697
  ISSN: 1047-7047, 1526-5536
  issue: '2'
  issued:
    - year: 2001
      month: 6
  language: en
  page: 208-222
  source: DOI.org (Crossref)
  title: >-
    Research Report: Richness Versus Parsimony in Modeling Technology Adoption
    Decisions—Understanding Merchant Adoption of a Smart Card-Based Payment
    System
  title-short: Research Report
  type: article-journal
  URL: http://pubsonline.informs.org/doi/abs/10.1287/isre.12.2.208.9697
  volume: '12'

- id: poeWhatKnowDebating2010
  accessed:
    - year: 2024
      month: 1
      day: 3
  author:
    - family: Poe
      given: Curtis "Ovid"
  citation-key: poeWhatKnowDebating2010
  container-title: 'Ovid: A blog about the Perl programming language'
  issued:
    - year: 2010
      month: 8
      day: 19
  title: What to know before debating type systems | Ovid [blogs.perl.org]
  type: post-weblog
  URL: >-
    https://blogs.perl.org/users/ovid/2010/08/what-to-know-before-debating-type-systems.html

- id: pohlyHiFiCollectingHighfidelity2012
  abstract: >-
    Data provenance---a record of the origin and evolution of data in a
    system---is a useful tool for forensic analysis. However, existing
    provenance collection mechanisms fail to achieve sufficient breadth or
    fidelity to provide a holistic view of a system's operation over time. We
    present Hi-Fi, a kernel-level provenance system which leverages the Linux
    Security Modules framework to collect high-fidelity whole-system provenance.
    We demonstrate that Hi-Fi is able to record a variety of malicious behavior
    within a compromised system. In addition, our benchmarks show the collection
    overhead from Hi-Fi to be less than 1% for most system calls and 3% in a
    representative workload, while simultaneously generating a system
    measurement that fully reflects system evolution. In this way, we show that
    we can collect broad, high-fidelity provenance data which is capable of
    supporting detailed forensic analysis.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Pohly
      given: Devin J.
    - family: McLaughlin
      given: Stephen
    - family: McDaniel
      given: Patrick
    - family: Butler
      given: Kevin
  citation-key: pohlyHiFiCollectingHighfidelity2012
  collection-title: ACSAC '12
  container-title: Proceedings of the 28th Annual Computer Security Applications Conference
  DOI: 10.1145/2420950.2420989
  event-place: New York, NY, USA
  ISBN: 978-1-4503-1312-4
  issued:
    - year: 2012
      month: 12
      day: 3
  page: 259–268
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Hi-Fi: collecting high-fidelity whole-system provenance'
  title-short: Hi-Fi
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2420950.2420989

- id: pordesOpenScienceGrid2007
  abstract: >-
    The Open Science Grid (OSG) provides a distributed facility where the
    Consortium members provide guaranteed and opportunistic access to shared
    computing and storage resources. OSG provides support for and evolution of
    the infrastructure through activities that cover operations, security,
    software, troubleshooting, addition of new capabilities, and support for
    existing and engagement with new communities. The OSG SciDAC-2 project
    provides specific activities to manage and evolve the distributed
    infrastructure and support it's use. The innovative aspects of the project
    are the maintenance and performance of a collaborative (shared & common)
    petascale national facility over tens of autonomous computing sites, for
    many hundreds of users, transferring terabytes of data a day, executing tens
    of thousands of jobs a day, and providing robust and usable resources for
    scientific groups of all types and sizes. More information can be found at
    the OSG web site: www.opensciencegrid. org.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Pordes
      given: Ruth
    - family: Petravick
      given: Don
    - family: Kramer
      given: Bill
    - family: Olson
      given: Doug
    - family: Livny
      given: Miron
    - family: Roy
      given: Alain
    - family: Avery
      given: Paul
    - family: Blackburn
      given: Kent
    - family: Wenaus
      given: Torre
    - family: Würthwein
      given: Frank
    - family: Foster
      given: Ian
    - family: Gardner
      given: Rob
    - family: Wilde
      given: Mike
    - family: Blatecky
      given: Alan
    - family: McGee
      given: John
    - family: Quick
      given: Rob
  citation-key: pordesOpenScienceGrid2007
  container-title: 'Journal of Physics: Conference Series'
  DOI: 10.1088/1742-6596/78/1/012057
  ISSN: 1742-6588
  issue: '1'
  issued:
    - year: 2007
      month: 7
      day: 1
  note: 'interest: 84'
  source: University of Illinois Urbana-Champaign
  title: The open science grid
  type: article-journal
  URL: http://www.scopus.com/inward/record.url?scp=36049001139&partnerID=8YFLogxK
  volume: '78'

- id: pouchardComputationalReproducibilityScientific2019
  abstract: >-
    We propose an approach for improved reproducibility that includes capturing
    and relating provenance characteristics and performance metrics. We discuss
    two use cases: scientific reproducibility of results in the Energy Exascale
    Earth System Model (E3SM – previously ACME), and performance reproducibility
    in molecular dynamics workflows on HPC computing platforms. In order to
    capture and persist the provenance and performance data of these workflows,
    we have designed and developed the Chimbuko and ProvEn frameworks. Chimbuko
    captures provenance and enables detailed single workflow performance
    analysis. ProvEn is a hybrid, queriable system for storing and analyzing the
    provenance and performance metrics of multiple runs in workflow performance
    analysis campaigns. Workflow provenance and performance data output from
    Chimbuko can be visualized in a dynamic, multi-level visualization providing
    overview and zoom-in capabilities for areas of interest. Provenance and
    related performance data ingested into ProvEn is queriable and can be used
    to reproduce runs. In conclusion, our provenance-based approach highlights
    challenges in extracting information and gaps in the information collected.
    It is agnostic to the type of provenance data it captures so that both the
    reproducibility of scientific results and that of performance can be
    explored with our tools.
  accessed:
    - year: 2023
      month: 2
      day: 20
  author:
    - family: Pouchard
      given: Line
    - family: Baldwin
      given: Sterling
    - family: Elsethagen
      given: Todd
    - family: Jha
      given: Shantenu
    - family: Raju
      given: Bibi
    - family: Stephan
      given: Eric
    - family: Tang
      given: Li
    - family: Van Dam
      given: Kerstin Kleese
  citation-key: pouchardComputationalReproducibilityScientific2019
  container-title: International Journal of High Performance Computing Applications
  DOI: 10.1177/1094342019839124
  ISSN: 1094-3420
  issue: '5'
  issued:
    - year: 2019
      month: 4
      day: 8
  language: English
  number: BNL-211854-2019-JAAM
  page: 763-776
  publisher: SAGE
  source: www.osti.gov
  title: Computational reproducibility of scientific workflows at extreme scales
  type: article-journal
  URL: https://www.osti.gov/pages/biblio/1542776
  volume: '33'

- id: powellFuturePostdoc2015
  abstract: >-
    There is a growing number of postdocs and few places in academia for them to
    go. But change could be on the way.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Powell
      given: Kendall
  citation-key: powellFuturePostdoc2015
  container-title: Nature
  DOI: 10.1038/520144a
  ISSN: 1476-4687
  issue: '7546'
  issued:
    - year: 2015
      month: 4
      day: 1
  language: en
  license: 2015 Nature Publishing Group
  number: '7546'
  page: 144-147
  publisher: Nature Publishing Group
  source: www.nature.com
  title: The future of the postdoc
  type: article-journal
  URL: https://www.nature.com/articles/520144a
  volume: '520'

- id: prabhakaranIRONFileSystems2005
  abstract: >-
    Commodity file systems trust disks to either work or fail completely, yet
    modern disks exhibit more complex failure modes. We suggest a new
    fail-partial failure model for disks, which incorporates realistic localized
    faults such as latent sector errors and block corruption. We then develop
    and apply a novel failure-policy fingerprinting framework, to investigate
    how commodity file systems react to a range of more realistic disk failures.
    We classify their failure policies in a new taxonomy that measures their
    Internal RObustNess (IRON), which includes both failure detection and
    recovery techniques. We show that commodity file system failure policies are
    often inconsistent, sometimes buggy, and generally inadequate in their
    ability to recover from partial disk failures. Finally, we design,
    implement, and evaluate a prototype IRON file system, Linux ixt3, showing
    that techniques such as in-disk checksumming, replication, and parity
    greatly enhance file system robustness while incurring minimal time and
    space overheads.
  accessed:
    - year: 2023
      month: 1
      day: 18
  author:
    - family: Prabhakaran
      given: Vijayan
    - family: Bairavasundaram
      given: Lakshmi N.
    - family: Agrawal
      given: Nitin
    - family: Gunawi
      given: Haryadi S.
    - family: Arpaci-Dusseau
      given: Andrea C.
    - family: Arpaci-Dusseau
      given: Remzi H.
  citation-key: prabhakaranIRONFileSystems2005
  collection-title: SOSP '05
  container-title: Proceedings of the twentieth ACM symposium on Operating systems principles
  DOI: 10.1145/1095810.1095830
  event-place: New York, NY, USA
  ISBN: 978-1-59593-079-8
  issued:
    - year: 2005
      month: 10
      day: 20
  note: 'interest: 40'
  page: 206–220
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: IRON file systems
  type: paper-conference
  URL: https://doi.org/10.1145/1095810.1095830

- id: prasadLocatingSystemProblems2005
  abstract: >-
    Diagnosing complex performance or kernel debugging problems often requires
    kernel modifications with multiple rebuilds and reboots. This is tedious,
    time-consuming work that most developers would prefer to minimize.

    Systemtap uses the kprobes infrastructure to dynamically instrument the
    kernel and user applications. Systemtap instrumentation incurs low overhead
    when enabled, and zero overhead when disabled. SystemTap provides facilities
    to define instrumentation points in a high-level language, and to aggregate
    and analyze the instrumentation data. Details of the SystemTap architecture
    and implementation are presented, along with an example of its application.
  author:
    - family: Prasad
      given: Vara
    - family: Cohen
      given: William
    - family: Eigler
      given: Frank
    - family: Hunt
      given: Martin
    - family: Keniston
      given: Jim
    - family: Chen
      given: Brad
  citation-key: prasadLocatingSystemProblems2005
  container-title: Proceedings of the Linux Symposium
  event-place: Ottawa, Ontario, Canada
  issued:
    - year: 2005
      month: 7
      day: 20
  page: 49-64
  publisher: kernel.org
  publisher-place: Ottawa, Ontario, Canada
  title: Locating System Problems Using Dynamic Instrumentation
  type: paper-conference
  volume: '2'

- id: priceHDF5GoodFormat2014
  abstract: >-
    The FITS (Flexible Image Transport System) data format was developed in the
    late 1970s for storage and exchange of astronomy-related image data. Since
    then, it has become a standard file format not only for images, but also for
    radio interferometer data (e.g. UVFITS, FITS-IDI). But is FITS the right
    format for next-generation telescopes to adopt? The newer Hierarchical Data
    Format (HDF5) file format offers considerable advantages over FITS, but has
    yet to gain widespread adoption within radio astronomy. One of the major
    holdbacks is that HDF5 is not well supported by data reduction software
    packages. Here, we present a comparison of FITS, HDF5, and the
    MeasurementSet (MS) format for storage of interferometric data. In addition,
    we present a tool for converting between formats. We show that the
    underlying data model of FITS can be ported to HDF5, a first step toward
    achieving wider HDF5 support.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Price
      given: Danny C.
    - family: Barsdell
      given: Benjamin R.
    - family: Greenhill
      given: Lincoln J.
  citation-key: priceHDF5GoodFormat2014
  container-title: arXiv:1411.0507 [astro-ph]
  issued:
    - year: 2014
      month: 11
      day: 3
  note: 'interest: 30'
  source: arXiv.org
  title: Is HDF5 a good format to replace UVFITS?
  type: article-journal
  URL: http://arxiv.org/abs/1411.0507

- id: priedhorskyCharliecloudUnprivilegedContainers2017
  abstract: >-
    Supercomputing centers are seeing increasing demand for user-defined
    software stacks (UDSS), instead of or in addition to the stack provided by
    the center. These UDSS support user needs such as complex dependencies or
    build requirements, externally required configurations, portability, and
    consistency. The challenge for centers is to provide these services in a
    usable manner while minimizing the risks: security, support burden, missing
    functionality, and performance. We present Charliecloud, which uses the
    Linux user and mount namespaces to run industry-standard Docker containers
    with no privileged operations or daemons on center resources. Our simple
    approach avoids most security risks while maintaining access to the
    performance and functionality already on offer, doing so in just 800 lines
    of code. Charliecloud promises to bring an industry-standard UDSS user
    workflow to existing, minimally altered HPC resources.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Priedhorsky
      given: Reid
    - family: Randles
      given: Tim
  citation-key: priedhorskyCharliecloudUnprivilegedContainers2017
  container-title: >-
    Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis
  DOI: 10.1145/3126908.3126925
  event-place: Denver Colorado
  event-title: >-
    SC '17: The International Conference for High Performance Computing,
    Networking, Storage and Analysis
  ISBN: 978-1-4503-5114-0
  issued:
    - year: 2017
      month: 11
      day: 12
  language: en
  page: 1-10
  publisher: ACM
  publisher-place: Denver Colorado
  source: DOI.org (Crossref)
  title: >-
    Charliecloud: unprivileged containers for user-defined software stacks in
    HPC
  title-short: Charliecloud
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3126908.3126925

- id: ProFTPDUsingAuthUserFiles
  accessed:
    - year: 2024
      month: 1
      day: 23
  citation-key: ProFTPDUsingAuthUserFiles
  title: 'ProFTPD: Using AuthUserFiles'
  type: webpage
  URL: http://www.proftpd.org/docs/howto/AuthFiles.html

- id: ProvONEDataModel
  abstract: >-
    Provenance describes the origin and processing history of an artifact. Data
    provenance is an important form of metadata that explains how a particular
    data product was generated, by detailing the steps in the computational
    process producing it. Provenance information brings transparency and helps
    to audit and interpret data products. The state of the art scientific
    workflow systems (e.g. Kepler, Taverna, VisTrails, etc.) provide
    environments for specifying and enacting complex computational pipelines
    commonly referred to as scientific workflows. In such systems, provenance
    information is automatically captured in the form of execution traces.
    However, they often rely on proprietary formats that make the interchange of
    provenance information difficult. Furthermore, the workflow itself, which
    represents very useful information, may be disregarded in provenance traces.
    The evolution history of the workflow (i.e. its provenance) can likewise be
    missing. To address these shortcomings we propose ProvONE, a standard for
    scientific workflow provenance representation. ProvONE is defined as an
    extension of the W3C recommended standard PROV, aiming to capture the most
    relevant information concerning scientific workflow computational processes,
    and providing extension points to accommodate the specificities of
    particular scientific workflow systems.


    This document specifies the ProvONE model and details how its constituting
    parts are related to the W3C PROV standard. The description provided is
    complemented by examples including queries on ProvONE data.
  accessed:
    - year: 2022
      month: 7
      day: 26
  citation-key: ProvONEDataModel
  title: The ProvONE Data Model for Scientific Workflow Provenance
  type: webpage
  URL: >-
    http://jenkins-1.dataone.org/jenkins/view/Documentation%20Projects/job/ProvONE-Documentation-trunk/ws/provenance/ProvONE/v1/provone.html

- id: Ptrace
  accessed:
    - year: 2023
      month: 8
      day: 24
  citation-key: Ptrace
  container-title: Linux manual page
  title: ptrace
  type: webpage
  URL: https://man7.org/linux/man-pages/man2/ptrace.2.html

- id: qiuRetractionNoteLimited2019
  abstract: >-
    The authors wish to retract this Letter as follow-up work has highlighted
    that two errors were committed in the analyses used to produce Figs 4d and
    5. In Fig. 4d, a software bug led to an incorrect value of the
    discriminative power represented by the blue bar. The correct value is τ =
    0.17, as opposed to the value τ = 0.15 reported in the Letter. In Fig. 5,
    the model plot was produced with erroneous data. Produced with the correct
    data, the authors’ model does not account for the virality of both high- and
    low-quality information observed in the empirical Facebook data (inset). In
    the revised figure shown in the correction notice, the distribution of
    high-quality meme popularity predicted by the model is substantially broader
    than that of low-quality memes, which do not become popular. Thus, the
    original conclusion, that the model predicts that low-quality information is
    just as likely to go viral as high-quality information, is not supported.
    All other results in the Letter remain valid.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Qiu
      given: Xiaoyan
    - family: Oliveira
      given: Diego F. M.
    - family: Shirazi
      given: Alireza Sahami
    - family: Flammini
      given: Alessandro
    - family: Menczer
      given: Filippo
  citation-key: qiuRetractionNoteLimited2019
  container-title: Nature Human Behaviour
  container-title-short: Nat Hum Behav
  DOI: 10.1038/s41562-018-0507-0
  ISSN: 2397-3374
  issue: '1'
  issued:
    - year: 2019
      month: 1
  language: en
  license: 2019 Springer Nature Limited
  number: '1'
  page: 102-102
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Retraction Note: Limited individual attention and online virality of
    low-quality information
  title-short: Retraction Note
  type: article-journal
  URL: https://www.nature.com/articles/s41562-018-0507-0
  volume: '3'

- id: quansightlabsandopen-sourcecontributorsNativeDependencies2023
  accessed:
    - year: 2023
      month: 2
      day: 24
  author:
    - family: Quansight Labs and open-source contributors
      given: ''
  citation-key: quansightlabsandopen-sourcecontributorsNativeDependencies2023
  container-title: pypackaging-native
  issued:
    - year: 2023
      month: 1
      day: 2
  title: Native dependencies
  type: webpage
  URL: https://pypackaging-native.github.io/key-issues/native-dependencies/

- id: rahmanAssessingUtilityTAM2017
  abstract: >-
    Advanced Driver Assistance Systems (ADAS) are intended to enhance driver
    performance and improve transportation safety. The potential benefits of
    these technologies, such as reduction in number of crashes, enhancing driver
    comfort or convenience, decreasing environmental impact, etc., have been
    acknowledged by transportation safety researchers and federal transportation
    agencies. Although these systems afford safety advantages, they may also
    challenge the traditional role of drivers in operating vehicles. Driver
    acceptance, therefore, is essential for the implementation of these systems
    into the transportation system. Recognizing the need for research into the
    factors affecting driver acceptance, this study assessed the utility of the
    Technology Acceptance Model (TAM), the Theory of Planned Behavior (TPB), and
    the Unified Theory of Acceptance and Use of Technology (UTAUT) for modelling
    driver acceptance in terms of Behavioral Intention to use an ADAS. Each of
    these models propose a set of factors that influence acceptance of a
    technology. Data collection was done using two approaches: a driving
    simulator approach and an online survey approach. In both approaches,
    participants interacted with either a fatigue monitoring system or an
    adaptive cruise control system combined with a lane-keeping system. Based on
    their experience, participants responded to several survey questions to
    indicate their attitude toward using the ADAS and their perception of its
    usefulness, usability, etc. A sample of 430 surveys were collected for this
    study. Results found that all the models (TAM, TPB, and UTAUT) can explain
    driver acceptance with their proposed sets of factors, each explaining 71%
    or more of the variability in Behavioral Intention. Among the models, TAM
    was found to perform the best in modelling driver acceptance followed by
    TPB. The findings of this study confirm that these models can be applied to
    ADAS technologies and that they provide a basis for understanding driver
    acceptance.
  accessed:
    - year: 2022
      month: 6
      day: 3
  author:
    - family: Rahman
      given: Md Mahmudur
    - family: Lesch
      given: Mary F.
    - family: Horrey
      given: William J.
    - family: Strawderman
      given: Lesley
  citation-key: rahmanAssessingUtilityTAM2017
  container-title: Accident Analysis & Prevention
  container-title-short: Accident Analysis & Prevention
  DOI: 10.1016/j.aap.2017.09.011
  ISSN: '00014575'
  issued:
    - year: 2017
      month: 11
  language: en
  page: 361-373
  source: DOI.org (Crossref)
  title: >-
    Assessing the utility of TAM, TPB, and UTAUT for advanced driver assistance
    systems
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S0001457517303329
  volume: '108'

- id: ramReportFirstURSSI2018
  accessed:
    - year: 2022
      month: 8
      day: 25
  author:
    - family: Ram
      given: Karthik
    - family: Katz
      given: Daniel S.
    - family: Carver
      given: Jeffrey
    - family: Weber
      given: Nic
    - family: Gesing
      given: Sandra
  citation-key: ramReportFirstURSSI2018
  issued:
    - year: 2018
      month: 8
      day: 23
  note: 'interest: 80'
  title: Report from the first URSSI workshop
  type: webpage
  URL: https://urssi.us/blog/2018/08/23/report-from-the-first-urssi-workshop/

- id: raybournIncentivizingAdoptionSoftware2022
  abstract: >-
    Although many software teams across the laboratories comply with yearly
    software quality engineering (SQE) assessments, the practice of introducing
    quality into each phase of the software lifecycle, or the team processes,
    may vary substantially. Even with the support of a quality engineer, many
    teams struggle to adapt and right-size software engineering best practices
    in quality to ?t their context, and these activities aren?t framed in a way
    that motivates teams to take action. In short, software quality is often a
    ?check the box for compliance? activity instead of a cultural practice that
    both values software quality and knows how to achieve it. In this report, we
    present the results of our 6600 VISTA Innovation Tournament project,
    "Incentivizing and Motivating High Con?dence and Research Software Teams to
    Adopt the Practice of Quality." We present our ?ndings and roadmap for
    future work based on 1) a rapid review of relevant literature, 2) lessons
    learned from an internal design thinking workshop, and 3) an external
    Collegeville 2021 workshop. These activities provided an opportunity for
    team ideation and community engagement/feedback. Based on our ?ndings, we
    believe a coordinated effort (e.g. strategic communication campaign) aimed
    at diffusing the innovation of the practice of quality across Sandia
    National Laboratories could over time effect meaningful organizational
    change. As such, our roadmap addresses strategies for motivating and
    incentivizing individuals ranging from early career to seasoned software
    developers/scientists.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Raybourn
      given: Elaine M.
    - family: Milewicz
      given: Reed
    - family: Mundt
      given: Miranda R.
  citation-key: raybournIncentivizingAdoptionSoftware2022
  DOI: 10.2172/1845193
  issued:
    - year: 2022
      month: 2
      day: 1
  language: English
  note: 'interest: 90'
  number: SAND2022-1691
  publisher: Sandia National Lab. (SNL-NM), Albuquerque, NM (United States)
  source: www.osti.gov
  title: Incentivizing Adoption of Software Quality Practices.
  type: report
  URL: https://www.osti.gov/biblio/1845193/

- id: RecommendationsBestPractices
  citation-key: RecommendationsBestPractices
  language: en
  source: Zotero
  title: >-
    Recommendations on the Best Practices for the Collection of Sexual
    Orientation and Gender Identity Data on Federal Statistical Surveys
  type: report

- id: redheadOASPASecondStatement2013
  abstract: "Since OASPA released its first response to the Science ‘Sting’ article published in October, the OASPA Board has been looking at the implications of the findings for the organisation and its members. There has also been much discussion of the Science article, exploring the strengths and weaknesses of the exercise and associated data, along with...\_Read full article >"
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Redhead
      given: Claire
  citation-key: redheadOASPASecondStatement2013
  container-title: OASPA
  issued:
    - year: 2013
      month: 11
      day: 11
  language: en-GB
  title: >-
    OASPA’s second statement following the article in Science entitled “Who’s
    Afraid of Peer Review?”
  title-short: >-
    OASPA’s second statement following the article in Science entitled “Who’s
    Afraid of Peer Review?
  type: webpage
  URL: >-
    https://oaspa.org/oaspas-second-statement-following-the-article-in-science-entitled-whos-afraid-of-peer-review/

- id: reichGenePattern202006
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Reich
      given: Michael
    - family: Liefeld
      given: Ted
    - family: Gould
      given: Joshua
    - family: Lerner
      given: Jim
    - family: Tamayo
      given: Pablo
    - family: Mesirov
      given: Jill P
  citation-key: reichGenePattern202006
  container-title: Nature Genetics
  container-title-short: Nat Genet
  DOI: 10.1038/ng0506-500
  ISSN: 1061-4036, 1546-1718
  issue: '5'
  issued:
    - year: 2006
      month: 5
  language: en
  license: http://www.springer.com/tdm
  page: 500-501
  source: DOI.org (Crossref)
  title: GenePattern 2.0
  type: article-journal
  URL: https://www.nature.com/articles/ng0506-500
  volume: '38'

- id: reynoldsHamSandwichNation2013
  abstract: >-
    Though extensive due process protections apply to the investigation of
    crimes, and to criminal trials, perhaps the most important part of the
    criminal process -- the decision whether to charge a defendant, and with
    what -- is almost entirely discretionary.  Given the plethora of criminal
    laws and regulations in today's society, this due process gap allows
    prosecutors to charge almost anyone they take a deep interest in.  This
    Essay discusses the problem in the context of recent prosecutorial
    controversies involving the cases of Aaron Swartz and David Gregory, and
    offers some suggested remedies, along with a call for further discussion.
  accessed:
    - year: 2023
      month: 6
      day: 7
  author:
    - family: Reynolds
      given: Glenn Harlan
  citation-key: reynoldsHamSandwichNation2013
  DOI: 10.2139/ssrn.2203713
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2013
      month: 1
      day: 20
  language: en
  number: '2203713'
  publisher-place: Rochester, NY
  source: Social Science Research Network
  title: 'Ham Sandwich Nation: Due Process When Everything is a Crime'
  title-short: Ham Sandwich Nation
  type: article
  URL: https://papers.ssrn.com/abstract=2203713

- id: riordanBridgeTooFar2016
  abstract: >-
    That fateful decision, made by the leader of the world’s most powerful
    government, established the founding rhetoric for the SSC project, which
    proved difficult to abandon when it came time to enlist foreign
    partners.6Some physicists will counter that the SSC was in fact being
    pursued as an international project, with the US taking the lead in
    anticipation that other nations would follow; it had done so on large
    physics projects in the past and was doing so with the much costlier
    International Space Station.5 But that argument ignores the inconvenient
    truth that the gargantuan project was launched by the Reagan administration
    as a deliberate attempt to reestablish US leadership in a scientific
    discipline the nation had long dominated. If other nations were to become
    involved, they would have had to do so as junior partners in a
    multibillion-dollar enterprise led by US physicists.
  accessed:
    - year: 2023
      month: 9
      day: 20
  author:
    - family: Riordan
      given: Michael
  citation-key: riordanBridgeTooFar2016
  container-title: Physics Today
  container-title-short: Physics Today
  DOI: 10.1063/PT.3.3329
  ISSN: 0031-9228
  issue: '10'
  issued:
    - year: 2016
      month: 10
      day: 1
  page: 48-54
  source: Silverchair
  title: 'A bridge too far: The demise of the Superconducting Super Collider'
  title-short: A bridge too far
  type: article-journal
  URL: https://doi.org/10.1063/PT.3.3329
  volume: '69'

- id: ritchieScienceFictionsHow2020
  author:
    - family: Ritchie
      given: Stuart
  citation-key: ritchieScienceFictionsHow2020
  edition: Illustrated edition
  event-place: New York
  ISBN: 978-1-250-22269-5
  issued:
    - year: 2020
      month: 7
      day: 21
  language: English
  number-of-pages: '368'
  publisher: Metropolitan Books
  publisher-place: New York
  source: Amazon
  title: >-
    Science Fictions: How Fraud, Bias, Negligence, and Hype Undermine the Search
    for Truth
  title-short: Science Fictions
  type: book

- id: rizziTechniquesWeCreate2016
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Rizzi
      given: Eric F.
    - family: Elbaum
      given: Sebastian
    - family: Dwyer
      given: Matthew B.
  citation-key: rizziTechniquesWeCreate2016
  container-title: Proceedings of the 38th International Conference on Software Engineering
  DOI: 10.1145/2884781.2884835
  event-place: Austin Texas
  event-title: 'ICSE ''16: 38th International Conference on Software Engineering'
  ISBN: 978-1-4503-3900-1
  issued:
    - year: 2016
      month: 5
      day: 14
  language: en
  page: 132-143
  publisher: ACM
  publisher-place: Austin Texas
  source: DOI.org (Crossref)
  title: >-
    On the techniques we create, the tools we build, and their misalignments: a
    study of KLEE
  title-short: On the techniques we create, the tools we build, and their misalignments
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2884781.2884835

- id: robertsPublicationScientificFortran1969
  abstract: >-
    This article outlines some general principles which appear to be necessary
    if an international literature of published scientific programs is to be
    successfully established. Programming conventions are suggested for Fortran,
    together with several automatic documentation tools which have already been
    tried out and found useful.
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Roberts
      given: K. V.
  citation-key: robertsPublicationScientificFortran1969
  container-title: Computer Physics Communications
  container-title-short: Computer Physics Communications
  DOI: 10.1016/0010-4655(69)90011-3
  ISSN: 0010-4655
  issue: '1'
  issued:
    - year: 1969
      month: 7
      day: 1
  language: en
  page: 1-9
  source: ScienceDirect
  title: The publication of scientific fortran programs
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/0010465569900113
  volume: '1'

- id: rocklinBiasedBenchmarks2017
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Rocklin
      given: Matthew
  citation-key: rocklinBiasedBenchmarks2017
  genre: Blog
  issued:
    - year: 2017
      month: 3
      day: 9
  title: Biased Benchmarks
  type: post-weblog
  URL: https://matthewrocklin.com/blog/work/2017/03/09/biased-benchmarks

- id: rocklinDaskParallelComputation2015
  abstract: >-
    Dask enables parallel and out-of-core computation. We couple blocked
    algorithms with dynamic and memory aware task scheduling to achieve a
    parallel and out-of-core NumPy clone. We show how this extends the effective
    scale of modern hardware to larger datasets and discuss how these ideas can
    be more broadly applied to other parallel collections.
  accessed:
    - year: 2023
      month: 5
      day: 3
  author:
    - family: Rocklin
      given: Matthew
  citation-key: rocklinDaskParallelComputation2015
  DOI: 10.25080/Majora-7b98e3ed-013
  event-place: Austin, Texas
  event-title: Python in Science Conference
  issued:
    - year: 2015
  language: en
  page: 126-132
  publisher-place: Austin, Texas
  source: DOI.org (Crossref)
  title: 'Dask: Parallel Computation with Blocked algorithms and Task Scheduling'
  title-short: Dask
  type: paper-conference
  URL: https://conference.scipy.org/proceedings/scipy2015/matthew_rocklin.html

- id: rodriguez-perezReproducibilityCredibilityEmpirical2018
  abstract: >-
    Context

    Reproducibility of Empirical Software Engineering (ESE) studies is an
    essential part for improving their credibility, as it offers the opportunity
    to the research community to verify, evaluate and improve their research
    outcomes.

    Objective

    We aim to study reproducibility and credibility in ESE with a case study, by
    investigating how they have been addressed in studies where SZZ, a
    widely-used algorithm by Śliwerski, Zimmermann and Zeller to detect the
    origin of a bug, has been applied.

    Methodology

    We have performed a systematic literature review to evaluate publications
    that use SZZ. In total, 187 papers have been analyzed for reproducibility,
    reporting of limitations and use of improved versions of the algorithm.

    Results

    We have found a situation with a lot of room for improvement in ESE as
    reproducibility is not commonly found; factors that undermine the
    credibility of results are common. We offer some lessons learned and
    guidelines for researchers and reviewers to address this problem.

    Conclusion

    Reproducibility and other related aspects that ensure a high quality
    scientific process should be taken more into consideration by the ESE
    community in order to increase the credibility of the research results.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Rodríguez-Pérez
      given: Gema
    - family: Robles
      given: Gregorio
    - family: González-Barahona
      given: Jesús M.
  citation-key: rodriguez-perezReproducibilityCredibilityEmpirical2018
  container-title: Information and Software Technology
  container-title-short: Information and Software Technology
  DOI: 10.1016/j.infsof.2018.03.009
  ISSN: 0950-5849
  issued:
    - year: 2018
      month: 7
      day: 1
  language: en
  note: 'interest: 93'
  page: 164-176
  source: ScienceDirect
  title: >-
    Reproducibility and credibility in empirical software engineering: A case
    study based on a systematic literature review of the use of the SZZ
    algorithm
  title-short: Reproducibility and credibility in empirical software engineering
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0950584917304275
  volume: '99'

- id: rogersDiffusionInnovations1983
  author:
    - family: Rogers
      given: Everett M.
  call-number: HM101 .R57 1983
  citation-key: rogersDiffusionInnovations1983
  edition: 3rd ed
  event-place: 'New York : London'
  ISBN: 978-0-02-926650-2
  issued:
    - year: 1983
  number-of-pages: '453'
  publisher: Free Press ; Collier Macmillan
  publisher-place: 'New York : London'
  source: Library of Congress ISBN
  title: Diffusion of innovations
  type: book

- id: roperTestingReproducibilityRobustness
  abstract: >-
    Scientific results should not just be ‘repeatable’ (replicable in the same
    laboratory under identical conditions), but also ‘reproducible’ (replicable
    in other laboratories under similar conditions). Results should also, if
    possible, be ‘robust’ (replicable under a wide range of conditions). The
    reproducibility and robustness of only a small fraction of published
    biomedical results has been tested; furthermore, when reproducibility is
    tested, it is often not found. This situation is termed ‘the reproducibility
    crisis', and it is one the most important issues facing biomedicine. This
    crisis would be solved if it were possible to automate reproducibility
    testing. Here, we describe the semi-automated testing for reproducibility
    and robustness of simple statements (propositions) about cancer cell biology
    automatically extracted from the literature. From 12 260 papers, we
    automatically extracted statements predicted to describe experimental
    results regarding a change of gene expression in response to drug treatment
    in breast cancer, from these we selected 74 statements of high biomedical
    interest. To test the reproducibility of these statements, two different
    teams used the laboratory automation system Eve and two breast cancer cell
    lines (MCF7 and MDA-MB-231). Statistically significant evidence for
    repeatability was found for 43 statements, and significant evidence for
    reproducibility/robustness in 22 statements. In two cases, the automation
    made serendipitous discoveries. The reproduced/robust knowledge provides
    significant insight into cancer. We conclude that semi-automated
    reproducibility testing is currently achievable, that it could be scaled up
    to generate a substantive source of reliable knowledge and that automation
    has the potential to mitigate the reproducibility crisis.
  accessed:
    - year: 2022
      month: 4
      day: 14
  author:
    - family: Roper
      given: Katherine
    - family: Abdel-Rehim
      given: A.
    - family: Hubbard
      given: Sonya
    - family: Carpenter
      given: Martin
    - family: Rzhetsky
      given: Andrey
    - family: Soldatova
      given: Larisa
    - family: King
      given: Ross D.
  citation-key: roperTestingReproducibilityRobustness
  container-title: Journal of The Royal Society Interface
  DOI: 10.1098/rsif.2021.0821
  issue: '189'
  note: 'interest: 20'
  page: '20210821'
  publisher: Royal Society
  source: royalsocietypublishing.org (Atypon)
  title: >-
    Testing the reproducibility and robustness of the cancer biology literature
    by robot
  type: article-journal
  URL: https://royalsocietypublishing.org/doi/10.1098/rsif.2021.0821
  volume: '19'

- id: roquesSysMLVsUML2011
  author:
    - family: Roques
      given: Pascal
  citation-key: roquesSysMLVsUML2011
  event-title: MoDELS’11 Tutorial
  issued:
    - year: 2011
      month: 10
      day: 16
  title: 'SysML vs. UML 2: A Detailed Comparison'
  type: speech
  URL: >-
    https://ecs.wgtn.ac.nz/foswiki/pub/Events/MODELS2011/Material/MODELS_2011_T2-Roques-SysML_UML2.pdf

- id: rosenbergNextFrontierMaking2019
  author:
    - family: Rosenberg
      given: David
    - family: Fillion
      given: Yves
    - family: Teasley
      given: Rebecca
    - family: Sandoval-Solis
      given: Samuel
    - family: Hecht
      given: Jory
    - family: Zyl
      given: Jakobus
      dropping-particle: van
    - family: McMahon
      given: George
    - family: Horsburgh
      given: Jeffery
    - family: Kasprzyk
      given: Joseph
    - family: Tarboton
      given: David
  citation-key: rosenbergNextFrontierMaking2019
  container-title: Journal of Water Resources Planning and Management
  DOI: https://doi.org/10.1061/(ASCE)WR.1943-5452.0001215
  issued:
    - year: 2019
      month: 11
      day: 27
  page: 1-10
  title: 'The Next Frontier: Making Research More Reproducible'
  title-short: The Next Frontier
  type: article-journal
  URL: https://digitalcommons.usu.edu/water_pubs/156

- id: roseneSoftwareMaintainabilityWhat1981
  abstract: >-
    The terms reliability and maintainability are often misunderstood in the
    software field since software does not `break' or `wear out' in the physical
    sense; it either works in a given environment or it does not. This means
    that the program is either right or wrong in the environment. However, it
    does not follow that a program that is right is reliable or maintainable.
    For the purposes of this paper a program is maintainable if it meets the
    following two conditions:

    There is a high probability of determining the cause of a problem in a
    timely manner the first time it occurs, and

    There is a high probability of being able to modify the program without
    causing an error in some other part of the program.


    There are three important topics in developing a program which fulfills the
    above criteria: documentation, standards, and system architecture. This
    paper discusses the third topic since it is the real key to the development
    of maintainable software. A particular architecture of structured modular
    design using controlled communication between modules is presented together
    with its relationship to maintainability and reliability. The benefits of
    this approach are:

    - High isolation between modules.

    - Communication visibility and monitoring.

    - Error location.

    - Overload control.

    - Simplified control program.
     Transparency to multicomputer configurations.

    A method is presented for calculating maintainability parameters related to
    this architecture, and examples of these calculations are given and
    interpreted.
  accessed:
    - year: 2022
      month: 5
      day: 26
  author:
    - family: Rosene
      given: A.F.
    - family: Connolly
      given: J.E.
    - family: Bracy
      given: K.M.
  citation-key: roseneSoftwareMaintainabilityWhat1981
  container-title: IEEE Transactions on Reliability
  container-title-short: IEEE Trans. Rel.
  DOI: 10.1109/TR.1981.5221065
  ISSN: 0018-9529, 1558-1721
  issue: '3'
  issued:
    - year: 1981
      month: 8
  page: 240-245
  source: DOI.org (Crossref)
  title: Software Maintainability - What It Means and How to Achieve It
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/5221065/
  volume: R-30

- id: roslingBestStatsYou2007
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: roslingBestStatsYou2007
  collection-title: TED
  dimensions: '20:35'
  director:
    - family: Rosling
      given: Hans
  event-place: Monterey, CA
  issued:
    - year: 2007
      month: 1
      day: 16
  note: 'interest: 98'
  publisher-place: Monterey, CA
  source: YouTube
  title: The best stats you've ever seen
  type: motion_picture
  URL: https://www.youtube.com/watch?v=hVimVzgtD6w

- id: roslingHowNotBe2014
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: roslingHowNotBe2014
  collection-title: TED
  dimensions: '19:09'
  director:
    - family: Rosling
      given: Hans
    - family: Rosling
      given: Ola
  issued:
    - year: 2014
      month: 9
      day: 11
  note: 'interest: 98'
  source: YouTube
  title: How not to be ignorant about the world
  type: motion_picture
  URL: https://www.youtube.com/watch?v=Sm5xF-UYgdg

- id: rossantMovingAwayHDF5
  accessed:
    - year: 2022
      month: 4
      day: 14
  author:
    - family: Rossant
      given: Cyrille
  citation-key: rossantMovingAwayHDF5
  title: Moving away from HDF5
  type: post-weblog
  URL: https://cyrille.rossant.net/moving-away-hdf5/

- id: rothermelAnalyzingRegressionTest1996
  abstract: >-
    Regression testing is a necessary but expensive maintenance activity aimed
    at showing that code has not been adversely affected by changes. Regression
    test selection techniques reuse tests from an existing test suite to test a
    modified program. Many regression test selection techniques have been
    proposed, however, it is difficult to compare and evaluate these techniques
    because they have different goals. This paper outlines the issues relevant
    to regression test selection techniques, and uses these issues as the basis
    for a framework within which to evaluate the techniques. The paper
    illustrates the application of the framework by using it to evaluate
    existing regression test selection techniques. The evaluation reveals the
    strengths and weaknesses of existing techniques, and highlights some
    problems that future work in this area should address.
  author:
    - family: Rothermel
      given: G.
    - family: Harrold
      given: M.J.
  citation-key: rothermelAnalyzingRegressionTest1996
  container-title: IEEE Transactions on Software Engineering
  DOI: 10.1109/32.536955
  ISSN: 1939-3520
  issue: '8'
  issued:
    - year: 1996
      month: 8
  note: 'interest: 73'
  page: 529-551
  source: IEEE Xplore
  title: Analyzing regression test selection techniques
  type: article-journal
  volume: '22'

- id: rothermelMethodologyTestingSpreadsheets2001
  abstract: >-
    Spreadsheet languages, which include commercial spreadsheets and various
    research systems, have had a substantial impact on end-user computing.
    Research shows, however, that spreadsheets often contain faults; thus, we
    would like to provide at least some of the benefits of formal testing
    methodologies to the creators of spreadsheets. This article presents a
    testing methodology that adapts data flow adequacy criteria and coverage
    monitoring to the task of testing spreadsheets. To accommodate the
    evaluation model used with spreadsheets, and the interactive process by
    which they are created, our methodology is incremental. To accommodate the
    users of spreadsheet languages, we provide an interface to our methodology
    that does not require an understanding of testing theory. We have
    implemented our testing methodology in the context of the Forms/3 visual
    spreadsheet language. We report on the methodology, its time and space
    costs, and the mapping from the testing strategy to the user interface. In
    an empirical study, we found that test suites created according to our
    methodology detected, on average, 81% of the faults in a set of faulty
    spreadsheets, significantly outperforming randomly generated test suites.
  accessed:
    - year: 2022
      month: 8
      day: 31
  author:
    - family: Rothermel
      given: Gregg
    - family: Burnett
      given: Margaret
    - family: Li
      given: Lixin
    - family: Dupuis
      given: Christopher
    - family: Sheretov
      given: Andrei
  citation-key: rothermelMethodologyTestingSpreadsheets2001
  container-title: ACM Transactions on Software Engineering and Methodology
  container-title-short: ACM Trans. Softw. Eng. Methodol.
  DOI: 10.1145/366378.366385
  ISSN: 1049-331X
  issue: '1'
  issued:
    - year: 2001
      month: 1
      day: 1
  note: 'interest: 54'
  page: 110–147
  source: Jan. 2001
  title: A methodology for testing spreadsheets
  type: article-journal
  URL: https://doi.org/10.1145/366378.366385
  volume: '10'

- id: rougierReScienceJournalReproducible2019
  abstract: "Independent replication is one of the most powerful methods to verify published scientific studies. In computational science, it requires the reimplementation of the methods described in the original article by a different team of researchers. Replication is often performed by scientists who wish to gain a better understanding of a published method, but its results are rarely made public. ReScience\_C is a peer-reviewed journal dedicated to the publication of high-quality computational replications that provide added value to the scientific community. To this end, ReScience\_C requires replications to be reproducible and implemented using Open Source languages and libraries. In this article, we provide an overview of ReScience\_C’s goals and quality standards, outline the submission and reviewing processes, and summarize the experience of its first three years of operation, concluding with an outlook towards evolutions envisaged for the near future."
  author:
    - family: Rougier
      given: Nicolas P.
    - family: Hinsen
      given: Konrad
  citation-key: rougierReScienceJournalReproducible2019
  collection-title: Lecture Notes in Computer Science
  container-title: Reproducible Research in Pattern Recognition
  DOI: 10.1007/978-3-030-23987-9_14
  editor:
    - family: Kerautret
      given: Bertrand
    - family: Colom
      given: Miguel
    - family: Lopresti
      given: Daniel
    - family: Monasse
      given: Pascal
    - family: Talbot
      given: Hugues
  event-place: Cham
  ISBN: 978-3-030-23987-9
  issued:
    - year: 2019
  language: en
  page: 150-156
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: >-
    ReScience C: A Journal for Reproducible Replications in Computational
    Science
  title-short: ReScience C
  type: paper-conference

- id: rougierSustainableComputationalScience2017
  abstract: >-
    Computer science offers a large set of tools for prototyping, writing,
    running, testing, validating, sharing and reproducing results, however
    computational science lags behind. In the best case, authors may provide
    their source code as a compressed archive and they may feel confident their
    research is reproducible. But this is not exactly true. James Buckheit and
    David Donoho proposed more than two decades ago that an article about
    computational results is advertising, not scholarship. The actual
    scholarship is the full software environment, code, and data that produced
    the result. This implies new workflows, in particular in peer-reviews.
    Existing journals have been slow to adapt: source codes are rarely
    requested, hardly ever actually executed to check that they produce the
    results advertised in the article. ReScience is a peer-reviewed journal that
    targets computational research and encourages the explicit replication of
    already published research, promoting new and open-source implementations in
    order to ensure that the original research can be replicated from its
    description. To achieve this goal, the whole publishing chain is radically
    different from other traditional scientific journals. ReScience resides on
    GitHub where each new implementation of a computational study is made
    available together with comments, explanations, and software tests.
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Rougier
      given: Nicolas P.
    - family: Hinsen
      given: Konrad
    - family: Alexandre
      given: Frédéric
    - family: Arildsen
      given: Thomas
    - family: Barba
      given: Lorena
    - family: Benureau
      given: Fabien C. Y.
    - family: Brown
      given: C. Titus
    - family: Buyl
      given: Pierre
      non-dropping-particle: de
    - family: Caglayan
      given: Ozan
    - family: Davison
      given: Andrew P.
    - family: Delsuc
      given: Marc André
    - family: Detorakis
      given: Georgios
    - family: Diem
      given: Alexandra K.
    - family: Drix
      given: Damien
    - family: Enel
      given: Pierre
    - family: Girard
      given: Benoît
    - family: Guest
      given: Olivia
    - family: Hall
      given: Matt G.
    - family: Henriques
      given: Rafael Neto
    - family: Hinaut
      given: Xavier
    - family: Jaron
      given: Kamil S.
    - family: Khamassi
      given: Mehdi
    - family: Klein
      given: Almar
    - family: Manninen
      given: Tiina
    - family: Marchesi
      given: Pietro
    - family: McGlinn
      given: Dan
    - family: Metzner
      given: Christoph
    - family: Petchey
      given: Owen L.
    - family: Plesser
      given: Hans Ekkehard
    - family: Poisot
      given: Timothée
    - family: Ram
      given: Karthik
    - family: Ram
      given: Yoav
    - family: Roesch
      given: Etienne
    - family: Rossant
      given: Cyrille
    - family: Rostami
      given: Vahid
    - family: Shifman
      given: Aaron
    - family: Stachelek
      given: Joseph
    - family: Stimberg
      given: Marcel
    - family: Stollmeier
      given: Frank
    - family: Vaggi
      given: Federico
    - family: Viejo
      given: Guillaume
    - family: Vitay
      given: Julien
    - family: Vostinar
      given: Anya
    - family: Yurchak
      given: Roman
    - family: Zito
      given: Tiziano
  citation-key: rougierSustainableComputationalScience2017
  container-title: PeerJ Computer Science
  DOI: 10.7717/peerj-cs.142
  ISSN: 2376-5992
  issued:
    - year: 2017
      month: 12
      day: 18
  page: e142
  source: arXiv.org
  title: 'Sustainable computational science: the ReScience initiative'
  title-short: Sustainable computational science
  type: article-journal
  URL: http://arxiv.org/abs/1707.04393
  volume: '3'

- id: rubinCausalInferenceUsing2005
  abstract: >-
    Causal effects are defined as comparisons of potential outcomes under
    different treatments on a common set of units. Observed values of the
    potential outcomes are revealed by the assignment mechanism—a probabilistic
    model for the treatment each unit receives as a function of covariates and
    potential outcomes. Fisher made tremendous contributions to causal inference
    through his work on the design of randomized experiments, but the potential
    outcomes perspective applies to other complex experiments and nonrandomized
    studies as well. As noted by Kempthorne in his 1976 discussion of Savage's
    Fisher lecture, Fisher never bridged his work on experimental design and his
    work on parametric modeling, a bridge that appears nearly automatic with an
    appropriate view of the potential outcomes framework, where the potential
    outcomes and covariates are given a Bayesian distribution to complete the
    model specification. Also, this framework crisply separates scientific
    inference for causal effects and decisions based on such inference, a
    distinction evident in Fisher's discussion of tests of significance versus
    tests in an accept/reject framework. But Fisher never used the potential
    outcomes framework, originally proposed by Neyman in the context of
    randomized experiments, and as a result he provided generally flawed advice
    concerning the use of the analysis of covariance to adjust for posttreatment
    concomitants in randomized trials.
  accessed:
    - year: 2022
      month: 9
      day: 9
  author:
    - family: Rubin
      given: Donald B
  citation-key: rubinCausalInferenceUsing2005
  container-title: Journal of the American Statistical Association
  DOI: 10.1198/016214504000001880
  ISSN: 0162-1459
  issue: '469'
  issued:
    - year: 2005
      month: 3
      day: 1
  note: 'interest: 85'
  page: 322-331
  publisher: Taylor & Francis
  source: Taylor and Francis+NEJM
  title: Causal Inference Using Potential Outcomes
  type: article-journal
  URL: https://doi.org/10.1198/016214504000001880
  volume: '100'

- id: rubinsteynParakeetJustInTimeParallel2012
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Rubinsteyn
      given: Alex
    - family: Hielscher
      given: Eric
    - family: Weinman
      given: Nathaniel
    - family: Shasha
      given: Dennis
  citation-key: rubinsteynParakeetJustInTimeParallel2012
  event-title: 4th USENIX Workshop on Hot Topics in Parallelism (HotPar 12)
  issued:
    - year: 2012
  language: en
  note: 'interest: 75'
  source: www.usenix.org
  title: 'Parakeet: A {Just-In-Time} Parallel Accelerator for Python'
  title-short: Parakeet
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/hotpar12/workshop-program/presentation/rubinsteyn

- id: ruleExplorationExplanationComputational2018
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Rule
      given: Adam
    - family: Tabard
      given: Aurélien
    - family: Hollan
      given: James D.
  citation-key: ruleExplorationExplanationComputational2018
  container-title: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems
  DOI: 10.1145/3173574.3173606
  event-place: Montreal QC Canada
  event-title: 'CHI ''18: CHI Conference on Human Factors in Computing Systems'
  ISBN: 978-1-4503-5620-6
  issued:
    - year: 2018
      month: 4
      day: 19
  language: en
  note: 'interest: 61'
  page: 1-12
  publisher: ACM
  publisher-place: Montreal QC Canada
  source: DOI.org (Crossref)
  title: Exploration and Explanation in Computational Notebooks
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3173574.3173606

- id: rupprechtImprovingReproducibilityData2020
  abstract: >-
    Data science has become prevalent in a large variety of domains. Inherent in
    its practice is an exploratory, probing, and fact finding journey, which
    consists of the assembly, adaptation, and execution of complex data science
    pipelines. The trustworthiness of the results of such pipelines rests
    entirely on their ability to be reproduced with fidelity, which is difficult
    if pipelines are not documented or recorded minutely and consistently. This
    difficulty has led to a reproducibility crisis and presents a major obstacle
    to the safe adoption of the pipeline results in production environments. The
    crisis can be resolved if the provenance for each data science pipeline is
    captured transparently as pipelines are executed. However, due to the
    complexity of modern data science pipelines, transparently capturing
    sufficient provenance to allow for reproducibility is challenging. As a
    result, most existing systems require users to augment their code or use
    specific tools to capture provenance, which hinders productivity and results
    in a lack of adoption. In this paper, we present Ursprung,1 a transparent
    provenance collection system designed for data science environments.2 The
    Ursprung philosophy is to capture provenance and build lineage by
    integrating with the execution environment to automatically track static and
    runtime configuration parameters of data science pipelines. Rather than
    requiring data scientists to make changes to their code, Ursprung records
    basic provenance information from system-level sources and combines it with
    provenance from application-level sources (e.g., log files, stdout), which
    can be accessed and recorded through a domain-specific language. In our
    evaluation, we show that Ursprung is able to capture sufficient provenance
    for a variety of use cases and only adds an overhead of up to 4%.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Rupprecht
      given: Lukas
    - family: Davis
      given: James C.
    - family: Arnold
      given: Constantine
    - family: Gur
      given: Yaniv
    - family: Bhagwat
      given: Deepavali
  citation-key: rupprechtImprovingReproducibilityData2020
  container-title: Proceedings of the VLDB Endowment
  container-title-short: Proc. VLDB Endow.
  DOI: 10.14778/3415478.3415556
  ISSN: 2150-8097
  issue: '12'
  issued:
    - year: 2020
      month: 8
      day: 1
  page: 3354–3368
  source: ACM Digital Library
  title: >-
    Improving reproducibility of data science pipelines through transparent
    provenance capture
  type: article-journal
  URL: https://dl.acm.org/doi/10.14778/3415478.3415556
  volume: '13'

- id: russelHowMakeThis2008
  accessed:
    - year: 2024
      month: 1
      day: 8
  author:
    - family: Russel
      given: Rusty
  citation-key: russelHowMakeThis2008
  issued:
    - year: 2008
      month: 3
      day: 30
  title: How Do I Make This Hard to Misuse?
  type: post-weblog
  URL: https://ozlabs.org/~rusty/index.cgi/tech/2008-03-30.html

- id: ryanFoundationsEvidenceBasedPolicymaking2019
  author:
    - family: Ryan
      given: Paul D.
  citation-key: ryanFoundationsEvidenceBasedPolicymaking2019
  issued:
    - year: 2019
      month: 1
      day: 14
  number: H.R. 4174
  title: Foundations for Evidence-Based Policymaking Act of 2018
  type: legislation

- id: saeedizadeDDBWSDynamicDeadline2021
  abstract: >-
    Workflow scheduling has been excessively studied in different environments
    like clusters, grids, and clouds. Cloud is a scalable, cost-effective
    environment that allows users to access an unlimited amount of resources and
    offers a pay-as-you-go model. An increase in the users’ desire to run their
    workflow applications on clouds leads to the development of multi-tenant
    environments like workflow-as-a-service platforms (WaaS). By leveraging
    cloud features, WaaS offers an environment where users can submit their
    workflows for execution with different quality of service (QoS) attributes
    at different. The problem of finding an appropriate scheduling algorithm
    considering factors like resource heterogeneity and QoS requirements is an
    NP-complete problem. Most of the existing algorithms in the literature are
    designed to schedule a single instance of a workflow or have a static
    behavior. Using static scheduling in dynamic environments like WaaS can lead
    to a low planning success rate. Besides, while it is possible to share
    resources between users, for simplicity purposes a majority of proposed
    algorithms schedule at most one task on a computing resource at any given
    point in time. They also consider either the time or cost as a hard
    constraint during scheduling. To cover these limitations in this study, we
    propose DDBWS, a Dynamic, Deadline and Budget-aware, Workflow Scheduling
    algorithm that is designed specifically for the WaaS environments. DDBWS
    schedules workflows by solving a multi-resource packing problem. Unlike
    several existing algorithms, it considers both CPU and memory demands for
    tasks simultaneously. Also, it leverages containers to run multiple tasks on
    a VM concurrently. It uses a bi-factor to control the tradeoff between cost
    and resource utilization during the mapping of tasks to resources. Based on
    real-world workflow traces, we have generated 6 different datasets of
    synthetic workflows. To compare the performance of the proposed scheduling
    algorithm, we chose two of the state-of-the-art dynamic concurrent workflow
    scheduling algorithms called EPSM and MW-HBDCS. We have conducted several
    experiments on these datasets. The results of the performed experiments show
    that DDBWS achieves at least 96% planning success rate on 6 different
    workloads which is a comparable PSR. The proposed algorithm decreases the
    total leased VM numbers considerably. It also outperforms its rivals in
    terms of the total execution cost and decreases the overall execution cost
    by at least 8.03% and on average 32.08%. The 95% confidence interval for
    this decrease is 32.08 ± 14.1 based on 12 samples.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Saeedizade
      given: Ehsan
    - family: Ashtiani
      given: Mehrdad
  citation-key: saeedizadeDDBWSDynamicDeadline2021
  container-title: The Journal of Supercomputing
  container-title-short: J Supercomput
  DOI: 10.1007/s11227-021-03858-6
  ISSN: 1573-0484
  issue: '12'
  issued:
    - year: 2021
      month: 12
      day: 1
  language: en
  note: 'interest: 76'
  page: 14525-14564
  source: Springer Link
  title: >-
    DDBWS: a dynamic deadline and budget-aware workflow scheduling algorithm in
    workflow-as-a-service environments
  title-short: DDBWS
  type: article-journal
  URL: https://doi.org/10.1007/s11227-021-03858-6
  volume: '77'

- id: sakalisSplash3ProperlySynchronized2016
  abstract: >-
    Benchmarks are indispensable in evaluating the performance implications of
    new research ideas. However, their usefulness is compromised if they do not
    work correctly on a system under evaluation or, in general, if they cannot
    be used consistently to compare different systems. A well-known benchmark
    suite of parallel applications is the Splash-2 suite. Since its creation in
    the context of the DASH project, Splash-2 benchmarks have been widely used
    in research. However, Splash-2 was released over two decades ago and does
    not adhere to the recent C memory consistency model. This leads to
    unexpected and often incorrect behavior when some Splash-2 benchmarks are
    used in conjunction with contemporary compilers and hardware (simulated or
    real). Most importantly, we discovered critical performance bugs that may
    question some of the reported benchmark results. In this work, we analyze
    the Splash-2 benchmarks and expose data races and related performance bugs.
    We rectify the problematic benchmarks and evaluate the resulting
    performance. Our work contributes to the community a new sanitized version
    of the Splash-2 benchmarks, called the Splash-3 benchmark suite.
  accessed:
    - year: 2024
      month: 2
      day: 8
  author:
    - family: Sakalis
      given: Christos
    - family: Leonardsson
      given: Carl
    - family: Kaxiras
      given: Stefanos
    - family: Ros
      given: Alberto
  citation-key: sakalisSplash3ProperlySynchronized2016
  container-title: >-
    2016 IEEE International Symposium on Performance Analysis of Systems and
    Software (ISPASS)
  DOI: 10.1109/ISPASS.2016.7482078
  event-title: >-
    2016 IEEE International Symposium on Performance Analysis of Systems and
    Software (ISPASS)
  issued:
    - year: 2016
      month: 4
  page: 101-111
  source: IEEE Xplore
  title: 'Splash-3: A properly synchronized benchmark suite for contemporary research'
  title-short: Splash-3
  type: paper-conference
  URL: >-
    https://ieeexplore.ieee.org/abstract/document/7482078?casa_token=Uy0X7NXLNqcAAAAA:6l5GoGgYgAIopiywBBMNih_IIiS0P1olFyxxW7r_cHaeB_eB6bsyUpQ1KQk0083iqBPJ0vt8

- id: samuelEndtoEndProvenanceRepresentation2022
  abstract: |-
    Abstract 
                 
                  Background 
                  The advancement of science and technologies play an immense role in the way scientific experiments are being conducted. Understanding how experiments are performed and how results are derived has become significantly more complex with the recent explosive growth of heterogeneous research data and methods. Therefore, it is important that the provenance of results is tracked, described, and managed throughout the research lifecycle starting from the beginning of an experiment to its end to ensure reproducibility of results described in publications. However, there is a lack of interoperable representation of end-to-end provenance of scientific experiments that interlinks data, processing steps, and results from an experiment’s computational and non-computational processes. 
                 
                 
                  Results 
                  We present the “REPRODUCE-ME” data model and ontology to describe the end-to-end provenance of scientific experiments by extending existing standards in the semantic web. The ontology brings together different aspects of the provenance of scientific studies by interlinking non-computational data and steps with computational data and steps to achieve understandability and reproducibility. We explain the important classes and properties of the ontology and how they are mapped to existing ontologies like PROV-O and P-Plan. The ontology is evaluated by answering competency questions over the knowledge base of scientific experiments consisting of computational and non-computational data and steps. 
                 
                 
                  Conclusion 
                  We have designed and developed an interoperable way to represent the complete path of a scientific experiment consisting of computational and non-computational steps. We have applied and evaluated our approach to a set of scientific experiments in different subject domains like computational science, biological imaging, and microscopy.
  accessed:
    - year: 2022
      month: 8
      day: 3
  author:
    - family: Samuel
      given: Sheeba
    - family: König-Ries
      given: Birgitta
  citation-key: samuelEndtoEndProvenanceRepresentation2022
  container-title: Journal of Biomedical Semantics
  container-title-short: J Biomed Semant
  DOI: 10.1186/s13326-021-00253-1
  ISSN: 2041-1480
  issue: '1'
  issued:
    - year: 2022
      month: 12
  language: en
  page: '1'
  source: DOI.org (Crossref)
  title: >-
    End-to-End provenance representation for the understandability and
    reproducibility of scientific experiments using a semantic approach
  type: article-journal
  URL: https://jbiomedsem.biomedcentral.com/articles/10.1186/s13326-021-00253-1
  volume: '13'

- id: sandersModifiedNewtonianDynamics2002
  abstract: >-
    Modified Newtonian dynamics (MOND) is an empirically motivated modification
    of Newtonian gravity or inertia suggested by Milgrom as an alternative to
    cosmic dark matter. The basic idea is that at accelerations below ao ≈ 10−8
    cm/s2 ≈ cHo/6 the effective gravitational attraction approaches , where gn
    is the usual Newtonian acceleration. This simple algorithm yields flat
    rotation curves for spiral galaxies and a mass-rotation velocity relation of
    the form M ∝ V4 that forms the basis for the observed luminosity–rotation
    velocity relation—the Tully-Fisher law. We review the phenomenological
    success of MOND on scales ranging from dwarf spheroidal galaxies to
    superclusters and demonstrate that the evidence for dark matter can be
    equally well interpreted as evidence for MOND. We discuss the possible
    physical basis for an acceleration-based modification of Newtonian dynamics
    as well as the extention of MOND to cosmology and structure formation.
  accessed:
    - year: 2022
      month: 5
      day: 2
  author:
    - family: Sanders
      given: Robert H.
    - family: McGaugh
      given: Stacy S.
  citation-key: sandersModifiedNewtonianDynamics2002
  container-title: Annual Review of Astronomy and Astrophysics
  DOI: 10.1146/annurev.astro.40.060401.093923
  issue: '1'
  issued:
    - year: 2002
  page: 263-317
  source: Annual Reviews
  title: Modified Newtonian Dynamics as an Alternative to Dark Matter
  type: article-journal
  URL: https://doi.org/10.1146/annurev.astro.40.060401.093923
  volume: '40'

- id: SandiaAnalysisWorkbench
  abstract: >-
    Integration Workflow Analysis The Sandia Analysis Workbench (SAW) is a
    family of software applications that boost productivity and quality by
    making modeling and simulation easier while enforcing best practices and
    supporting ubiquitous V&V. Capabilities include workflow management, model
    bui...
  accessed:
    - year: 2022
      month: 6
      day: 14
  citation-key: SandiaAnalysisWorkbench
  container-title: Sandia Analysis Workbench
  language: en-US
  title: Sandia Analysis Workbench (SAW)
  type: webpage
  URL: https://www.sandia.gov/saw/

- id: sandveTenSimpleRules2013
  accessed:
    - year: 2023
      month: 11
      day: 20
  author:
    - family: Sandve
      given: Geir Kjetil
    - family: Nekrutenko
      given: Anton
    - family: Taylor
      given: James
    - family: Hovig
      given: Eivind
  citation-key: sandveTenSimpleRules2013
  container-title: PLOS Computational Biology
  container-title-short: PLOS Computational Biology
  DOI: 10.1371/journal.pcbi.1003285
  ISSN: 1553-7358
  issue: '10'
  issued:
    - year: 2013
      month: 10
      day: 24
  language: en
  page: e1003285
  publisher: Public Library of Science
  source: PLoS Journals
  title: Ten Simple Rules for Reproducible Computational Research
  type: article-journal
  URL: >-
    https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285
  volume: '9'

- id: santana-perezReproducibilityScientificWorkflows2015
  abstract: >-
    It is commonly agreed that in silico scientific experiments should be
    executable and repeatable processes. Most of the current approaches for
    computational experiment conservation and reproducibility have focused so
    far on two of the main components of the experiment, namely, data and
    method. In this paper, we propose a new approach that addresses the third
    cornerstone of experimental reproducibility: the equipment. This work
    focuses on the equipment of a computational experiment, that is, the set of
    software and hardware components that are involved in the execution of a
    scientific workflow. In order to demonstrate the feasibility of our
    proposal, we describe a use case scenario on the Text Analytics domain and
    the application of our approach to it. From the original workflow, we
    document its execution environment, by means of a set of semantic models and
    a catalogue of resources, and generate an equivalent infrastructure for
    reexecuting it.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Santana-Perez
      given: Idafen
    - family: Pérez-Hernández
      given: María S.
  citation-key: santana-perezReproducibilityScientificWorkflows2015
  container-title: Scientific Programming
  DOI: 10.1155/2015/243180
  ISSN: 1058-9244
  issued:
    - year: 2015
      month: 2
      day: 24
  language: en
  note: 'interest: 90'
  page: e243180
  publisher: Hindawi
  source: www.hindawi.com
  title: >-
    Towards Reproducibility in Scientific Workflows: An Infrastructure-Based
    Approach
  title-short: Towards Reproducibility in Scientific Workflows
  type: article-journal
  URL: https://www.hindawi.com/journals/sp/2015/243180/
  volume: '2015'

- id: sarLineageFileSystem
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Sar
      given: Can
    - family: Cao
      given: Pei
  citation-key: sarLineageFileSystem
  title: Lineage File System
  type: manuscript
  URL: http://crypto.stanford.edu/~cao/lineage.html

- id: schaureckerSuperresolvingDarkMatter2021
  abstract: >-
    Generative deep learning methods built upon Convolutional Neural Networks
    (CNNs) provide a great tool for predicting non-linear structure in
    cosmology. In this work we predict high resolution dark matter halos from
    large scale, low resolution dark matter only simulations. This is achieved
    by mapping lower resolution to higher resolution density fields of
    simulations sharing the same cosmology, initial conditions and box-sizes. To
    resolve structure down to a factor of 8 increase in mass resolution, we use
    a variation of U-Net with a conditional GAN, generating output that visually
    and statistically matches the high resolution target extremely well. This
    suggests that our method can be used to create high resolution density
    output over Gpc/h box-sizes from low resolution simulations with negligible
    computational effort.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Schaurecker
      given: David
    - family: Li
      given: Yin
    - family: Tinker
      given: Jeremy
    - family: Ho
      given: Shirley
    - family: Refregier
      given: Alexandre
  citation-key: schaureckerSuperresolvingDarkMatter2021
  container-title: arXiv:2111.06393 [astro-ph]
  issued:
    - year: 2021
      month: 11
      day: 11
  source: arXiv.org
  title: Super-resolving Dark Matter Halos using Generative Deep Learning
  type: article-journal
  URL: http://arxiv.org/abs/2111.06393

- id: scheideggerTacklingProvenanceChallenge2008
  abstract: >-
    VisTrails is a new workflow and provenance management system that provides
    support for scientific data exploration and visualization. Whereas workflows
    have been traditionally used to automate repetitive tasks, for applications
    that are exploratory in nature, change is the norm. VisTrails uses a new
    change-based provenance mechanism, which was designed to handle rapidly
    evolving workflows. It uniformly and automatically captures provenance
    information for data products and for the evolution of the workflows used to
    generate these products. In this paper, we describe how the VisTrails
    provenance data are organized in layers and present a first approach for
    querying this data that we developed to tackle the Provenance Challenge
    queries. Copyright © 2007 John Wiley & Sons, Ltd.
  accessed:
    - year: 2023
      month: 7
      day: 18
  author:
    - family: Scheidegger
      given: Carlos
    - family: Koop
      given: David
    - family: Santos
      given: Emanuele
    - family: Vo
      given: Huy
    - family: Callahan
      given: Steven
    - family: Freire
      given: Juliana
    - family: Silva
      given: Cláudio
  citation-key: scheideggerTacklingProvenanceChallenge2008
  container-title: 'Concurrency and Computation: Practice and Experience'
  DOI: 10.1002/cpe.1237
  ISSN: 1532-0634
  issue: '5'
  issued:
    - year: 2008
  language: en
  license: Copyright © 2007 John Wiley & Sons, Ltd.
  page: 473-483
  source: Wiley Online Library
  title: Tackling the Provenance Challenge one layer at a time
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1237
  volume: '20'

- id: schimmackMostPublishedResults2021
  abstract: >-
    Peer Reviewed by Editors of Biostatistics “You have produced a nicely
    written paper that seems to be mathematically correct and I enjoyed reading”
    (Professor Dimitris Rizopoulos & P…
  accessed:
    - year: 2022
      month: 11
      day: 14
  author:
    - family: Schimmack
      given: Ulrich
  citation-key: schimmackMostPublishedResults2021
  container-title: Replicability-Index
  issued:
    - year: 2021
      month: 8
      day: 10
  language: en-US
  note: 'interest: 87'
  title: Most published results in medical journals are not false
  type: webpage
  URL: https://replicationindex.com/2021/08/10/fpr-medicine/

- id: schlawackSemanticVersioningWill2021
  abstract: >-
    The widely used Python package cryptography changed their build system to
    use Rust for low-level code which caused an emotional GitHub thread.
    Enthusiasts of 32-bit hardware from the 1990s aside, there was a vocal
    faction that stipulated adherence to Semantic Versioning from the
    maintainers – claiming it would’ve prevented all grief. I will show you not
    only why this is wrong, but also how relying on Semantic Versioning hurts
    you.
  accessed:
    - year: 2022
      month: 4
      day: 20
  author:
    - family: Schlawack
      given: Hynek
  citation-key: schlawackSemanticVersioningWill2021
  container-title: Hynek Schlawack
  issued:
    - year: 2021
      month: 3
      day: 2
  language: en-us
  title: Semantic Versioning Will Not Save You
  type: post-weblog
  URL: https://hynek.me/articles/semver-will-not-save-you/

- id: schlueterKikLeftpadNpm
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Schlueter
      given: Isaac
  citation-key: schlueterKikLeftpadNpm
  container-title: npm Blog (Archive)
  title: kik, left-pad, and npm
  title-short: kik, left-pad, and npm
  type: post-weblog
  URL: https://blog.npmjs.org/post/141577284765/kik-left-pad-and-npm

- id: schreinerShouldYouUse
  abstract: >-
    Bound version constraints (upper caps) are starting to show up in the Python

    ecosystem. This is causing real world problems with

    libraries following this recommendation, and is likely to continue to get

    worse; this practice does not scale to large

    numbers of libraries or large numbers of users. In this discussion I would
    like

    to explain why always providing an upper limit causes far more harm than
    good

    even for true SemVer libraries, why libraries that pin upper limits require

    more frequent updates rather than less, and why it is not scalable.  After

    reading this, hopefully you will always consider every cap you add, you will

    know the (few) places where pinning an upper limit is reasonable, and will

    possibly even avoid using libraries that pin upper limits needlessly until
    the

    author updates them to remove these pins.

    If this 10,000 word behemoth is a bit long for you, then skip around using
    the

    table of contents, or see the TL;DR section at the end, or read

    version numbers by Bernát Gábor, which is shorter but is a

    fantastic read with good examples and cute dog pictures. Or Hynek’s Semantic

    Versioning Will Not Save You

    Be sure to check at least the JavaScript project

    analysis before you leave!

    Also be warned, I pick on Poetry quite a bit.  The rising popularity of

    Poetry is likely due to the simplicity of having one tool vs. many for

    packaging, but it happens to also have a special dependency solver, a new
    upper

    bound syntax, and a strong recommendation to always limit upper

    versions - in direct opposition to members of the

    Python core developer team and PyPA developers. Not

    all libraries with excessive version capping are Poetry projects (like

    TensorFlow), but many, many of them are. To be clear, Poetry doesn’t force

    version pinning on you, but it does push you really, really hard to always

    version cap, and it’s targeting new Python users that don’t know any better
    yet

    than to accept bad recommendations. And these affect the whole ecosystem,

    including users who do not use poetry, but want to depend on libraries that
    do!

    I do really like other aspects of Poetry, and would like to

    eventually help it build binary packages with Scikit-build

    (CMake) via a plugin, and I use it on some of my projects happily. If I
    don’t

    pick on Poetry enough for you, don’t worry, I have a follow-up

    post that picks on it in much more detail. Also, check out

    pdm, which gives many of the benefits of Poetry while following PEP
    standards.
  accessed:
    - year: 2022
      month: 4
      day: 20
  author:
    - family: Schreiner
      given: Henry
  citation-key: schreinerShouldYouUse
  container-title: ISciNumPy.dev
  language: en
  title: Should You Use Upper Bound Version Constraints?
  type: post-weblog
  URL: https://iscinumpy.dev/post/bound-version-constraints/

- id: schuirmannComparisonTwoOneSided1987
  abstract: >-
    The statistical test of the hypothesis of no difference between the average
    bioavailabilities of two drug formulations, usually supplemented by an
    assessment of what the power of the statistical test would have been if the
    true averages had been inequivalent, continues to be used in the statistical
    analysis of bioavailability/bioequivalence studies. In the present article,
    this Power Approach (which in practice usually consists of testing the
    hypothesis of no difference at level 0.05 and requiring an estimated power
    of 0.80) is compared to another statistical approach, the Two One-Sided
    Tests Procedure, which leads to the same conclusion as the approach proposed
    by Westlake (2) based on the usual (shortest) 1–2α confidence interval for
    the true average difference. It is found that for the specific choice of
    α=0.05 as the nominal level of the one-sided tests, the two one-sided tests
    procedure has uniformly superior properties to the power approach in most
    cases. The only cases where the power approach has superior properties when
    the true averages are equivalent correspond to cases where the chance of
    concluding equivalence with the power approach when the true averages are
    notequivalent exceeds 0.05. With appropriate choice of the nominal level of
    significance of the one-sided tests, the two one-sided tests procedure
    always has uniformly superior properties to the power approach. The two
    one-sided tests procedure is compared to the procedure proposed by Hauck and
    Anderson (1).
  accessed:
    - year: 2024
      month: 1
      day: 29
  author:
    - family: Schuirmann
      given: Donald J.
  citation-key: schuirmannComparisonTwoOneSided1987
  DOI: 10.1007/bf01068419
  issued:
    - year: 1987
      month: 12
      day: 1
  source: Zenodo
  title: >-
    A comparison of the Two One-Sided Tests Procedure and the Power Approach for
    assessing the equivalence of average bioavailability
  type: article-journal
  URL: https://zenodo.org/records/1232484

- id: schunemannReviewsRapidRapid2015
  accessed:
    - year: 2023
      month: 10
      day: 27
  author:
    - family: Schünemann
      given: Holger J.
    - family: Moja
      given: Lorenzo
  citation-key: schunemannReviewsRapidRapid2015
  container-title: Systematic Reviews
  container-title-short: Systematic Reviews
  DOI: 10.1186/2046-4053-4-4
  ISSN: 2046-4053
  issue: '1'
  issued:
    - year: 2015
      month: 1
      day: 14
  page: '4'
  source: BioMed Central
  title: 'Reviews: Rapid! Rapid! Rapid! …and systematic'
  title-short: Reviews
  type: article-journal
  URL: https://doi.org/10.1186/2046-4053-4-4
  volume: '4'

- id: schwabMakingScientificComputations2000
  abstract: >-
    To verify a research paper's computational results, readers typically have
    to recreate them from scratch. ReDoc is a simple software filing system for
    authors that lets readers easily reproduce computational results using
    standardized rules and commands.
  author:
    - family: Schwab
      given: M.
    - family: Karrenbach
      given: N.
    - family: Claerbout
      given: J.
  citation-key: schwabMakingScientificComputations2000
  container-title: Computing in Science & Engineering
  DOI: 10.1109/5992.881708
  ISSN: 1558-366X
  issue: '6'
  issued:
    - year: 2000
      month: 11
  note: 'interest: 87'
  page: 61-67
  source: IEEE Xplore
  title: Making scientific computations reproducible
  type: article-journal
  volume: '2'

- id: scottmeyerWhySailsWhen2014
  abstract: >-
    Source: http://tech.yandex.ru/events/cpp-part...


    I especially like 36:58


    The Vasa was a 17th-century Swedish warship which suffered such feature
    creep during construction that it sank shortly after leaving the harbour on
    its maiden voyage. In the early 1990s, the C++ standardisation committee
    adopted the Vasa as a cautionary tale, discouraging prospective language
    extensions with "Remember the Vasa!" Yet C++ continued to grow, and by the
    time C++ was standardised, its complexity made the Vasa look like a rowboat.


    The Vasa sank, however, while C++ cruised, and it looks likely to continue
    doing so even as the latest revised standards (C++11 and C++14) add dozens
    of new features, each with its own idiosyncrasies. Clearly, C++ has gotten
    some important things right. In this talk, Scott Meyers considers the
    lessons to be learned from the ongoing success of a complex programming
    language that's over 30 years old, yet very much alive and kicking.
  accessed:
    - year: 2023
      month: 4
      day: 4
  author:
    - literal: Scott Meyer
  citation-key: scottmeyerWhySailsWhen2014
  issued:
    - year: 2014
      month: 6
      day: 23
  title: Why C++ Sails When the Vasa Sank
  type: speech
  URL: https://www.youtube.com/watch?v=ltCgzYcpFUI

- id: seagerNewCalculationRecombination1999
  abstract: >-
    We have developed an improved recombination calculation of H, He I, and He
    II in the early universe that involves a line-by-line treatment of each
    atomic level. We find two major differences compared with previous
    calculations. First, the ionization fraction xe is approximately 10% smaller
    for redshifts <\~800 because of nonequilibrium processes in the excited
    states of H. Second, He I recombination is much slower than previously
    thought, and it is delayed until just before H recombines. We describe the
    basic physics behind the new results and present a simple way to reproduce
    our calculation. This should enable a fast computation of the ionization
    history (and of the quantities such as the power spectrum of cosmic
    microwave background anisotropies that depend on it) for arbitrary
    cosmologies, without the need to consider the hundreds of atomic levels used
    in our complete model.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Seager
      given: S.
    - family: Sasselov
      given: D. D.
    - family: Scott
      given: D.
  citation-key: seagerNewCalculationRecombination1999
  container-title: The Astrophysical Journal
  DOI: 10.1086/312250
  ISSN: 0004-637X
  issued:
    - year: 1999
      month: 9
      day: 1
  note: 'ADS Bibcode: 1999ApJ...523L...1S'
  page: L1-L5
  source: NASA ADS
  title: A New Calculation of the Recombination Epoch
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1999ApJ...523L...1S
  volume: '523'

- id: seljakLineofSightIntegrationApproach1996
  abstract: >-
    We present a new method for calculating linear cosmic microwave background
    (CM B) anisotropy spectra based on integration over sources along the photon
    past light cone. In this approach the temperature anisotropy is written as a
    time integral over the product of a geometrical term and a source term. The
    geometrical term is given by radial eigenfunctions, which do not depend on
    the particular cosmological model. The source term can be expressed in terms
    of photon, baryon, and metric perturbations, all of which can be calculated
    using a small number of differential equations. This split clearly separates
    the dynamical from the geometrical effects on the CMB anisotropies. More
    importantly, it allows us to significantly reduce the computational time
    compared to standard methods. This is achieved because the source term,
    which depends on the model and is generally the most time-consuming part of
    calculation, is a slowly varying function of wavelength and needs to be
    evaluated only in a small number of points. The geometrical term, which
    oscillates much more rapidly than the source term, does not depend on the
    particular model and can be precomputed in advance. Standard methods that do
    not separate the two terms require a much higher number of evaluations. The
    new method leads to about 2 orders of magnitude reduction in CPU time when
    compared to standard methods and typically requires a few minutes on a
    workstation for a single model. The method should be especially useful for
    accurate determinations of cosmological parameters from CMB anisotropy and
    polarization measurements that will become with the next generation of
    experiments. A program implementing this method can be obtained from the
    authors.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Seljak
      given: Uros
    - family: Zaldarriaga
      given: Matias
  citation-key: seljakLineofSightIntegrationApproach1996
  container-title: The Astrophysical Journal
  DOI: 10.1086/177793
  ISSN: 0004-637X
  issued:
    - year: 1996
      month: 10
      day: 1
  note: 'ADS Bibcode: 1996ApJ...469..437S'
  page: '437'
  source: NASA ADS
  title: >-
    A Line-of-Sight Integration Approach to Cosmic Microwave Background
    Anisotropies
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1996ApJ...469..437S
  volume: '469'

- id: SenateReportPrewar2022
  abstract: >-
    The Senate Report on Iraqi WMD Intelligence (formally, the "Report of the
    Select Committee on Intelligence on the U.S. Intelligence Community's Prewar
    Intelligence Assessments on Iraq") was the report by the United States
    Senate Select Committee on Intelligence concerning the U.S. intelligence
    community's assessments of Iraq during the time leading up to the 2003
    invasion of Iraq. The report, which was released on July 9, 2004, identified
    numerous failures in the intelligence-gathering and -analysis process. The
    report found that these failures led to the creation of inaccurate materials
    that misled both government policy makers and the American public.

    The Committee's nine Republicans and eight Democrats agreed on the report's
    major conclusions and unanimously endorsed its findings. They disagreed,
    though, on the impact that statements on Iraq by senior members of the Bush
    administration had on the intelligence process.  The second phase of the
    investigation, addressing the way senior policymakers used the intelligence,
    was published on May 25, 2007.  Portions of the phase II report not released
    at that time include the review of public statements by U.S. government
    leaders prior to the war, and the assessment of the activities of Douglas
    Feith and the Pentagon's Office of Special Plans.
  accessed:
    - year: 2022
      month: 8
      day: 28
  citation-key: SenateReportPrewar2022
  container-title: Wikipedia
  issued:
    - year: 2022
      month: 8
      day: 21
  language: en
  license: Creative Commons Attribution-ShareAlike License
  note: 'Page Version ID: 1105631972'
  source: Wikipedia
  title: Senate Report on Pre-war Intelligence on Iraq
  type: entry-encyclopedia
  URL: >-
    https://en.wikipedia.org/w/index.php?title=Senate_Report_on_Pre-war_Intelligence_on_Iraq&oldid=1105631972

- id: sfiligoiAcceleratingKeyBioinformatics2021
  abstract: >-
    Most experimental sciences now rely on computing, and biological sciences
    are no exception. As datasets get bigger, so do the computing costs, making
    proper optimization of the codes used by scientists increasingly important.
    Many of the codes developed in recent years are based on the Python-based
    NumPy, due to its ease of use and good performance characteristics. The
    composable nature of NumPy, however, does not generally play well with the
    multi-tier nature of modern CPUs, making any non-trivial multi-step
    algorithm limited by the external memory access speeds, which are hundreds
    of times slower than the CPU's compute capabilities. In order to fully
    utilize the CPU compute capabilities, one must keep the working memory
    footprint small enough to fit in the CPU caches, which requires splitting
    the problem into smaller portions and fusing together as many steps as
    possible. In this paper, we present changes based on these principles to two
    important functions in the scikit-bio library, principal coordinates
    analysis and the Mantel test, that resulted in over 100x speed improvement
    in these widely used, general-purpose tools.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Sfiligoi
      given: Igor
    - family: McDonald
      given: Daniel
    - family: Knight
      given: Rob
  citation-key: sfiligoiAcceleratingKeyBioinformatics2021
  collection-title: PEARC '21
  container-title: Practice and Experience in Advanced Research Computing
  DOI: 10.1145/3437359.3465562
  event-place: New York, NY, USA
  ISBN: 978-1-4503-8292-2
  issued:
    - year: 2021
      month: 7
      day: 17
  note: 'interest: 91'
  page: 1–5
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Accelerating Key Bioinformatics Tasks 100-fold by Improving Memory Access
  type: paper-conference
  URL: https://doi.org/10.1145/3437359.3465562

- id: shafferLightweightFunctionMonitors2021
  abstract: >-
    Python has become a widely used programming language for research, not only
    for small one-off analyses, but also for complex application pipelines
    running at supercomputer-scale. Modern parallel programming frameworks for
    Python present users with a more granular unit of management than
    traditional Unix processes and batch submissions: the Python function. We
    review the challenges involved in running native Python functions at scale,
    and present techniques for dynamically determining a minimal set of
    dependencies and for assembling a lightweight function monitor (LFM) that
    captures the software environment and manages resources at the granularity
    of single functions. We evaluate these techniques in a range of
    environments, from campus cluster to supercomputer, and show that our
    advanced dependency management planning and dynamic resource management
    methods provide superior performance and utilization relative to
    coarser-grained management approaches, achieving several-fold decrease in
    execution time for several large Python applications.
  author:
    - family: Shaffer
      given: Tim
    - family: Li
      given: Zhuozhao
    - family: Tovar
      given: Ben
    - family: Babuji
      given: Yadu
    - family: Dasso
      given: TJ
    - family: Surma
      given: Zoe
    - family: Chard
      given: Kyle
    - family: Foster
      given: Ian
    - family: Thain
      given: Douglas
  citation-key: shafferLightweightFunctionMonitors2021
  container-title: >-
    2021 IEEE International Parallel and Distributed Processing Symposium
    (IPDPS)
  DOI: 10.1109/IPDPS49936.2021.00088
  event-title: >-
    2021 IEEE International Parallel and Distributed Processing Symposium
    (IPDPS)
  ISSN: 1530-2075
  issued:
    - year: 2021
      month: 5
  note: 'interest: 89'
  page: 786-796
  source: IEEE Xplore
  title: >-
    Lightweight Function Monitors for Fine-Grained Management in Large Scale
    Python Applications
  type: paper-conference

- id: shamirPracticesSourceCode2013
  abstract: >-
    While software and algorithms have become increasingly important in
    astronomy, the majority of authors who publish computational astronomy
    research do not share the source code they develop, making it difficult to
    replicate and reuse the work. In this paper we discuss the importance of
    sharing scientific source code with the entire astrophysics community, and
    propose that journals require authors to make their code publicly available
    when a paper is published. That is, we suggest that a paper that involves a
    computer program not be accepted for publication unless the source code
    becomes publicly available. The adoption of such a policy by editors,
    editorial boards, and reviewers will improve the ability to replicate
    scientific results, and will also make computational astronomy methods more
    available to other researchers who wish to apply them to their data.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Shamir
      given: Lior
    - family: Wallin
      given: John F.
    - family: Allen
      given: Alice
    - family: Berriman
      given: Bruce
    - family: Teuben
      given: Peter
    - family: Nemiroff
      given: Robert J.
    - family: Mink
      given: Jessica
    - family: Hanisch
      given: Robert J.
    - family: DuPrie
      given: Kimberly
  citation-key: shamirPracticesSourceCode2013
  container-title: Astronomy and Computing
  container-title-short: Astronomy and Computing
  DOI: 10.1016/j.ascom.2013.04.001
  ISSN: 2213-1337
  issued:
    - year: 2013
      month: 2
      day: 1
  language: en
  page: 54-58
  source: ScienceDirect
  title: Practices in source code sharing in astrophysics
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S2213133713000073
  volume: '1'

- id: shamseerPotentialPredatoryLegitimate2017
  abstract: >-
    Background

    The Internet has transformed scholarly publishing, most notably, by the
    introduction of open access publishing. Recently, there has been a rise of
    online journals characterized as ‘predatory’, which actively solicit
    manuscripts and charge publications fees without providing robust peer
    review and editorial services. We carried out a cross-sectional comparison
    of characteristics of potential predatory, legitimate open access, and
    legitimate subscription-based biomedical journals.


    Methods

    On July 10, 2014, scholarly journals from each of the following groups were
    identified – potential predatory journals (source: Beall’s List), presumed
    legitimate, fully open access journals (source: PubMed Central), and
    presumed legitimate subscription-based (including hybrid) journals (source:
    Abridged Index Medicus). MEDLINE journal inclusion criteria were used to
    screen and identify biomedical journals from within the potential predatory
    journals group. One hundred journals from each group were randomly selected.
    Journal characteristics (e.g., website integrity, look and feel, editors and
    staff, editorial/peer review process, instructions to authors, publication
    model, copyright and licensing, journal location, and contact) were
    collected by one assessor and verified by a second. Summary statistics were
    calculated.


    Results

    Ninety-three predatory journals, 99 open access, and 100 subscription-based
    journals were analyzed; exclusions were due to website unavailability. Many
    more predatory journals’ homepages contained spelling errors (61/93, 66%)
    and distorted or potentially unauthorized images (59/93, 63%) compared to
    open access journals (6/99, 6% and 5/99, 5%, respectively) and
    subscription-based journals (3/100, 3% and 1/100, 1%, respectively).
    Thirty-one (33%) predatory journals promoted a bogus impact metric – the
    Index Copernicus Value – versus three (3%) open access journals and no
    subscription-based journals. Nearly three quarters (n = 66, 73%) of
    predatory journals had editors or editorial board members whose affiliation
    with the journal was unverified versus two (2%) open access journals and one
    (1%) subscription-based journal in which this was the case. Predatory
    journals charge a considerably smaller publication fee (median $100 USD, IQR
    $63–$150) than open access journals ($1865 USD, IQR $800–$2205) and
    subscription-based hybrid journals ($3000 USD, IQR $2500–$3000).


    Conclusions

    We identified 13 evidence-based characteristics by which predatory journals
    may potentially be distinguished from presumed legitimate journals. These
    may be useful for authors who are assessing journals for possible submission
    or for others, such as universities evaluating candidates’ publications as
    part of the hiring process.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Shamseer
      given: Larissa
    - family: Moher
      given: David
    - family: Maduekwe
      given: Onyi
    - family: Turner
      given: Lucy
    - family: Barbour
      given: Virginia
    - family: Burch
      given: Rebecca
    - family: Clark
      given: Jocalyn
    - family: Galipeau
      given: James
    - family: Roberts
      given: Jason
    - family: Shea
      given: Beverley J.
  citation-key: shamseerPotentialPredatoryLegitimate2017
  container-title: BMC Medicine
  container-title-short: BMC Med
  DOI: 10.1186/s12916-017-0785-9
  ISSN: 1741-7015
  issued:
    - year: 2017
      month: 3
      day: 16
  page: '28'
  PMCID: PMC5353955
  PMID: '28298236'
  source: PubMed Central
  title: >-
    Potential predatory and legitimate biomedical journals: can you tell the
    difference? A cross-sectional comparison
  title-short: Potential predatory and legitimate biomedical journals
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5353955/
  volume: '15'

- id: shankarDITTOAutomaticIncrementalization2007
  abstract: >-
    We present DITTO, an automatic incrementalizer for dynamic, side-effect-free
    data structure invariant checks. Incrementalization speeds up the execution
    of a check by reusing its previous executions, checking the invariant anew
    only the changed parts of the data structure. DITTO exploits properties
    specific to the domain of invariant checks to automate and simplify the
    process without restricting what mutations the program can perform. Our
    incrementalizer works for modern imperative languages such as Java and C#.
    It can incrementalize,for example, verification of red-black tree properties
    and the consistency of the hash code in a hash table bucket. Our
    source-to-source implementation for Java is automatic, portable, and
    efficient. DITTO provides speedups on data structures with as few as 100
    elements; on larger data structures, its speedups are characteristic of
    non-automatic incrementalizers: roughly 5-fold at 5,000 elements,and growing
    linearly with data structure size.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Shankar
      given: Ajeet
    - family: Bodík
      given: Rastislav
  citation-key: shankarDITTOAutomaticIncrementalization2007
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/1273442.1250770
  ISSN: 0362-1340
  issue: '6'
  issued:
    - year: 2007
      month: 6
      day: 10
  note: 'interest: 61'
  page: 310–319
  source: June 2007
  title: >-
    DITTO: automatic incrementalization of data structure invariant checks (in
    Java)
  title-short: DITTO
  type: article-journal
  URL: https://doi.org/10.1145/1273442.1250770
  volume: '42'

- id: shawGoldenAgeSoftware2006
  abstract: >-
    Since the late 1980s, software architecture has emerged as the principled
    understanding of the large-scale structures of software systems. From its
    roots in qualitative descriptions of empirically observed useful system
    organizations, software architecture has matured to encompass a broad set of
    notations, tools, and analysis techniques. Whereas initially the research
    area interpreted software practice, it now offers concrete guidance for
    complex software design and development. It has made the transition from
    basic research to an essential element of software system design and
    construction. This retrospective examines software architecture's growth in
    the context of a technology maturation model, matching its significant
    accomplishments to the model's stages to gain perspective on where the field
    stands today. This trajectory has taken architecture to its golden age.
  author:
    - family: Shaw
      given: M.
    - family: Clements
      given: P.
  citation-key: shawGoldenAgeSoftware2006
  container-title: IEEE Software
  DOI: 10.1109/MS.2006.58
  ISSN: 1937-4194
  issue: '2'
  issued:
    - year: 2006
      month: 3
  note: 'interest: 85'
  page: 31-39
  source: IEEE Xplore
  title: The golden age of software architecture
  type: article-journal
  volume: '23'

- id: shawHasPythonGIL2019
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Shaw
      given: Anthony
  citation-key: shawHasPythonGIL2019
  container-title: HackerNoon
  issued:
    - year: 2019
      month: 5
      day: 15
  language: en
  title: Has the Python GIL been slain?
  title-short: Has the Python GIL been slain?
  type: post-weblog
  URL: https://hackernoon.com/has-the-python-gil-been-slain-9440d28fa93d

- id: shawWhatMakesGood2002
  abstract: >-
    Physics, biology, and medicine have well-refined public explanations of
    their research processes. Even in simplified form, these provide guidance
    about what counts as “good research” both inside and outside the field.
    Software engineering has not yet explicitly identified and explained either
    our research processes or the ways we recognize excellent work. Science and
    engineering research fields can be characterized in terms of the kinds of
    questions they find worth investigating, the research methods they adopt,
    and the criteria by which they evaluate their results. I will present such a
    characterization for software engineering, showing the diversity of research
    strategies and the way they shift as ideas mature. Understanding these
    strategies should help software engineers design research plans and report
    the results clearly; it should also help explain the character of software
    engineering research to computer science at large and to other scientists.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Shaw
      given: Mary
  citation-key: shawWhatMakesGood2002
  container-title: International Journal on Software Tools for Technology Transfer
  container-title-short: STTT
  DOI: 10.1007/s10009-002-0083-4
  ISSN: 1433-2779
  issue: '1'
  issued:
    - year: 2002
      month: 10
      day: 1
  language: en
  note: 'interest: 90'
  page: 1-7
  source: Springer Link
  title: What makes good research in software engineering?
  type: article-journal
  URL: https://doi.org/10.1007/s10009-002-0083-4
  volume: '4'

- id: shawWritingGoodSoftware2003
  abstract: >-
    Software engineering researchers solve problems of several different kinds.
    To do so, they produce several different kinds of results, and they should
    develop appropriate evidence to validate these results. They often report
    their research in conference papers. I analyzed the abstracts of research
    papers submitted to ICSE 2002 in order to identify the types of research
    reported in the submitted and accepted papers, and I observed the program
    committee discussions about which papers to accept. This report presents the
    research paradigms of the papers, common concerns of the program committee,
    and statistics on success rates. This information should help researchers
    design better research projects and write papers that present their results
    to best advantage.
  accessed:
    - year: 2022
      month: 5
      day: 12
  author:
    - family: Shaw
      given: Mary
  citation-key: shawWritingGoodSoftware2003
  collection-title: ICSE '03
  container-title: Proceedings of the 25th International Conference on Software Engineering
  event-place: USA
  ISBN: 978-0-7695-1877-0
  issued:
    - year: 2003
      month: 5
      day: 3
  page: 726–736
  publisher: IEEE Computer Society
  publisher-place: USA
  source: ACM Digital Library
  title: 'Writing good software engineering research papers: minitutorial'
  title-short: Writing good software engineering research papers
  type: paper-conference

- id: shewchukThreeSinsAuthors
  accessed:
    - year: 2023
      month: 1
      day: 20
  author:
    - family: Shewchuk
      given: Jonathan
  citation-key: shewchukThreeSinsAuthors
  title: Three Sins of Authors in Computer Science and Math
  type: webpage
  URL: http://www.cs.cmu.edu/~jrs/sins.html

- id: shiExperienceReportProducing2022
  abstract: >-
    Build verifiability is a safety property for a software system which can be
    used to check against various security-related issues during the build
    process. In summary, a verifiable build generates equivalent build artifacts
    for every build instance, allowing independent auditors to verify that the
    generated artifacts correspond to their source code. Producing a verifiable
    build is a very challenging problem, as non-equivalences in the build
    artifacts can be caused by non-determinsm from the build environment, the
    build toolchain, or the system implementation. Existing research and
    practices on build verifiability mainly focus on remediating sources of
    non-determinism. However, such a process does not work well with large-scale
    commercial systems (LSCSs) due to their stringent security requirements,
    complex third party dependencies, and large volumes of code changes. In this
    paper, we present an experience report on using a unified process and a
    toolkit to produce verifiable builds for LSCSs. A unified process contrasts
    with the existing practices in which recommendations to mitigate sources of
    non-determinism are proposed on a case-by-case basis and are not codified in
    a comprehensive tool. Our approach supports the following three strategies
    to systematically mitigate non-equivalences in the build artifacts:
    remediation, controlling, and interpretation. Case study on three LSCSs
    within \sf HuaweiHuawei shows that our approach is able to increase the
    proportion of verified build artifacts from less than 50 to 100 percent. To
    cross-validate our approach, we successfully applied our approach to build
    2,218 open source packages distributed under \sf CentOSCentOS 7.8,
    increasing the proportion of verified build artifacts from 85 to 99 percent
    with minimal human intervention. We also provide an overview of our
    mitigation guideline, which describes the recommended strategies to mitigate
    various non-equivalences. Finally, we present some discussions and open
    research problems in this area based on our experience and lessons learned
    in the past few years of applying our approach within the company. This
    paper will be useful for practitioners and software engineering researchers
    who are interested in build verifiability.
  accessed:
    - year: 2023
      month: 12
      day: 18
  author:
    - family: Shi
      given: Yong
    - family: Wen
      given: Mingzhi
    - family: Cogo
      given: Filipe R.
    - family: Chen
      given: Boyuan
    - family: Jiang
      given: Zhen Ming
  citation-key: shiExperienceReportProducing2022
  container-title: IEEE Transactions on Software Engineering
  DOI: 10.1109/TSE.2021.3092692
  ISSN: 1939-3520
  issue: '9'
  issued:
    - year: 2022
      month: 9
  page: 3361-3377
  source: IEEE Xplore
  title: >-
    An Experience Report on Producing Verifiable Builds for Large-Scale
    Commercial Systems
  type: article-journal
  URL: >-
    https://ieeexplore.ieee.org/abstract/document/9465650?casa_token=thRn5ctU4g0AAAAA:RFJ6sU_u8vmnEeKIDwZws5s4n75cs9oWEbC6bLWw_zNr-NZKLGUtGIolRbCnA3GograW75NKOQ
  volume: '48'

- id: shiReflectionawareStaticRegression2019
  abstract: >-
    Regression test selection (RTS) aims to speed up regression testing by
    rerunning only tests that are affected by code changes. RTS can be performed
    using static or dynamic analysis techniques. Our prior study showed that
    static and dynamic RTS perform similarly for medium-sized Java projects.
    However, the results of that prior study also showed that static RTS can be
    unsafe, missing to select tests that dynamic RTS selects, and that
    reflection was the only cause of unsafety observed among the evaluated
    projects. In this paper, we investigate five techniques—three purely static
    techniques and two hybrid static-dynamic techniques—that aim to make static
    RTS safe with respect to reflection. We implement these reflection-aware
    (RA) techniques by extending the reflection-unaware (RU) class-level static
    RTS technique in a tool called STARTS. To evaluate these RA techniques, we
    compare their end-to-end times with RU, and with RetestAll, which reruns all
    tests after every code change. We also compare safety and precision of the
    RA techniques with Ekstazi, a state-of-the-art dynamic RTS technique;
    precision is a measure of unaffected tests selected. Our evaluation on 1173
    versions of 24 open-source Java projects shows negative results. The RA
    techniques improve the safety of RU but at very high costs. The purely
    static techniques are safe in our experiments but decrease the precision of
    RU, with end-to-end time at best 85.8% of RetestAll time, versus 69.1% for
    RU. One hybrid static-dynamic technique improves the safety of RU but at
    high cost, with end-to-end time that is 91.2% of RetestAll. The other hybrid
    static-dynamic technique provides better precision, is safer than RU, and
    incurs lower end-to-end time—75.8% of RetestAll, but it can still be unsafe
    in the presence of test-order dependencies. Our study highlights the
    challenges involved in making static RTS safe with respect to reflection.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Shi
      given: August
    - family: Hadzi-Tanovic
      given: Milica
    - family: Zhang
      given: Lingming
    - family: Marinov
      given: Darko
    - family: Legunsen
      given: Owolabi
  citation-key: shiReflectionawareStaticRegression2019
  container-title: Proceedings of the ACM on Programming Languages
  container-title-short: Proc. ACM Program. Lang.
  DOI: 10.1145/3360613
  ISSN: 2475-1421
  issue: OOPSLA
  issued:
    - year: 2019
      month: 10
      day: 10
  language: en
  note: 'interest: 93'
  page: 1-29
  source: DOI.org (Crossref)
  title: Reflection-aware static regression test selection
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/3360613
  volume: '3'

- id: shirtsLessonsLearnedComparing2017
  abstract: "We describe our efforts to prepare common starting structures and models for the SAMPL5 blind prediction challenge. We generated the starting input files and single configuration potential energies for the host-guest in the SAMPL5 blind prediction challenge for the GROMACS, AMBER, LAMMPS, DESMOND and CHARMM molecular simulation programs. All conversions were fully automated from the originally prepared AMBER input files using a combination of the ParmEd and InterMol conversion programs. We find that the energy calculations for all molecular dynamics engines for this molecular set agree to better than 0.1\_% relative absolute energy for all energy components, and in most cases an order of magnitude better, when reasonable choices are made for different cutoff parameters. However, there are some surprising sources of statistically significant differences. Most importantly, different choices of Coulomb’s constant between programs are one of the largest sources of discrepancies in energies. We discuss the measures required to get good agreement in the energies for equivalent starting configurations between the simulation programs, and the energy differences that occur when simulations are run with program-specific default simulation parameter values. Finally, we discuss what was required to automate this conversion and comparison."
  accessed:
    - year: 2023
      month: 2
      day: 23
  author:
    - family: Shirts
      given: Michael R.
    - family: Klein
      given: Christoph
    - family: Swails
      given: Jason M.
    - family: Yin
      given: Jian
    - family: Gilson
      given: Michael K.
    - family: Mobley
      given: David L.
    - family: Case
      given: David A.
    - family: Zhong
      given: Ellen D.
  citation-key: shirtsLessonsLearnedComparing2017
  container-title: Journal of Computer-Aided Molecular Design
  container-title-short: J Comput Aided Mol Des
  DOI: 10.1007/s10822-016-9977-1
  ISSN: 1573-4951
  issue: '1'
  issued:
    - year: 2017
      month: 1
      day: 1
  language: en
  page: 147-161
  source: Springer Link
  title: >-
    Lessons learned from comparing molecular dynamics engines on the SAMPL5
    dataset
  type: article-journal
  URL: https://doi.org/10.1007/s10822-016-9977-1
  volume: '31'

- id: shottonCiTOCitationTyping2010
  abstract: >-
    CiTO, the Citation Typing Ontology, is an ontology for describing the nature
    of reference citations in scientific research articles and other scholarly
    works, both to other such publications and also to Web information
    resources, and for publishing these descriptions on the Semantic Web.
    Citation are described in terms of the factual and rhetorical relationships
    between citing publication and cited publication, the in-text and global
    citation frequencies of each cited work, and the nature of the cited work
    itself, including its publication and peer review status. This paper
    describes CiTO and illustrates its usefulness both for the annotation of
    bibliographic reference lists and for the visualization of citation
    networks. The latest version of CiTO, which this paper describes, is CiTO
    Version 1.6, published on 19 March 2010. CiTO is written in the Web Ontology
    Language OWL, uses the namespace http://purl.org/net/cito/, and is available
    from http://purl.org/net/cito/. This site uses content negotiation to
    deliver to the user an OWLDoc Web version of the ontology if accessed via a
    Web browser, or the OWL ontology itself if accessed from an ontology
    management tool such as Protégé 4 (http://protege.stanford.edu/).
    Collaborative work is currently under way to harmonize CiTO with other
    ontologies describing bibliographies and the rhetorical structure of
    scientific discourse.
  accessed:
    - year: 2023
      month: 5
      day: 25
  author:
    - family: Shotton
      given: David
  citation-key: shottonCiTOCitationTyping2010
  container-title: Journal of Biomedical Semantics
  container-title-short: J Biomed Semant
  DOI: 10.1186/2041-1480-1-S1-S6
  ISSN: 2041-1480
  issue: '1'
  issued:
    - year: 2010
      month: 6
      day: 22
  language: en
  page: S6
  source: Springer Link
  title: CiTO, the Citation Typing Ontology
  type: article-journal
  URL: https://doi.org/10.1186/2041-1480-1-S1-S6
  volume: '1'

- id: shresthaSLNETRedistributableCorpus2022
  abstract: >-
    MATLAB/Simulink is widely used for model-based design. Engineers create
    Simulink models and compile them to embedded code, often to control
    safety-critical cyber-physical systems in automotive, aerospace, and
    healthcare applications. Despite Simulink's importance, there are few
    large-scale empirical Simulink studies, perhaps because there is no large
    readily available corpus of third-party open-source Simulink models. To
    enable empirical Simulink studies, this paper introduces SLNET, the largest
    corpus of freely available third-party Simulink models. SLNET has several
    advantages over earlier collections. Specifically, SLNET is 8 times larger
    than the largest previous corpus of Simulink models, includes fine-grained
    metadata, is constructed automatically, is self-contained, and allows
    redistribution. SLNET is available under permissive open-source licenses and
    contains all of its collection and analysis tools.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Shrestha
      given: Sohil Lal
    - family: Chowdhury
      given: Shafiul Azam
    - family: Csallner
      given: Christoph
  citation-key: shresthaSLNETRedistributableCorpus2022
  container-title: arXiv:2203.17112 [cs]
  DOI: 10.1145/3524842.3528001
  issued:
    - year: 2022
      month: 3
      day: 31
  source: arXiv.org
  title: 'SLNET: A Redistributable Corpus of 3rd-party Simulink Models'
  title-short: SLNET
  type: article-journal
  URL: http://arxiv.org/abs/2203.17112

- id: siciliaCommunityCurationOpen2017
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Sicilia
      given: Miguel-Angel
    - family: García-Barriocanal
      given: Elena
    - family: Sánchez-Alonso
      given: Salvador
  citation-key: siciliaCommunityCurationOpen2017
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2017.03.009
  ISSN: '18770509'
  issued:
    - year: 2017
  language: en
  page: 54-60
  source: DOI.org (Crossref)
  title: 'Community Curation in Open Dataset Repositories: Insights from Zenodo'
  title-short: Community Curation in Open Dataset Repositories
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1877050917302776
  volume: '106'

- id: siegmundViewsInternalExternal2015
  abstract: >-
    Empirical methods have grown common in software engineering, but there is no
    consensus on how to apply them properly. Is practical relevance key? Do
    internally valid studies have any value? Should we replicate more to address
    the tradeoff between internal and external validity? We asked the community
    how empirical research should take place in software engineering, with a
    focus on the tradeoff between internal and external validity and
    replication, complemented with a literature review about the status of
    empirical research in software engineering. We found that the opinions
    differ considerably, and that there is no consensus in the community when to
    focus on internal or external validity and how to conduct and review
    replications.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Siegmund
      given: Janet
    - family: Siegmund
      given: Norbert
    - family: Apel
      given: Sven
  citation-key: siegmundViewsInternalExternal2015
  container-title: 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
  DOI: 10.1109/ICSE.2015.24
  event-place: Florence, Italy
  event-title: >-
    2015 IEEE/ACM 37th IEEE International Conference on Software Engineering
    (ICSE)
  ISBN: 978-1-4799-1934-5
  issued:
    - year: 2015
      month: 5
  note: 'interest: 78'
  page: 9-19
  publisher: IEEE
  publisher-place: Florence, Italy
  source: DOI.org (Crossref)
  title: Views on Internal and External Validity in Empirical Software Engineering
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/7194557/

- id: sierraTroubleKoolaidPoint2014
  accessed:
    - year: 2023
      month: 9
      day: 27
  author:
    - family: Sierra
      given: Kathy
  citation-key: sierraTroubleKoolaidPoint2014
  container-title: Serious Pony
  issued:
    - year: 2014
      month: 10
      day: 7
  title: Trouble at the Koolaid Point
  type: post-weblog
  URL: >-
    https://web.archive.org/web/20211031133101/http://seriouspony.com/trouble-at-the-koolaid-point

- id: sikosProvenanceAwareKnowledgeRepresentation2020
  abstract: |-
    Abstract 
                Expressing machine-interpretable statements in the form of subject-predicate-object triples is a well-established practice for capturing semantics of structured data. However, the standard used for representing these triples, RDF, inherently lacks the mechanism to attach provenance data, which would be crucial to make automatically generated and/or processed data authoritative. This paper is a critical review of data models, annotation frameworks, knowledge organization systems, serialization syntaxes, and algebras that enable provenance-aware RDF statements. The various approaches are assessed in terms of standard compliance, formal semantics, tuple type, vocabulary term usage, blank nodes, provenance granularity, and scalability. This can be used to advance existing solutions and help implementers to select the most suitable approach (or a combination of approaches) for their applications. Moreover, the analysis of the mechanisms and their limitations highlighted in this paper can serve as the basis for novel approaches in RDF-powered applications with increasing provenance needs.
  accessed:
    - year: 2022
      month: 8
      day: 2
  author:
    - family: Sikos
      given: Leslie F.
    - family: Philp
      given: Dean
  citation-key: sikosProvenanceAwareKnowledgeRepresentation2020
  container-title: Data Science and Engineering
  container-title-short: Data Sci. Eng.
  DOI: 10.1007/s41019-020-00118-0
  ISSN: 2364-1185, 2364-1541
  issue: '3'
  issued:
    - year: 2020
      month: 9
  language: en
  page: 293-316
  source: DOI.org (Crossref)
  title: >-
    Provenance-Aware Knowledge Representation: A Survey of Data Models and
    Contextualized Knowledge Graphs
  title-short: Provenance-Aware Knowledge Representation
  type: article-journal
  URL: https://link.springer.com/10.1007/s41019-020-00118-0
  volume: '5'

- id: simeonEssenceXML2003
  abstract: >-
    The World-Wide Web Consortium (W3C) promotes XML and related standards,
    including XML Schema, XQuery, and XPath. This paper describes a
    formalization of XML Schema. A formal semantics based on these ideas is part
    of the official XQuery and XPath specification, one of the first uses of
    formal methods by a standards body. XML Schema features both named and
    structural types, with structure based on tree grammars. While structural
    types and matching have been studied in other work (notably XDuce, Relax NG,
    and a previous formalization of XML Schema), this is the first work to study
    the relation between named types and structural types, and the relation
    between matching and validation.
  accessed:
    - year: 2023
      month: 9
      day: 11
  author:
    - family: Siméon
      given: Jérôme
    - family: Wadler
      given: Philip
  citation-key: simeonEssenceXML2003
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/640128.604132
  ISSN: 0362-1340
  issue: '1'
  issued:
    - year: 2003
      month: 1
      day: 15
  page: 1–13
  source: ACM Digital Library
  title: The essence of XML
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/640128.604132
  volume: '38'

- id: simmhanSurveyDataProvenance2005
  abstract: >-
    Data management is growing in complexity as large-scale applications take
    advantage of the loosely coupled resources brought together by grid
    middleware and by abundant storage capacity. Metadata describing the data
    products used in and generated by these applications is essential to
    disambiguate the data and enable reuse. Data provenance, one kind of
    metadata, pertains to the derivation history of a data product starting from
    its original sources.In this paper we create a taxonomy of data provenance
    characteristics and apply it to current research efforts in e-science,
    focusing primarily on scientific workflow approaches. The main aspect of our
    taxonomy categorizes provenance systems based on why they record provenance,
    what they describe, how they represent and store provenance, and ways to
    disseminate it. The survey culminates with an identification of open
    research problems in the field.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Simmhan
      given: Yogesh L.
    - family: Plale
      given: Beth
    - family: Gannon
      given: Dennis
  citation-key: simmhanSurveyDataProvenance2005
  container-title: ACM SIGMOD Record
  container-title-short: SIGMOD Rec.
  DOI: 10.1145/1084805.1084812
  ISSN: 0163-5808
  issue: '3'
  issued:
    - year: 2005
      month: 9
  language: en
  page: 31-36
  source: DOI.org (Crossref)
  title: A survey of data provenance in e-science
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/1084805.1084812
  volume: '34'

- id: simmonsFalsePositivePsychologyUndisclosed2011
  abstract: >-
    In this article, we accomplish two things. First, we show that despite
    empirical psychologists’ nominal endorsement of a low rate of false-positive
    findings (≤ .05), flexibility in data collection, analysis, and reporting
    dramatically increases actual false-positive rates. In many cases, a
    researcher is more likely to falsely find evidence that an effect exists
    than to correctly find evidence that it does not. We present computer
    simulations and a pair of actual experiments that demonstrate how
    unacceptably easy it is to accumulate (and report) statistically significant
    evidence for a false hypothesis. Second, we suggest a simple, low-cost, and
    straightforwardly effective disclosure-based solution to this problem. The
    solution involves six concrete requirements for authors and four guidelines
    for reviewers, all of which impose a minimal burden on the publication
    process.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Simmons
      given: Joseph P.
    - family: Nelson
      given: Leif D.
    - family: Simonsohn
      given: Uri
  citation-key: simmonsFalsePositivePsychologyUndisclosed2011
  container-title: Psychological Science
  container-title-short: Psychol Sci
  DOI: 10.1177/0956797611417632
  ISSN: 0956-7976
  issue: '11'
  issued:
    - year: 2011
      month: 11
      day: 1
  language: en
  page: 1359-1366
  publisher: SAGE Publications Inc
  source: SAGE Journals
  title: >-
    False-Positive Psychology: Undisclosed Flexibility in Data Collection and
    Analysis Allows Presenting Anything as Significant
  title-short: False-Positive Psychology
  type: article-journal
  URL: https://doi.org/10.1177/0956797611417632
  volume: '22'

- id: simsResearchSoftwareEngineering2021
  abstract: >-
    This report summarizes the results of a study of research software
    professionals based on 17 interviews conducted at U.S. national laboratories
    and academic institutions. The study focuses on understanding the emerging
    professional role of research software engineer (RSE) and examining how the
    history of software engineering might be relevant to the emergence of
    research software engineering as a professional movement. The report first
    uses interview data to identify several models of professional identity for
    research software professionals. These include dominance models in which a
    person strongly identifies with a single professional identity (such as
    RSE), and multiplicity models in which an individual sees an identity like
    RSE as just one facet of their overall professional role. Next, it
    identifies three types of work situations for software professionals at
    national laboratories, covering those who work in dedicated RSE
    organizations, those who play an integral role in large multiphysics code
    projects, and those who work in a more isolated capacity to provide software
    development support for less computationally intensive research projects. It
    then reviews some key ideas from the sociology of professions in the context
    of the software engineering profession, suggesting parallels between the
    current RSE professional movement and aspects of the history of software
    engineering. It concludes by suggesting some potential practical
    implications of these findings for the RSE movement and professional
    organizations.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Sims
      given: Benjamin
  citation-key: simsResearchSoftwareEngineering2021
  DOI: 10.2172/1845242
  issued:
    - year: 2021
      month: 2
      day: 8
  language: en
  note: 'interst: 50'
  number: LA-UR-22-21250, 1845242
  page: LA-UR-22-21250, 1845242
  publisher: Los Alamos National Laboratory
  source: DOI.org (Crossref)
  title: 'Research software engineering: Professionalization, roles, and identity'
  title-short: Research software engineering
  type: report
  URL: https://www.osti.gov/servlets/purl/1845242/

- id: singhFigShare2011
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Singh
      given: Jatinder
  citation-key: singhFigShare2011
  container-title: Journal of Pharmacology and Pharmacotherapeutics
  container-title-short: Journal of Pharmacology and Pharmacotherapeutics
  DOI: 10.4103/0976-500X.81919
  ISSN: 0976-500X, 0976-5018
  issue: '2'
  issued:
    - year: 2011
      month: 6
  language: en
  page: 138-139
  source: DOI.org (Crossref)
  title: FigShare
  type: article-journal
  URL: http://journals.sagepub.com/doi/10.4103/0976-500X.81919
  volume: '2'

- id: singularitydevelopersSecuritySingularityCESingularityCE2023
  accessed:
    - year: 2023
      month: 2
      day: 18
  author:
    - family: Singularity Developers
      given: ''
  citation-key: singularitydevelopersSecuritySingularityCESingularityCE2023
  issued:
    - year: 2023
  title: Security in SingularityCE — SingularityCE Admin Guide 3.11 documentation
  type: webpage
  URL: https://docs.sylabs.io/guides/latest/admin-guide/security.html

- id: sitakerMemoryModelsThat
  abstract: >-
    There are about six major conceptualizations of memory, which I’m calling
    “memory models”, that dominate today’s programming. Three of them derive
    from the three most historically important programming languages of the
    1950s — COBOL, LISP, and FORTRAN — and the other three derive from the three
    historically important data storage systems: magnetic tape, Unix-style
    hierarchical filesystems, and relational databases.


    These models shape what our programming languages can or cannot do at a much
    deeper layer than mere syntax or even type systems. Mysteriously, I’ve never
    seen a good explanation of them — you pretty much just have to absorb them
    by osmosis instead of having them explained to you — and so I’m going to try
    now. Then I’m going to explain some possible alternatives to the mainstream
    options and why they might be interesting.


    - Nested records, the COBOL memory model: memory is a tax form

    - Object graphs, the LISP memory model: memory is a labeled directed graph

    - Parallel arrays, the FORTRAN memory model: memory is a bunch of arrays

    - Pipes, the magnetic tape memory model: the moving finger writes, and,
    having writen, moves on

    - Directories, the Multics memory model: memory is a string-labeled tree
    with blob leaves

    - Relations, the SQL memory model: memory is a collection of mutable
    multivalued finite functions

    - Also: why is there no Lua, Erlang, or Forth memory model?
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Sitaker
      given: Kragen Javier
  citation-key: sitakerMemoryModelsThat
  title: The memory models that underlie programming languages
  type: post-weblog
  URL: http://canonical.org/~kragen/memory-models/

- id: slaughterRegentHighproductivityProgramming2015
  abstract: >-
    We present Regent, a high-productivity programming language for high
    performance computing with logical regions. Regent users compose programs
    with tasks (functions eligible for parallel execution) and logical regions
    (hierarchical collections of structured objects). Regent programs appear to
    execute sequentially, require no explicit synchronization, and are trivially
    deadlock-free. Regent's type system catches many common classes of mistakes
    and guarantees that a program with correct serial execution produces
    identical results on parallel and distributed machines. We present an
    optimizing compiler for Regent that translates Regent programs into
    efficient implementations for Legion, an asynchronous task-based model.
    Regent employs several novel compiler optimizations to minimize the dynamic
    overhead of the runtime system and enable efficient operation. We evaluate
    Regent on three benchmark applications and demonstrate that Regent achieves
    performance comparable to hand-tuned Legion.
  accessed:
    - year: 2023
      month: 7
      day: 14
  author:
    - family: Slaughter
      given: Elliott
    - family: Lee
      given: Wonchan
    - family: Treichler
      given: Sean
    - family: Bauer
      given: Michael
    - family: Aiken
      given: Alex
  citation-key: slaughterRegentHighproductivityProgramming2015
  collection-title: SC '15
  container-title: >-
    Proceedings of the International Conference for High Performance Computing,
    Networking, Storage and Analysis
  DOI: 10.1145/2807591.2807629
  event-place: New York, NY, USA
  ISBN: 978-1-4503-3723-6
  issued:
    - year: 2015
      month: 11
      day: 15
  page: 1–12
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Regent: a high-productivity programming language for HPC with logical
    regions
  title-short: Regent
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2807591.2807629

- id: slusherLifeTimesGus
  abstract: >-
    City Council Member Gus Garcia retires after more than 30 years of public
    service
  accessed:
    - year: 2022
      month: 10
      day: 12
  author:
    - family: Slusher
      given: Daryl
    - literal: FRI.
    - family: June 2
      given: ''
    - literal: '2000'
  citation-key: slusherLifeTimesGus
  language: en-US
  title: The Life and Times of Gus Garcia
  type: webpage
  URL: >-
    https://www.austinchronicle.com/news/2000-06-02/the-life-and-times-of-gus-garcia/

- id: smalleyLinuxSecurityModules
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Smalley
      given: Stephen
    - family: Fraser
      given: Timothy
    - family: Vance
      given: Chris
  citation-key: smalleyLinuxSecurityModules
  container-title: The Linux Kernel documentation
  title: 'Linux Security Modules: General Security Hooks for Linux'
  type: webpage
  URL: https://docs.kernel.org/security/lsm.html

- id: smithYt_astro_analysisVersion2022
  abstract: >-
    yt_astro_analysis is a yt extension package for astrophysical analysis. This
    package contains functionality for: Halo finding and analysis Lightcones
    Planning cosmological simulations for making lightcones and lightrays
    Exporting to the RADMC-3D radiation transport code Creating PPV FITS cubes
    This is a bugfix release; no new features have been added. Bugfixes Make
    sure to initialize index before checking particle types
    https://github.com/yt-project/yt_astro_analysis/pull/127 Fix broken example
    with halo plotting https://github.com/yt-project/yt_astro_analysis/pull/132
    Make total particles a 64 bit integer
    https://github.com/yt-project/yt_astro_analysis/pull/133 Set output
    directory properly for rockstar halo finder
    https://github.com/yt-project/yt_astro_analysis/pull/134 Full Changelog:
    https://github.com/yt-project/yt_astro_analysis/compare/yt_astro_analysis-1.1.0...yt_astro_analysis-1.1.1
  accessed:
    - year: 2022
      month: 5
      day: 3
  author:
    - family: Smith
      given: Britton
    - family: Turk
      given: Matthew
    - family: ZuHone
      given: John
    - family: Goldbaum
      given: Nathan
    - family: Hummels
      given: Cameron
    - family: Egan
      given: Hilary
    - family: Wise
      given: John
    - family: Scopatz
      given: Anthony
    - family: Val-Borro
      given: Miguel
      dropping-particle: de
    - family: Keller
      given: Ben
    - family: Richardson
      given: Mark
    - family: Robert
      given: Clément
  citation-key: smithYt_astro_analysisVersion2022
  DOI: 10.5281/zenodo.5911048
  issued:
    - year: 2022
      month: 1
      day: 27
  publisher: Zenodo
  source: Zenodo
  title: yt_astro_analysis version 1.1.1
  title-short: yt-project/yt_astro_analysis
  type: software
  URL: https://zenodo.org/record/5911048

- id: sochatResearchSoftwareEncyclopedia2022
  abstract: >-
    The Research Software Encyclopedia is a community driven, open source
    strategy to define the term “research software” in different contexts. It
    consists of several elements: a base library to manage a database of
    software, criteria and taxonomy items that can be used to answer questions
    about the software in the database, and several ways for an interested party
    to interact. A community database is stored in version control (GitHub), and
    by way of providing and updating this database, the Research Software
    Encyclopedia takes a strategy of small contributions over time to grow a
    valuable resource. Using a community-driven open source approach offers a
    number of advantages over attempting to derive a single, holistic definition
    for research software. First, it takes into account the context under which
    the definition is considered. Second, community and scoped contributions to
    specific components of the task are easy. Third, it provides a resource that
    can be extended to other use cases. Finally, this initiative creates a
    solution that requires no grants or other funding to maintain, increasing
    its ability to grow, adapt, and evolve over time.
  accessed:
    - year: 2022
      month: 4
      day: 12
  archive_location: research software engineers
  author:
    - family: Sochat
      given: Vanessa
    - family: May
      given: Nicholas
    - family: Cosden
      given: Ian
    - family: Martinez-Ortiz
      given: Carlos
    - family: Bartholomew
      given: Sadie
  citation-key: sochatResearchSoftwareEncyclopedia2022
  container-title: Journal of Open Research Software
  DOI: 10.5334/jors.359
  ISSN: 2049-9647
  issue: '1'
  issued:
    - year: 2022
      month: 3
      day: 4
  language: en
  license: >-
    Authors who publish with this journal agree to the following terms:   
    Authors retain copyright and grant the journal right of first publication
    with the work simultaneously licensed under a  Creative Commons Attribution
    License  that allows others to share the work with an acknowledgement of the
    work's authorship and initial publication in this journal.  Authors are able
    to enter into separate, additional contractual arrangements for the
    non-exclusive distribution of the journal's published version of the work
    (e.g., post it to an institutional repository or publish it in a book), with
    an acknowledgement of its initial publication in this journal.  Authors are
    permitted and encouraged to post their work online (e.g., in institutional
    repositories or on their website) prior to and during the submission
    process, as it can lead to productive exchanges, as well as earlier and
    greater citation of published work (See  The Effect of Open Access ).  All
    third-party images reproduced on this journal are shared under Educational
    Fair Use. For more information on  Educational Fair Use , please see  this
    useful checklist prepared by Columbia University Libraries .   All
    copyright  of third-party content posted here for research purposes belongs
    to its original owners.  Unless otherwise stated all references to
    characters and comic art presented on this journal are ©, ® or ™ of their
    respective owners. No challenge to any owner’s rights is intended or should
    be inferred.
  note: 'interest: 73'
  number: '1'
  page: '2'
  publisher: Ubiquity Press
  source: openresearchsoftware.metajnl.com
  title: >-
    The Research Software Encyclopedia: A Community Framework to Define Research
    Software
  title-short: The Research Software Encyclopedia
  type: article-journal
  URL: http://openresearchsoftware.metajnl.com/articles/10.5334/jors.359/
  volume: '10'

- id: soiland-reyesPackagingResearchArtefacts2022
  abstract: >-
    An increasing number of researchers support reproducibility by including
    pointers to and descriptions of datasets, software and methods in their
    publications. However, scientific articles may be ambiguous, incomplete and
    difficult to process by autom
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Soiland-Reyes
      given: Stian
    - family: Sefton
      given: Peter
    - family: Crosas
      given: Mercè
    - family: Castro
      given: Leyla Jael
    - family: Coppens
      given: Frederik
    - family: Fernández
      given: José M.
    - family: Garijo
      given: Daniel
    - family: Grüning
      given: Björn
    - family: La Rosa
      given: Marco
    - family: Leo
      given: Simone
    - family: Ó Carragáin
      given: Eoghan
    - family: Portier
      given: Marc
    - family: Trisovic
      given: Ana
    - family: RO-Crate Community
      given: ''
    - family: Groth
      given: Paul
    - family: Goble
      given: Carole
  citation-key: soiland-reyesPackagingResearchArtefacts2022
  container-title: Data Science
  DOI: 10.3233/DS-210053
  ISSN: 2451-8484
  issue: '2'
  issued:
    - year: 2022
      month: 1
      day: 1
  language: en
  note: 'interest: 99'
  page: 97-138
  publisher: IOS Press
  source: content.iospress.com
  title: Packaging research artefacts with RO-Crate
  type: article-journal
  URL: https://content.iospress.com/articles/data-science/ds210053
  volume: '5'

- id: soiland-reyesWf4EverResearchObject2013
  abstract: >-
    The Wf4Ever Research Object Model provides a vocabulary for the description
    of workflow-centric Research Objects: aggregations of resources relating to
    scientific workflows.


    <strong>Permalink</strong>: https://w3id.org/ro/2013-11-30/
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Soiland-Reyes
      given: Stian
    - family: Bechhofer
      given: Sean
    - family: Belhajjame
      given: Khalid
    - family: Klyne
      given: Graham
    - family: Garijo
      given: Daniel
    - family: Coricho
      given: Oscar
    - family: García Cuesta
      given: Esteban
    - family: Palma
      given: Raul
  citation-key: soiland-reyesWf4EverResearchObject2013
  DOI: 10.5281/ZENODO.12744
  issued:
    - year: 2013
      month: 11
      day: 30
  license: Creative Commons Attribution 4.0, Open Access
  publisher: Zenodo
  source: DOI.org (Datacite)
  title: Wf4Ever Research Object Model
  type: article-journal
  URL: https://zenodo.org/record/12744

- id: sollaciIntroductionMethodsResults2004
  abstract: >-
    Background: The scientific article in the health sciences evolved from the
    letter form and purely descriptive style in the seventeenth century to a
    very standardized structure in the twentieth century known as introduction,
    methods, results, and discussion (IMRAD). The pace in which this structure
    began to be used and when it became the most used standard of today's
    scientific discourse in the health sciences is not well established.,
    Purpose: The purpose of this study is to point out the period in time during
    which the IMRAD structure was definitively and widely adopted in medical
    scientific writing., Methods: In a cross-sectional study, the frequency of
    articles written under the IMRAD structure was measured from 1935 to 1985 in
    a randomly selected sample of articles published in four leading journals in
    internal medicine: the British Medical Journal, JAMA, The Lancet, and the
    New England Journal of Medicine., Results: The IMRAD structure, in those
    journals, began to be used in the 1940s. In the 1970s, it reached 80% and,
    in the 1980s, was the only pattern adopted in original papers., Conclusions:
    Although recommended since the beginning of the twentieth century, the IMRAD
    structure was adopted as a majority only in the 1970s. The influence of
    other disciplines and the recommendations of editors are among the facts
    that contributed to authors adhering to it.
  accessed:
    - year: 2022
      month: 5
      day: 31
  author:
    - family: Sollaci
      given: Luciana B.
    - family: Pereira
      given: Mauricio G.
  citation-key: sollaciIntroductionMethodsResults2004
  container-title: Journal of the Medical Library Association
  container-title-short: J Med Libr Assoc
  ISSN: 1536-5050
  issue: '3'
  issued:
    - year: 2004
      month: 7
  page: 364-371
  PMCID: PMC442179
  PMID: '15243643'
  source: PubMed Central
  title: >-
    The introduction, methods, results, and discussion (IMRAD) structure: a
    fifty-year survey
  title-short: The introduction, methods, results, and discussion (IMRAD) structure
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC442179/
  volume: '92'

- id: SomeoneBeenMessing
  accessed:
    - year: 2022
      month: 11
      day: 14
  citation-key: SomeoneBeenMessing
  container-title: Someone’s Been Messing With My Subnormals!
  note: 'interest: 90'
  title: Someone’s Been Messing With My Subnormals!
  type: post-weblog
  URL: >-
    https://moyix.blogspot.com/2022/09/someones-been-messing-with-my-subnormals.html

- id: sorosFallibilityReflexivityHuman2013
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Soros
      given: George
  citation-key: sorosFallibilityReflexivityHuman2013
  container-title: Journal of Economic Methodology
  DOI: 10.1080/1350178X.2013.859415
  ISSN: 1350-178X
  issue: '4'
  issued:
    - year: 2013
      month: 12
      day: 1
  page: 309-329
  publisher: Routledge
  source: Taylor and Francis+NEJM
  title: Fallibility, reflexivity, and the human uncertainty principle
  type: article-journal
  URL: https://doi.org/10.1080/1350178X.2013.859415
  volume: '20'

- id: souzaDataReductionScientific2020
  abstract: >-
    Scientific workflows need to be iteratively, and often interactively,
    executed for large input datasets. Reducing data from input datasets is a
    powerful way to reduce overall execution time in such workflows. When this
    is accomplished online (i.e., without requiring the user to stop execution
    to reduce the data, and then resume), it can save much time. However,
    determining which subsets of the input data should be removed becomes a
    major problem. A related problem is to guarantee that the workflow system
    will maintain execution and data consistent with the reduction. Keeping
    track of how users interact with the workflow is essential for data
    provenance purposes. In this paper, we adopt the “human-in-the-loop”
    approach, which enables users to steer the running workflow and reduce
    subsets from datasets online. We propose an adaptive workflow monitoring
    approach that combines provenance data monitoring and computational steering
    to support users in analyzing the evolution of key parameters and
    determining the subset of data to remove. We extend a provenance data model
    to keep track of users’ interactions when they reduce data at runtime. In
    our experimental validation, we develop a test case from the oil and gas
    domain, using a 936-cores cluster. The results on this test case show that
    the approach yields reductions of 32% of execution time and 14% of the data
    processed.
  accessed:
    - year: 2022
      month: 11
      day: 29
  author:
    - family: Souza
      given: Renan
    - family: Silva
      given: Vítor
    - family: Coutinho
      given: Alvaro L. G. A.
    - family: Valduriez
      given: Patrick
    - family: Mattoso
      given: Marta
  citation-key: souzaDataReductionScientific2020
  container-title: Future Generation Computer Systems
  container-title-short: Future Generation Computer Systems
  DOI: 10.1016/j.future.2017.11.028
  ISSN: 0167-739X
  issued:
    - year: 2020
      month: 9
      day: 1
  language: en
  note: 'interest: 99'
  page: 481-501
  source: ScienceDirect
  title: >-
    Data reduction in scientific workflows using provenance monitoring and user
    steering
  type: article-journal
  URL: https://www.sciencedirect.com/science/article/pii/S0167739X16308238
  volume: '110'

- id: souzaOnlineInputData2016
  abstract: >-
    Many scientific workflows are data-intensive and need be iteratively
    executed for large input sets of data elements. Reducing input data is a
    powerful way to reduce overall execution time in such workflows. When this
    is accomplished online (i.e., without requiring users to stop execution to
    reduce the data and resume execution), it can save much time and user
    interactions can integrate within workflow execution. Then, a major problem
    is to determine which subset of the input data should be removed. Other
    related problems include guaranteeing that the workflow system will maintain
    execution and data consistent after reduction, and keeping track of how
    users interacted with execution. In this paper, we adopt the approach "
    human-in-the-loop " for scientific workflows by enabling users to steer the
    workflow execution and reduce input elements from datasets at runtime. We
    propose an adaptive monitoring approach that combines workflow provenance
    monitoring and computational steering to support users in analyzing the
    evolution of key parameters and determining which subset of the data should
    be removed. We also extend a provenance data model to keep track of user
    interactions when users reduce data at runtime. In our experimental
    validation, we develop a test case from the oil and gas industry, using a
    936-cores cluster. The results on our parameter sweep test case show that
    the user interactions for online data reduction yield a 37% reduction of
    execution time.
  accessed:
    - year: 2022
      month: 11
      day: 29
  author:
    - family: Souza
      given: Renan
    - family: Silva
      given: Vítor
    - family: Coutinho
      given: Alvaro L. G. A.
    - family: Valduriez
      given: Patrick
    - family: Mattoso
      given: Marta
  citation-key: souzaOnlineInputData2016
  event-title: 'WORKS: Workflows in Support of Large-scale Science'
  issued:
    - year: 2016
      month: 11
      day: 14
  language: en
  note: 'interest: 99'
  source: hal-lirmm.ccsd.cnrs.fr
  title: Online Input Data Reduction in Scientific Workflows
  type: paper-conference
  URL: https://hal-lirmm.ccsd.cnrs.fr/lirmm-01400538

- id: speerMeetingComputerHalfway
  author:
    - family: Speer
      given: Rob
    - family: Havasi
      given: Catherine
  citation-key: speerMeetingComputerHalfway
  language: en
  page: '3'
  source: Zotero
  title: >-
    Meeting the Computer Halfway: Language Processing in the Artiﬁcial Language
    Lojban
  type: article-journal

- id: speicherWhatUsabilityCharacterization2015
  abstract: >-
    According to Brooke* "Usability does not exist in any absolute sense; it can
    only be defined with reference to particular contexts." That is, one cannot
    speak of usability without specifying what that particular usability is
    characterized by. Driven by the feedback of a reviewer at an international
    conference, I explore in which way one can precisely specify the kind of
    usability they are investigating in a given setting. Finally, I come up with
    a formalism that defines usability as a quintuple comprising the elements
    level of usability metrics, product, users, goals and context of use.
    Providing concrete values for these elements then constitutes the
    investigated type of usability. The use of this formalism is demonstrated in
    two case studies. * J. Brooke. SUS: A "quick and dirty" usability scale. In
    P. W. Jordan, B. Thomas, B. A. Weerdmeester, and A. L. McClelland, editors,
    Usability Evaluation in Industry. Taylor and Francis, 1996.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Speicher
      given: Maximilian
  citation-key: speicherWhatUsabilityCharacterization2015
  container-title: arXiv:1502.06792 [cs]
  issued:
    - year: 2015
      month: 2
      day: 24
  source: arXiv.org
  title: What is Usability? A Characterization based on ISO 9241-11 and ISO/IEC 25010
  title-short: What is Usability?
  type: article-journal
  URL: http://arxiv.org/abs/1502.06792

- id: spornyJSONLDWhyHate2014
  abstract: >-
    The desire for better Web APIs is what motivated the creation of JSON-LD,
    not the Semantic Web. If you want to make the Semantic Web a reality, stop
    making the case for it and spend your time doing something more useful, like
    actually making machines smarter or helping people publish data in a way
    that’s useful to them.
  author:
    - family: Sporny
      given: Manu
  citation-key: spornyJSONLDWhyHate2014
  container-title: >-
    The Beautiful, Tormented Machine: Art, technology and leaving the world
    better off than we found it.
  issued:
    - year: 2014
      month: 1
      day: 21
  title: JSON-LD and Why I Hate the Semantic Web
  type: post-weblog
  URL: >-
    https://web.archive.org/web/20230526160411/https://manu.sporny.org/2014/json-ld-origins-2/

- id: springelPurSiMuove2010
  abstract: >-
    Hydrodynamic cosmological simulations at present usually employ either the
    Lagrangian smoothed particle hydrodynamics (SPH) technique or Eulerian
    hydrodynamics on a Cartesian mesh with (optional) adaptive mesh refinement
    (AMR). Both of these methods have disadvantages that negatively impact their
    accuracy in certain situations, for example the suppression of fluid
    instabilities in the case of SPH, and the lack of Galilean invariance and
    the presence of overmixing in the case of AMR. We here propose a novel
    scheme which largely eliminates these weaknesses. It is based on a moving
    unstructured mesh defined by the Voronoi tessellation of a set of discrete
    points. The mesh is used to solve the hyperbolic conservation laws of ideal
    hydrodynamics with a finite-volume approach, based on a second-order unsplit
    Godunov scheme with an exact Riemann solver. The mesh-generating points can
    in principle be moved arbitrarily. If they are chosen to be stationary, the
    scheme is equivalent to an ordinary Eulerian method with second-order
    accuracy. If they instead move with the velocity of the local flow, one
    obtains a Lagrangian formulation of continuum hydrodynamics that does not
    suffer from the mesh distortion limitations inherent in other mesh-based
    Lagrangian schemes. In this mode, our new method is fully Galilean
    invariant, unlike ordinary Eulerian codes, a property that is of significant
    importance for cosmological simulations where highly supersonic bulk flows
    are common. In addition, the new scheme can adjust its spatial resolution
    automatically and continuously, and hence inherits the principal advantage
    of SPH for simulations of cosmological structure growth. The high accuracy
    of Eulerian methods in the treatment of shocks is also retained, while the
    treatment of contact discontinuities improves. We discuss how this approach
    is implemented in our new code arepo, both in 2D and in 3D, and is
    parallelized for distributed memory computers. We also discuss techniques
    for adaptive refinement or de-refinement of the unstructured mesh. We
    introduce an individual time-step approach for finite-volume hydrodynamics,
    and present a high-accuracy treatment of self-gravity for the gas that
    allows the new method to be seamlessly combined with a high-resolution
    treatment of collisionless dark matter. We use a suite of test problems to
    examine the performance of the new code and argue that the hydrodynamic
    moving-mesh scheme proposed here provides an attractive and competitive
    alternative to current SPH and Eulerian techniques.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Springel
      given: Volker
  citation-key: springelPurSiMuove2010
  container-title: Monthly Notices of the Royal Astronomical Society
  container-title-short: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1111/j.1365-2966.2009.15715.x
  ISSN: 0035-8711
  issue: '2'
  issued:
    - year: 2010
      month: 1
      day: 11
  page: 791-851
  source: Silverchair
  title: >-
    E pur si muove: Galilean-invariant cosmological hydrodynamical simulations
    on a moving mesh
  title-short: E pur si muove
  type: article-journal
  URL: https://doi.org/10.1111/j.1365-2966.2009.15715.x
  volume: '401'

- id: srivastavaGroundbreakingDefinitiveJournals2011
  abstract: >-
    Do our top journals need to rethink their missions of publishing research
    that is both groundbreaking and definitive? And as a part of that, do they —
    and we scientists — need to recons…
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Srivastava
      given: Sanjay
  citation-key: srivastavaGroundbreakingDefinitiveJournals2011
  container-title: SPSP
  issued:
    - year: 2011
      month: 12
      day: 31
  language: en
  title: Groundbreaking or Definitive? Journals Need to Pick One
  title-short: Groundbreaking or Definitive?
  type: post-weblog
  URL: >-
    https://spsptalks.wordpress.com/2011/12/31/groundbreaking-or-definitive-journals-need-to-pick-one/

- id: stamatogiannakisDecouplingProvenanceCapture2015
  abstract: >-
    Capturing provenance usually involves the direct observation and
    instrumentation of the execution of a program or workflow. However, this
    approach restricts provenance analysis to pre-determined programs and
    methods. This may not pose a problem when one is interested in the
    provenance of a well-defined workflow, but may limit the analysis of
    unstructured processes such as interactive desktop computing. In this paper,
    we present a new approach to capturing provenance based on full execution
    record and replay. Our approach leverages full-system execution trace
    logging and replay, which allows the complete decoupling of analysis from
    the original execution. This enables the selective analysis of the execution
    using progressively heavier instrumentation.
  accessed:
    - year: 2024
      month: 1
      day: 21
  author:
    - family: Stamatogiannakis
      given: Manolis
    - family: Groth
      given: Paul
    - family: Bos
      given: Herbert
  citation-key: stamatogiannakisDecouplingProvenanceCapture2015
  collection-title: TaPP'15
  container-title: >-
    Proceedings of the 7th USENIX Conference on Theory and Practice of
    Provenance
  event-place: USA
  issued:
    - year: 2015
      month: 7
      day: 8
  language: en
  page: '3'
  publisher: USENIX Association
  publisher-place: USA
  source: ACM Digital Library
  title: Decoupling provenance capture and analysis from execution
  type: paper-conference

- id: stamatogiannakisLookingBlackBoxCapturing2015
  abstract: >-
    Knowing the provenance of a data item helps in ascertaining its
    trustworthiness. Various approaches have been proposed to track or infer
    data provenance. However, these approaches either treat an executing program
    as a black-box, limiting the fidelity of the captured provenance, or require
    developers to modify the program to make it provenance-aware. In this paper,
    we introduce DataTracker, a new approach to capturing data provenance based
    on taint tracking, a technique widely used in the security and reverse
    engineering fields. Our system is able to identify data provenance relations
    through dynamic instrumentation of unmodified binaries, without requiring
    access to, or knowledge of, their source code. Hence, we can track
    provenance for a variety of well-known applications. Because DataTracker
    looks inside the executing program, it captures high-fidelity and accurate
    data provenance.
  author:
    - family: Stamatogiannakis
      given: Manolis
    - family: Groth
      given: Paul
    - family: Bos
      given: Herbert
  citation-key: stamatogiannakisLookingBlackBoxCapturing2015
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-319-16462-5_12
  editor:
    - family: Ludäscher
      given: Bertram
    - family: Plale
      given: Beth
  event-place: Cham
  ISBN: 978-3-319-16462-5
  issued:
    - year: 2015
  language: en
  page: 155-167
  publisher: Springer International Publishing
  publisher-place: Cham
  source: Springer Link
  title: >-
    Looking Inside the Black-Box: Capturing Data Provenance Using Dynamic
    Instrumentation
  title-short: Looking Inside the Black-Box
  type: paper-conference

- id: SteveysBlogRants
  accessed:
    - year: 2024
      month: 9
      day: 22
  citation-key: SteveysBlogRants
  language: en
  title: 'Stevey''s Blog Rants: Lisp is Not an Acceptable Lisp'
  title-short: Stevey's Blog Rants
  type: post-weblog
  URL: https://steve-yegge.blogspot.com/2006/04/lisp-is-not-acceptable-lisp.html

- id: stigauthorsRedHatEnterprise2020
  abstract: >-
    Security Technical Implementation Guides (STIGs) that provides a methodology
    for standardized secure installation and maintenance of DOD IA and
    IA-enabled devices and systems.
  accessed:
    - year: 2023
      month: 2
      day: 18
  author:
    - family: STIG Authors
      given: ''
  citation-key: stigauthorsRedHatEnterprise2020
  container-title: STIG Viewer | Unified Compliance Framework®
  issued:
    - year: 2020
      month: 11
      day: 25
  language: en-US
  title: Red Hat Enterprise Linux 8 Security Technical Implementation Guide
  type: webpage
  URL: https://www.stigviewer.com/stig/red_hat_enterprise_linux_8/2020-11-25/

- id: stinnerBenchmarks
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Stinner
      given: Victor
  citation-key: stinnerBenchmarks
  container-title: Victor Stinner's Notes 1.0
  title: Benchmarks
  type: webpage
  URL: https://vstinner.readthedocs.io/benchmark.html

- id: stoddenBestPracticesComputational2014
  abstract: >-
    The goal of this article is to coalesce a discussion around best practices
    for scholarly research that utilizes computational methods, by providing a
    formalized set of best practice recommendations to guide computational
    scientists and other stakeholders wishing to disseminate reproducible
    research, facilitate innovation by enabling data and code re-use, and enable
    broader communication of the output of computational scientific research.
    Scholarly dissemination and communication standards are changing to reflect
    the increasingly computational nature of scholarly research, primarily to
    include the sharing of the data and code associated with published results.
    We also present these Best Practices as a living, evolving, and changing
    document at http://wiki.stodden.net/Best_Practices.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Stodden
      given: Victoria
    - family: Miguez
      given: Sheila
  citation-key: stoddenBestPracticesComputational2014
  container-title: Journal of Open Research Software
  DOI: 10.5334/jors.ay
  ISSN: 2049-9647
  issue: '1'
  issued:
    - year: 2014
      month: 7
      day: 9
  language: en
  license: >-
    Authors who publish with this journal agree to the following terms:   
    Authors retain copyright and grant the journal right of first publication
    with the work simultaneously licensed under a  Creative Commons Attribution
    License  that allows others to share the work with an acknowledgement of the
    work's authorship and initial publication in this journal.  Authors are able
    to enter into separate, additional contractual arrangements for the
    non-exclusive distribution of the journal's published version of the work
    (e.g., post it to an institutional repository or publish it in a book), with
    an acknowledgement of its initial publication in this journal.  Authors are
    permitted and encouraged to post their work online (e.g., in institutional
    repositories or on their website) prior to and during the submission
    process, as it can lead to productive exchanges, as well as earlier and
    greater citation of published work (See  The Effect of Open Access ).  All
    third-party images reproduced on this journal are shared under Educational
    Fair Use. For more information on  Educational Fair Use , please see  this
    useful checklist prepared by Columbia University Libraries .   All
    copyright  of third-party content posted here for research purposes belongs
    to its original owners.  Unless otherwise stated all references to
    characters and comic art presented on this journal are ©, ® or ™ of their
    respective owners. No challenge to any owner’s rights is intended or should
    be inferred.
  number: '1'
  page: e21
  publisher: Ubiquity Press
  source: openresearchsoftware.metajnl.com
  title: >-
    Best Practices for Computational Science: Software Infrastructure and
    Environments for Reproducible and Extensible Research
  title-short: Best Practices for Computational Science
  type: article-journal
  URL: http://openresearchsoftware.metajnl.com/articles/10.5334/jors.ay/
  volume: '2'

- id: stoddenEmpiricalAnalysisJournal2018
  abstract: >-
    A key component of scientific communication is sufficient information for
    other researchers in the field to reproduce published findings. For
    computational and data-enabled research, this has often been interpreted to
    mean making available the raw data from which results were generated, the
    computer code that generated the findings, and any additional information
    needed such as workflows and input parameters. Many journals are revising
    author guidelines to include data and code availability. This work evaluates
    the effectiveness of journal policy that requires the data and code
    necessary for reproducibility be made available postpublication by the
    authors upon request. We assess the effectiveness of such a policy by (i)
    requesting data and code from authors and (ii) attempting replication of the
    published findings. We chose a random sample of 204 scientific papers
    published in the journal Science after the implementation of their policy in
    February 2011. We found that we were able to obtain artifacts from 44% of
    our sample and were able to reproduce the findings for 26%. We find this
    policy—author remission of data and code postpublication upon request—an
    improvement over no policy, but currently insufficient for reproducibility.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Stodden
      given: Victoria
    - family: Seiler
      given: Jennifer
    - family: Ma
      given: Zhaokun
  citation-key: stoddenEmpiricalAnalysisJournal2018
  container-title: Proceedings of the National Academy of Sciences
  DOI: 10.1073/pnas.1708290115
  issue: '11'
  issued:
    - year: 2018
      month: 3
      day: 13
  page: 2584-2589
  publisher: Proceedings of the National Academy of Sciences
  source: pnas.org (Atypon)
  title: >-
    An empirical analysis of journal policy effectiveness for computational
    reproducibility
  type: article-journal
  URL: https://www.pnas.org/doi/full/10.1073/pnas.1708290115
  volume: '115'

- id: stoddenEnhancingReproducibilityComputational2016
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Stodden
      given: Victoria
    - family: McNutt
      given: Marcia
    - family: Bailey
      given: David H.
    - family: Deelman
      given: Ewa
    - family: Gil
      given: Yolanda
    - family: Hanson
      given: Brooks
    - family: Heroux
      given: Michael A.
    - family: Ioannidis
      given: John P.A.
    - family: Taufer
      given: Michela
  citation-key: stoddenEnhancingReproducibilityComputational2016
  container-title: Science
  DOI: 10.1126/science.aah6168
  issue: '6317'
  issued:
    - year: 2016
      month: 12
      day: 9
  note: 'interest: 96'
  page: 1240-1241
  publisher: American Association for the Advancement of Science
  source: science.org (Atypon)
  title: Enhancing reproducibility for computational methods
  type: article-journal
  URL: https://www.science.org/doi/full/10.1126/science.aah6168
  volume: '354'

- id: stoddenReproducibleResearchAddressing2010
  abstract: >-
    Roundtable participants identified ways of making computational research
    details readily available, which is a crucial step in addressing the current
    credibility crisis.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Stodden
      given: Victoria C.
  citation-key: stoddenReproducibleResearchAddressing2010
  DOI: 10.7916/D8WH30H4
  issued:
    - year: 2010
  source: DOI.org (Datacite)
  title: >-
    Reproducible Research: Addressing the Need for Data and Code Sharing in
    Computational Science
  title-short: Reproducible Research
  type: article-journal
  URL: https://academiccommons.columbia.edu/doi/10.7916/D8WH30H4

- id: stoddenRunMyCodeorgNovelDissemination2012
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Stodden
      given: Victoria
    - family: Hurlin
      given: Christophe
    - family: Perignon
      given: Christophe
  citation-key: stoddenRunMyCodeorgNovelDissemination2012
  container-title: 2012 IEEE 8th International Conference on E-Science
  DOI: 10.1109/eScience.2012.6404455
  event-place: Chicago, IL, USA
  event-title: 2012 IEEE 8th International Conference on E-Science (e-Science)
  ISBN: 978-1-4673-4466-1 978-1-4673-4467-8 978-1-4673-4465-4
  issued:
    - year: 2012
      month: 10
  page: 1-8
  publisher: IEEE
  publisher-place: Chicago, IL, USA
  source: DOI.org (Crossref)
  title: >-
    RunMyCode.org: A novel dissemination and collaboration platform for
    executing published computational results
  title-short: RunMyCode.org
  type: paper-conference
  URL: http://ieeexplore.ieee.org/document/6404455/

- id: stoddenScientificMethodPractice2010
  abstract: >-
    Since the 1660’s the scientific method has included reproducibility

    as a mainstay in its effort to root error from scientific discovery. With
    the

    explosive growth of digitization in scientific research and communication,

    it is easier than ever to satisfy this requirement. In computational
    research

    experimental details and methods can be recorded in code and scripts,

    data is digital, papers are frequently online, and the result is the
    potential

    for “really reproducible research.”1 Imagine the ability to routinely

    inspect code and data and recreate others’ results: Every step taken to

    achieve the findings can potentially be transparent. Now imagine anyone

    with an Internet connection and the capability of running the code being

    able to do this.


    This paper investigates the obstacles blocking the sharing of code

    and data to understand conditions under which computational scientists

    reveal their full research compendium. A survey of registrants at a top

    machine learning conference (NIPS) was used to discover the strength of

    underlying factors that affect the decision to reveal code, data, and ideas.

    Sharing of code and data is becoming more common as about a third of

    respondents post some on their websites, and about 85% self report to

    have some code or data publicly available on the web. Contrary to

    theoretical expectations, the decision to share work is grounded in

    communitarian norms, although when work remains hidden private

    incentives dominate the decision. We find that code, data, and ideas are

    each regarded differently in terms of how they are revealed and that

    guidance from scientific norms varies with pervasiveness of computation

    in the field. The largest barriers to sharing are time involved in

    preparation of work and the legal Intellectual Property framework

    scientists face.


    This paper does two things. It provides evidence in the debate

    about whether scientists’ research revealing behavior is wholly governed

    by considerations of personal impact or whether the reasoning behind the

    revealing decision involves larger scientific ideals, and secondly, this

    research describes the actual sharing behavior in the Machine Learning

    community.
  accessed:
    - year: 2023
      month: 1
      day: 24
  author:
    - family: Stodden
      given: Victoria
  citation-key: stoddenScientificMethodPractice2010
  DOI: 10.2139/ssrn.1550193
  event-place: Rochester, NY
  genre: SSRN Scholarly Paper
  issued:
    - year: 2010
      month: 2
      day: 9
  language: en
  note: 'interst: 90'
  number: '1550193'
  publisher-place: Rochester, NY
  source: Social Science Research Network
  title: >-
    The Scientific Method in Practice: Reproducibility in the Computational
    Sciences
  title-short: The Scientific Method in Practice
  type: article
  URL: https://papers.ssrn.com/abstract=1550193

- id: stoddenSettingDefaultReproducible2013
  abstract: >-
    Science is built upon foundations of theory and experiment validated and
    improved through open, transparent communication. With the increasingly
    central role of computation in scientific discovery this means communicating
    all details of the computations needed for others to replicate the
    experiment, i.e. making available to others the associated data and code.
    The "reproducible research" movement recognizes that traditional scientific
    research and publication practices now fall short of this ideal, and
    encourages all those involved in the production of computational science ---
    scientists who use computational methods and the institutions that employ
    them, journals and dissemination mechanisms, and funding agencies --- to
    facilitate and practice really reproducible research
  author:
    - family: Stodden
      given: V.
    - family: Bailey
      given: D. H.
    - family: Borwein
      given: J.
    - family: LeVeque
      given: R. J.
    - family: Rider
      given: W.
    - family: Stein
      given: W.
  citation-key: stoddenSettingDefaultReproducible2013
  issued:
    - year: 2013
      month: 2
      day: 16
  language: en-US
  note: 'interest: 88'
  publisher: >-
    Reproducibility in Computational and Experimental Mathematics (ICERM)
    Workshop
  title: >-
    Setting the Default to Reproducible: Reproducibility in Computational and
    Experimental Mathematics
  title-short: Setting the Default to Reproducible
  type: report
  URL: https://icerm.brown.edu/topical_workshops/tw12-5-rcem/icerm_report.pdf

- id: stoneZEUS2DRadiationMagnetohydrodynamics1992
  abstract: >-
    A detailed description of ZEUS-2D, a numerical code for the simulation of
    fluid dynamical flows including a self-consistent treatment of the effects
    of magnetic fields and radiation transfer is presented. Attention is given
    to the hydrodynamic (HD) algorithms which form the foundation for the more
    complex MHD and radiation HD algorithms. The effect of self-gravity on the
    flow dynamics is accounted for by an iterative solution of the sparse-banded
    matrix resulting from discretizing the Poisson equation in multidimensions.
    The results of an extensive series of HD test problems are presented. A
    detailed description of the MHD algorithms in ZEUS-2D is presented. A new
    method of computing the electromotive force is developed using the method of
    characteristics (MOC). It is demonstrated through the results of an
    extensive series of MHD test problems that the resulting hybrid
    MOC-constrained transport method provides for the accurate evolution of all
    modes of MHD wave families.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Stone
      given: James M.
    - family: Norman
      given: Michael L.
  citation-key: stoneZEUS2DRadiationMagnetohydrodynamics1992
  container-title: The Astrophysical Journal Supplement Series
  DOI: 10.1086/191680
  ISSN: 0067-0049
  issued:
    - year: 1992
      month: 6
      day: 1
  note: 'ADS Bibcode: 1992ApJS...80..753S'
  page: '753'
  source: NASA ADS
  title: >-
    ZEUS-2D: A Radiation Magnetohydrodynamics Code for Astrophysical Flows in
    Two Space Dimensions. I. The Hydrodynamic Algorithms and Tests
  title-short: ZEUS-2D
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1992ApJS...80..753S
  volume: '80'

- id: stoneZEUS2DRadiationMagnetohydrodynamics1992a
  abstract: >-
    In this, the second of a series of three papers, we continue a detailed
    description of ZEUS-2D, a numerical code for the simulation of fluid
    dynamical flows in astrophysics including a self-consistent treatment of the
    effects of magnetic fields and radiation transfer. In this paper, we give a
    detailed description of the magnetohydrodynamical (MHD) algorithms in
    ZEUS-2D. The recently developed constrained transport (CT) algorithm is
    implemented for the numerical evolution of the components of the magnetic
    field for MHD simulations. This formalism guarantees the numerically evolved
    field components will satisfy the divergence-free constraint at all times.
    We find, however, that the method used to compute the electromotive forces
    must be chosen carefully to propagate accurately all modes of MHD wave
    families (in particular shear Alfvén waves). A new method of computing the
    electromotive force is developed using the method of characteristics (MOC).
    It is demonstrated through the results of an extensive series of MHD test
    problems that the resulting hybrid MOC-CT method provides for the accurate
    evolution of all modes of MHD wave families.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Stone
      given: James M.
    - family: Norman
      given: Michael L.
  citation-key: stoneZEUS2DRadiationMagnetohydrodynamics1992a
  container-title: The Astrophysical Journal Supplement Series
  DOI: 10.1086/191681
  ISSN: 0067-0049
  issued:
    - year: 1992
      month: 6
      day: 1
  note: 'ADS Bibcode: 1992ApJS...80..791S'
  page: '791'
  source: NASA ADS
  title: >-
    ZEUS-2D: A Radiation Magnetohydrodynamics Code for Astrophysical Flows in
    Two Space Dimensions. II. The Magnetohydrodynamic Algorithms and Tests
  title-short: ZEUS-2D
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1992ApJS...80..791S
  volume: '80'

- id: strangConstructionComparisonDifference1968
  abstract: >-
    In this paper, we present Approximation Schemes for Capacitated Vehicle
    Routing Problem (CVRP) on several classes of graphs. In CVRP, introduced by
    Dantzig and Ramser in 1959 [13], we are given a graph G = (V, E) with metric
    edges costs, a depot r ∊ V, and a vehicle of bounded capacity Q. The goal is
    to find a minimum cost collection of tours for the vehicle that returns to
    the depot, each visiting at most Q nodes, such that they cover all the
    nodes. This generalizes classic TSP and has been studied extensively. In the
    more general setting, each node v has a demand dv and the total demand of
    each tour must be no more than Q. Either the demand of each node must be
    served by one tour (unsplittable) or can be served by multiple tours
    (splittable). The best known approximation algorithm for general graphs has
    ratio α + 2(1–∊) (for the unsplittable) and α + 1–∊ (for the splittable) for
    some fixed  is the best approximation for TSP. Even for the case of trees,
    the best approximation ratio is 4/3 [5], and it has been an open question if
    there is an approximation scheme for this simple class of graphs. Das and
    Mathieu [14] presented an approximation scheme with time  for Euclidean
    plane ℝ2. No other approximation scheme is known for any other class of
    metrics (without further restrictions on Q). In this paper, we make
    significant progress on this classic problem by presenting Quasi-Polynomial
    Time Approximation Schemes (QPTAS) for graphs of bounded treewidth, graphs
    of bounded highway dimensions, and graphs of bounded doubling dimensions.
    For comparison, our result implies an approximation scheme for Euclidean
    plane with run time .
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Strang
      given: Gilbert
  citation-key: strangConstructionComparisonDifference1968
  container-title: SIAM Journal on Numerical Analysis
  container-title-short: SIAM J. Numer. Anal.
  DOI: 10.1137/0705041
  ISSN: 0036-1429
  issue: '3'
  issued:
    - year: 1968
      month: 9
  page: 506-517
  publisher: Society for Industrial and Applied Mathematics
  source: epubs.siam.org (Atypon)
  title: On the Construction and Comparison of Difference Schemes
  type: article-journal
  URL: https://epubs.siam.org/doi/10.1137/0705041
  volume: '5'

- id: suenS2LoggerEndtoEndData2013
  abstract: >-
    The inability to effectively track data in cloud computing environments is
    becoming one of the top concerns for cloud stakeholders. This inability is
    due to two main reasons. Firstly, the lack of data tracking tools built for
    clouds. Secondly, current logging mechanisms are only designed from a
    system-centric perspective. There is a need for data-centric logging
    techniques which can trace data activities (e.g. file creation, edition,
    duplication, transfers, deletions, etc.) within and across all cloud
    servers. This will effectively enable full transparency and accountability
    for data movements in the cloud. In this paper, we introduce S2Logger, a
    data event logging mechanism which captures, analyses and visualizes data
    events in the cloud from the data point of view. By linking together atomic
    data events captured at both file and block level, the resulting sequence of
    data events depicts the cloud data provenance records throughout the data
    lifecycle. With this information, we can then detect critical data-related
    cloud security problems such as malicious actions, data leakages and data
    policy violations by analysing the data provenance. S2Logger also enables us
    to address the gaps and inadequacies of existing system-centric security
    tools.
  author:
    - family: Suen
      given: Chun Hui
    - family: Ko
      given: Ryan K.L.
    - family: Tan
      given: Yu Shyang
    - family: Jagadpramana
      given: Peter
    - family: Lee
      given: Bu Sung
  citation-key: suenS2LoggerEndtoEndData2013
  container-title: >-
    2013 12th IEEE International Conference on Trust, Security and Privacy in
    Computing and Communications
  DOI: 10.1109/TrustCom.2013.73
  event-title: >-
    2013 12th IEEE International Conference on Trust, Security and Privacy in
    Computing and Communications
  ISSN: 2324-9013
  issued:
    - year: 2013
      month: 7
  page: 594-602
  source: IEEE Xplore
  title: 'S2Logger: End-to-End Data Tracking Mechanism for Cloud Data Provenance'
  title-short: S2Logger
  type: paper-conference

- id: suhEMPExecutionTime2017
  abstract: >-
    Measuring execution time is one of the most used performance evaluation
    techniques in computer science research. Inaccurate measurements cannot be
    used for a fair performance comparison between programs. Despite the
    prevalence of its use, the intrinsic variability in the time measurement
    makes it hard to obtain repeatable and accurate timing results of a program
    running on an operating system. We propose a novel execution time
    measurement protocol (termed EMP) for measuring the execution time of a
    compute-bound program on Linux, while minimizing that measurement's
    variability. During the development of execution time measurement protocol,
    we identified several factors that disturb execution time measurement. We
    introduce successive refinements to the protocol by addressing each of these
    factors, in concert, reducing variability by more than an order of
    magnitude. We also introduce a new visualization technique, what we term
    ‘dual-execution scatter plot’ that highlights infrequent, long-running
    daemons, differentiating them from frequent and/or short-running daemons.
    Our empirical results show that the proposed protocol successfully achieves
    three major aspects—precision, accuracy, and scalability—in execution time
    measurement that can work for open-source and proprietary software.
    Copyright © 2017 John Wiley & Sons, Ltd.
  accessed:
    - year: 2023
      month: 8
      day: 22
  author:
    - family: Suh
      given: Young-Kyoon
    - family: Snodgrass
      given: Richard T.
    - family: Kececioglu
      given: John D.
    - family: Downey
      given: Peter J.
    - family: Maier
      given: Robert S.
    - family: Yi
      given: Cheng
  citation-key: suhEMPExecutionTime2017
  container-title: 'Software: Practice and Experience'
  DOI: 10.1002/spe.2476
  ISSN: 0038-0644, 1097-024X
  issue: '4'
  issued:
    - year: 2017
      month: 4
  language: en
  license: Copyright © 2017 John Wiley & Sons, Ltd.
  note: 'interest: 71'
  page: 559-597
  source: Wiley Online Library
  title: 'EMP: execution time measurement protocol for compute-bound programs'
  title-short: EMP
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/spe.2476
  volume: '47'

- id: sultanaFileProvenanceSystem2013
  abstract: >-
    A file provenance system supports the automatic collection and management of
    provenance i.e. the complete processing history of a data object. File
    system level provenance provides functionality unavailable in the existing
    provenance systems. In this paper, we discuss the design objectives for a
    flexible and efficient file provenance system and then propose the design of
    such a system, called FiPS. We design FiPS as a thin stackable file system
    for capturing provenance in a portable manner. FiPS can capture provenance
    at various degrees of granularity, can transform provenance records into
    secure information, and can direct the resulting provenance data to various
    persistent storage systems.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Sultana
      given: Salmin
    - family: Bertino
      given: Elisa
  citation-key: sultanaFileProvenanceSystem2013
  collection-title: CODASPY '13
  container-title: >-
    Proceedings of the third ACM conference on Data and application security and
    privacy
  DOI: 10.1145/2435349.2435368
  event-place: New York, NY, USA
  ISBN: 978-1-4503-1890-7
  issued:
    - year: 2013
      month: 2
      day: 18
  page: 153–156
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: A file provenance system
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/2435349.2435368

- id: SystemdJournal
  accessed:
    - year: 2023
      month: 9
      day: 17
  citation-key: SystemdJournal
  title: systemd journal
  type: webpage
  URL: >-
    https://docs.google.com/document/u/0/d/1IC9yOXj7j6cdLLxWEBAGRL6wl97tFxgjLUEHIX3MSTs/pub

- id: szalaySloanDigitalSky1999
  abstract: >-
    The next-generation astronomy archives will cover most of the universe at
    fine resolution in many wavelengths. One of the first of these projects, the
    Sloan Digital Sky Survey (SDSS) will create a 5-wavelength catalog over
    10,000 square degrees of the sky. The 200 million objects in the
    multi-terabyte database will have mostly numerical attributes, defining a
    space of 100+ dimensions. Points in this space have highly correlated
    distributions. The archive will enable astronomers to explore the data
    interactively. Data access will be aided by multidimensional spatial
    indices. The data will be partitioned in many ways. Small tag objects
    consisting of the most popular attributes speed up frequent searches.
    Splitting the data among multiple servers enables parallel, scalable I/O.
    Hashing techniques allow efficient clustering and pairwise comparison
    algorithms. Randomly sampled subsets allow debugging otherwise large queries
    at the desktop. Central servers will operate a data pump that supports
    sweeping searches that touch most of the data.
  accessed:
    - year: 2022
      month: 4
      day: 12
  author:
    - family: Szalay
      given: Alexander S.
    - family: Kunszt
      given: Peter
    - family: Thakar
      given: Anirudha
    - family: Gray
      given: Jim
    - family: Slutz
      given: Don
  citation-key: szalaySloanDigitalSky1999
  container-title: arXiv:astro-ph/9912382
  issued:
    - year: 1999
      month: 12
      day: 17
  note: 'interest: 80'
  source: arXiv.org
  title: The Sloan Digital Sky Survey and its Archive
  type: article-journal
  URL: http://arxiv.org/abs/astro-ph/9912382

- id: targetHowLispBecame2018
  abstract: >-
    A look at the fascinating history behind the one programming language with
    magical powers.
  accessed:
    - year: 2024
      month: 1
      day: 3
  author:
    - family: Target
      given: Sinclair
  citation-key: targetHowLispBecame2018
  issued:
    - year: 2018
      month: 10
      day: 14
  title: How Lisp Became God's Own Programming Language
  type: post-weblog
  URL: https://twobithistory.org/2018/10/14/lisp.html

- id: targetWhateverHappenedSemantic2018
  accessed:
    - year: 2023
      month: 6
      day: 6
  author:
    - family: Target
      given: Sinclair
  citation-key: targetWhateverHappenedSemantic2018
  issued:
    - year: 2018
      month: 5
      day: 27
  title: Whatever Happened to the Semantic Web?
  type: webpage
  URL: https://twobithistory.org/2018/05/27/semantic-web.html

- id: tariqAutomatedCollectionApplicationLevel2012
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Tariq
      given: Dawood
    - family: Masaim
      given: Ali
    - family: Gehani
      given: Ashish
  citation-key: tariqAutomatedCollectionApplicationLevel2012
  event-title: 4th USENIX Workshop on the Theory and Practice of Provenance (TaPP 12)
  issued:
    - year: 2012
  language: en
  source: www.usenix.org
  title: Towards Automated Collection of {Application-Level} Data Provenance
  type: paper-conference
  URL: https://www.usenix.org/conference/tapp12/workshop-program/presentation/tariq

- id: taylorWorkflowsEScienceScientific2014
  abstract: >-
    Workflows for e-Science is divided into four parts, which represent four
    broad but distinct areas of scientific workflows. In the first part,
    Background, we introduce the concept of scientific workflows and set the
    scene by describing how they differ from their business workflow
    counterpart. In Part II, Application and User Perspective, we provide a
    number of scientific examples that currently use workflows for their
    e-Science experiments. In Workflow Representation and Common Structure (Part
    III), we describe core workflow themes, such as control flow or dataflow and
    the use of components or services. In this part, we also provide overviews
    for a number of common workflow languages, such as Petri Nets, the Business
    Process Execution Language (BPEL), and the Virtual Data Language (VDL),
    along with service interfaces. In Part IV, Frameworks and Tools, we take a
    look at many of the popular environments that are currently being used for
    e-Science applications by paying particular attention to their workflow
    capabilities. The following four sections describe the chapters in each part
    and therefore provide a comprehensive summary of the book as a whole.
  citation-key: taylorWorkflowsEScienceScientific2014
  edition: 2007th edition
  editor:
    - family: Taylor
      given: Ian J.
    - family: Deelman
      given: Ewa
    - family: Gannon
      given: Dennis B.
    - family: Shields
      given: Matthew
  ISBN: 978-1-84996-619-1
  issued:
    - year: 2014
      month: 3
      day: 27
  language: English
  number-of-pages: '548'
  publisher: Springer
  source: link.springer.com
  title: 'Workflows for e-Science: Scientific Workflows for Grids'
  title-short: Workflows for e-Science
  type: book
  URL: https://link.springer.com/book/10.1007/978-1-84628-757-2

- id: tennantMultidisciplinaryPerspectiveEmergent2017
  abstract: >-
    Peer review of research articles is a core part of our scholarly
    communication system. In spite of its importance, the status and purpose of
    peer review is often contested. What is its role in our modern digital
    research and communications infrastructure? Does it perform to the high
    standards with which it is generally regarded? Studies of peer review have
    shown that it is prone to bias and abuse in numerous dimensions, frequently
    unreliable, and can fail to detect even fraudulent research. With the advent
    of web technologies, we are now witnessing a phase of innovation and
    experimentation in our approaches to peer review. These developments
    prompted us to examine emerging models of peer review from a range of
    disciplines and venues, and to ask how they might address some of the issues
    with our current systems of peer review. We examine the functionality of a
    range of social Web platforms, and compare these with the traits underlying
    a viable peer review system: quality control, quantified performance metrics
    as engagement incentives, and certification and reputation. Ideally, any new
    systems will demonstrate that they out-perform and reduce the biases of
    existing models as much as possible. We conclude that there is considerable
    scope for new peer review initiatives to be developed, each with their own
    potential issues and advantages. We also propose a novel hybrid platform
    model that could, at least partially, resolve many of the socio-technical
    issues associated with peer review, and potentially disrupt the entire
    scholarly communication system. Success for any such development relies on
    reaching a critical threshold of research community engagement with both the
    process and the platform, and therefore cannot be achieved without a
    significant change of incentives in research environments.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Tennant
      given: Jonathan P.
    - family: Dugan
      given: Jonathan M.
    - family: Graziotin
      given: Daniel
    - family: Jacques
      given: Damien C.
    - family: Waldner
      given: François
    - family: Mietchen
      given: Daniel
    - family: Elkhatib
      given: Yehia
    - family: B. Collister
      given: Lauren
    - family: Pikas
      given: Christina K.
    - family: Crick
      given: Tom
    - family: Masuzzo
      given: Paola
    - family: Caravaggi
      given: Anthony
    - family: Berg
      given: Devin R.
    - family: Niemeyer
      given: Kyle E.
    - family: Ross-Hellauer
      given: Tony
    - family: Mannheimer
      given: Sara
    - family: Rigling
      given: Lillian
    - family: Katz
      given: Daniel S.
    - family: Greshake Tzovaras
      given: Bastian
    - family: Pacheco-Mendoza
      given: Josmel
    - family: Fatima
      given: Nazeefa
    - family: Poblet
      given: Marta
    - family: Isaakidis
      given: Marios
    - family: Irawan
      given: Dasapta Erwin
    - family: Renaut
      given: Sébastien
    - family: Madan
      given: Christopher R.
    - family: Matthias
      given: Lisa
    - family: Nørgaard Kjær
      given: Jesper
    - family: O'Donnell
      given: Daniel Paul
    - family: Neylon
      given: Cameron
    - family: Kearns
      given: Sarah
    - family: Selvaraju
      given: Manojkumar
    - family: Colomb
      given: Julien
  citation-key: tennantMultidisciplinaryPerspectiveEmergent2017
  container-title: F1000Research
  container-title-short: F1000Res
  DOI: 10.12688/f1000research.12037.3
  ISSN: 2046-1402
  issued:
    - year: 2017
      month: 11
      day: 29
  note: 'interest: 87'
  page: '1151'
  PMCID: PMC5686505
  PMID: '29188015'
  source: PubMed Central
  title: >-
    A multi-disciplinary perspective on emergent and future innovations in peer
    review
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5686505/
  volume: '6'

- id: terrellPeonageUnitedStates1907
  accessed:
    - year: 2022
      month: 5
      day: 10
  author:
    - family: Terrell
      given: Mary Church
  citation-key: terrellPeonageUnitedStates1907
  issued:
    - year: 1907
  language: en
  title: Peonage in the United States
  type: document
  URL: >-
    https://awpc.cattcenter.iastate.edu/2019/11/22/peonage-in-the-united-states-1907/

- id: thainCommonModelHighly2014
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Thain
      given: Douglas
  citation-key: thainCommonModelHighly2014
  container-title: Prof. Douglas Thain
  issued:
    - year: 2014
      month: 5
      day: 19
  note: 'interest: 64'
  title: Toward a Common Model of Highly Concurrent Programming
  title-short: Prof. Douglas Thain
  type: post-weblog
  URL: https://dthain.blogspot.com/2014/05/toward-common-model-of-highly.html

- id: thalheimInspectorDataProvenance2016a
  abstract: >-
    Data provenance strives for explaining how the computation was performed by
    recording a trace of the execution. The provenance trace is useful across a
    wide-range of workflows to improve the dependability, security, and
    efficiency of software systems. In this paper, we present Inspector, a
    POSIX-compliant data provenance library for shared-memory multithreaded
    programs. The Inspector library is completely transparent and easy to use:
    it can be used as a replacement for the pthreads library by a simple
    exchange of libraries linked, without even recompiling the application code.
    To achieve this result, we present a parallel provenance algorithm that
    records control, data, and schedule dependencies using a Concurrent
    Provenance Graph (CPG). We implemented our algorithm to operate at the
    compiled binary code level by leveraging a combination of OS-specific
    mechanisms, and recently released Intel PT ISA extensions as part of the
    Broadwell micro-architecture. Our evaluation on a multicore platform using
    applications from multithreaded benchmarks suites (PARSEC and Phoenix) shows
    reasonable provenance overheads for a majority of applications. Lastly, we
    briefly describe three case-studies where the generic interface exported by
    Inspector is being used to improve the dependability, security, and
    efficiency of systems. The Inspector library is publicly available for
    further use in a wide range of other provenance workflows.
  accessed:
    - year: 2024
      month: 1
      day: 21
  author:
    - family: Thalheim
      given: Jörg
    - family: Bhatotia
      given: Pramod
    - family: Fetzer
      given: Christof
  citation-key: thalheimInspectorDataProvenance2016a
  DOI: 10.48550/arXiv.1605.00498
  issued:
    - year: 2016
      month: 5
      day: 2
  number: arXiv:1605.00498
  publisher: arXiv
  source: arXiv.org
  title: 'Inspector: A Data Provenance Library for Multithreaded Programs'
  title-short: Inspector
  type: article
  URL: http://arxiv.org/abs/1605.00498

- id: thegalaxycommunityGalaxyPlatformAccessible2024
  abstract: "Abstract\n            Galaxy (https://galaxyproject.org) is deployed globally, predominantly through free-to-use services, supporting user-driven research that broadens in scope each year. Users are attracted to public Galaxy services by platform stability, tool and reference dataset diversity, training, support\_and integration, which enables complex, reproducible, shareable data analysis. Applying the principles of user experience design (UXD), has driven improvements in accessibility, tool discoverability through Galaxy Labs/subdomains, and a redesigned Galaxy ToolShed. Galaxy tool capabilities are progressing in two strategic directions: integrating general purpose graphical processing units (GPGPU) access for cutting-edge methods, and licensed tool support. Engagement with global research consortia is being increased by developing more workflows in Galaxy and by resourcing the public Galaxy services to run them. The Galaxy Training Network (GTN) portfolio has grown in both size, and accessibility, through learning paths and direct integration with Galaxy tools that feature in training courses. Code development continues in line with the Galaxy Project roadmap, with improvements to job scheduling and the user interface. Environmental impact assessment is also helping engage users and developers, reminding them of their role in sustainability, by displaying estimated CO2 emissions generated by each Galaxy job."
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - literal: The Galaxy Community
    - family: Abueg
      given: Linelle Ann L
    - family: Afgan
      given: Enis
    - family: Allart
      given: Olivier
    - family: Awan
      given: Ahmed H
    - family: Bacon
      given: Wendi A
    - family: Baker
      given: Dannon
    - family: Bassetti
      given: Madeline
    - family: Batut
      given: Bérénice
    - family: Bernt
      given: Matthias
    - family: Blankenberg
      given: Daniel
    - family: Bombarely
      given: Aureliano
    - family: Bretaudeau
      given: Anthony
    - family: Bromhead
      given: Catherine J
    - family: Burke
      given: Melissa L
    - family: Capon
      given: Patrick K
    - family: Čech
      given: Martin
    - family: Chavero-Díez
      given: María
    - family: Chilton
      given: John M
    - family: Collins
      given: Tyler J
    - family: Coppens
      given: Frederik
    - family: Coraor
      given: Nate
    - family: Cuccuru
      given: Gianmauro
    - family: Cumbo
      given: Fabio
    - family: Davis
      given: John
    - family: De Geest
      given: Paul F
    - family: De Koning
      given: Willem
    - family: Demko
      given: Martin
    - family: DeSanto
      given: Assunta
    - family: Begines
      given: José Manuel Domínguez
    - family: Doyle
      given: Maria A
    - family: Droesbeke
      given: Bert
    - family: Erxleben-Eggenhofer
      given: Anika
    - family: Föll
      given: Melanie C
    - family: Formenti
      given: Giulio
    - family: Fouilloux
      given: Anne
    - family: Gangazhe
      given: Rendani
    - family: Genthon
      given: Tanguy
    - family: Goecks
      given: Jeremy
    - family: Beltran
      given: Alejandra N Gonzalez
    - family: Goonasekera
      given: Nuwan A
    - family: Goué
      given: Nadia
    - family: Griffin
      given: Timothy J
    - family: Grüning
      given: Björn A
    - family: Guerler
      given: Aysam
    - family: Gundersen
      given: Sveinung
    - family: Gustafsson
      given: Ove Johan Ragnar
    - family: Hall
      given: Christina
    - family: Harrop
      given: Thomas W
    - family: Hecht
      given: Helge
    - family: Heidari
      given: Alireza
    - family: Heisner
      given: Tillman
    - family: Heyl
      given: Florian
    - family: Hiltemann
      given: Saskia
    - family: Hotz
      given: Hans-Rudolf
    - family: Hyde
      given: Cameron J
    - family: Jagtap
      given: Pratik D
    - family: Jakiela
      given: Julia
    - family: Johnson
      given: James E
    - family: Joshi
      given: Jayadev
    - family: Jossé
      given: Marie
    - family: Jum’ah
      given: Khaled
    - family: Kalaš
      given: Matúš
    - family: Kamieniecka
      given: Katarzyna
    - family: Kayikcioglu
      given: Tunc
    - family: Konkol
      given: Markus
    - family: Kostrykin
      given: Leonid
    - family: Kucher
      given: Natalie
    - family: Kumar
      given: Anup
    - family: Kuntz
      given: Mira
    - family: Lariviere
      given: Delphine
    - family: Lazarus
      given: Ross
    - family: Bras
      given: Yvan Le
    - family: Corguillé
      given: Gildas Le
    - family: Lee
      given: Justin
    - family: Leo
      given: Simone
    - family: Liborio
      given: Leandro
    - family: Libouban
      given: Romane
    - family: Tabernero
      given: David López
    - family: Lopez-Delisle
      given: Lucille
    - family: Los
      given: Laila S
    - family: Mahmoud
      given: Alexandru
    - family: Makunin
      given: Igor
    - family: Marin
      given: Pierre
    - family: Mehta
      given: Subina
    - family: Mok
      given: Winnie
    - family: Moreno
      given: Pablo A
    - family: Morier-Genoud
      given: François
    - family: Mosher
      given: Stephen
    - family: Müller
      given: Teresa
    - family: Nasr
      given: Engy
    - family: Nekrutenko
      given: Anton
    - family: Nelson
      given: Tiffanie M
    - family: Oba
      given: Asime J
    - family: Ostrovsky
      given: Alexander
    - family: Polunina
      given: Polina V
    - family: Poterlowicz
      given: Krzysztof
    - family: Price
      given: Elliott J
    - family: Price
      given: Gareth R
    - family: Rasche
      given: Helena
    - family: Raubenolt
      given: Bryan
    - family: Royaux
      given: Coline
    - family: Sargent
      given: Luke
    - family: Savage
      given: Michelle T
    - family: Savchenko
      given: Volodymyr
    - family: Savchenko
      given: Denys
    - family: Schatz
      given: Michael C
    - family: Seguineau
      given: Pauline
    - family: Serrano-Solano
      given: Beatriz
    - family: Soranzo
      given: Nicola
    - family: Srikakulam
      given: Sanjay Kumar
    - family: Suderman
      given: Keith
    - family: Syme
      given: Anna E
    - family: Tangaro
      given: Marco Antonio
    - family: Tedds
      given: Jonathan A
    - family: Tekman
      given: Mehmet
    - family: Cheng (Mike) Thang
      given: Wai
    - family: Thanki
      given: Anil S
    - family: Uhl
      given: Michael
    - family: Van Den Beek
      given: Marius
    - family: Varshney
      given: Deepti
    - family: Vessio
      given: Jenn
    - family: Videm
      given: Pavankumar
    - family: Von Kuster
      given: Greg
    - family: Watson
      given: Gregory R
    - family: Whitaker-Allen
      given: Natalie
    - family: Winter
      given: Uwe
    - family: Wolstencroft
      given: Martin
    - family: Zambelli
      given: Federico
    - family: Zierep
      given: Paul
    - family: Zoabi
      given: Rand
  citation-key: thegalaxycommunityGalaxyPlatformAccessible2024
  container-title: Nucleic Acids Research
  DOI: 10.1093/nar/gkae410
  ISSN: 0305-1048, 1362-4962
  issue: W1
  issued:
    - year: 2024
      month: 7
      day: 5
  language: en
  license: https://creativecommons.org/licenses/by/4.0/
  page: W83-W94
  source: DOI.org (Crossref)
  title: >-
    The Galaxy platform for accessible, reproducible, and collaborative data
    analyses: 2024 update
  title-short: >-
    The Galaxy platform for accessible, reproducible, and collaborative data
    analyses
  type: article-journal
  URL: https://academic.oup.com/nar/article/52/W1/W83/7676834
  volume: '52'

- id: thiesStreamItLanguageStreaming2002
  abstract: >-
    We characterize high-performance streaming applications as a new and
    distinct domain of programs that is becoming increasingly important. The
    StreamIt language provides novel high-level representations to improve
    programmer productivity and program robustness within the streaming domain.
    At the same time, the StreamIt compiler aims to improve the performance of
    streaming applications via stream-specific analyses and optimizations. In
    this paper, we motivate, describe and justify the language features of
    StreamIt, which include: a structured model of streams, a messaging system
    for control, a re-initialization mechanism, and a natural textual syntax.
  author:
    - family: Thies
      given: William
    - family: Karczmarek
      given: Michal
    - family: Amarasinghe
      given: Saman
  citation-key: thiesStreamItLanguageStreaming2002
  collection-title: Lecture Notes in Computer Science
  container-title: Compiler Construction
  DOI: 10.1007/3-540-45937-5_14
  editor:
    - family: Horspool
      given: R. Nigel
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-45937-8
  issued:
    - year: 2002
  language: en
  note: 'interest: 75'
  page: 179-196
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: 'StreamIt: A Language for Streaming Applications'
  title-short: StreamIt
  type: paper-conference

- id: timperleyUnderstandingImprovingArtifact2021
  abstract: >-
    In recent years, many software engineering researchers have begun to include
    artifacts alongside their research papers. Ideally, artifacts, including
    tools, benchmarks, and data, support the dissemination of ideas, provide
    evidence for research claims, and serve as a starting point for future
    research. However, in practice, artifacts suffer from a variety of issues
    that prevent the realization of their full potential. To help the software
    engineering community realize the full potential of artifacts, we seek to
    understand the challenges involved in the creation, sharing, and use of
    artifacts. To that end, we perform a mixed-methods study including a survey
    of artifacts in software engineering publications, and an online survey of
    153 software engineering researchers. By analyzing the perspectives of
    artifact creators, users, and reviewers, we identify several high-level
    challenges that affect the quality of artifacts including mismatched
    expectations between these groups, and a lack of sufficient reward for both
    creators and reviewers. Using Diffusion of Innovations as an analytical
    framework, we examine how these challenges relate to one another, and build
    an understanding of the factors that affect the sharing and success of
    artifacts. Finally, we make recommendations to improve the quality of
    artifacts based on our results and existing best practices.
  accessed:
    - year: 2023
      month: 5
      day: 6
  author:
    - family: Timperley
      given: Christopher S.
    - family: Herckis
      given: Lauren
    - family: Goues
      given: Claire Le
    - family: Hilton
      given: Michael
  citation-key: timperleyUnderstandingImprovingArtifact2021
  container-title: Empirical Software Engineering
  container-title-short: Empir Software Eng
  DOI: 10.1007/s10664-021-09973-5
  ISSN: 1382-3256, 1573-7616
  issue: '4'
  issued:
    - year: 2021
      month: 7
  note: 'interest: 90'
  page: '67'
  source: arXiv.org
  title: >-
    Understanding and Improving Artifact Sharing in Software Engineering
    Research
  type: article-journal
  URL: http://arxiv.org/abs/2008.01046
  volume: '26'

- id: tomassiBugSwarmMiningContinuously2019
  abstract: >-
    Fault-detection, localization, and repair methods are vital to software
    quality; but it is difficult to evaluate their generality, applicability,
    and current effectiveness. Large, diverse, realistic datasets of
    durably-reproducible faults and fixes are vital to good experimental
    evaluation of approaches to software quality, but they are difficult and
    expensive to assemble and keep current. Modern continuous-integration (CI)
    approaches, like TRAVIS-CI, which are widely used, fully configurable, and
    executed within custom-built containers, promise a path toward much larger
    defect datasets. If we can identify and archive failing and subsequent
    passing runs, the containers will provide a substantial assurance of durable
    future reproducibility of build and test. Several obstacles, however, must
    be overcome to make this a practical reality. We describe BUGSWARM, a
    toolset that navigates these obstacles to enable the creation of a scalable,
    diverse, realistic, continuously growing set of durably reproducible failing
    and passing versions of real-world, open-source systems. The BUGSWARM
    toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all
    packaged within fully reproducible containers. Furthermore, the toolkit can
    be run periodically to detect fail-pass activities, thus growing the dataset
    continually.
  author:
    - family: Tomassi
      given: David A.
    - family: Dmeiri
      given: Naji
    - family: Wang
      given: Yichen
    - family: Bhowmick
      given: Antara
    - family: Liu
      given: Yen-Chuan
    - family: Devanbu
      given: Premkumar T.
    - family: Vasilescu
      given: Bogdan
    - family: Rubio-González
      given: Cindy
  citation-key: tomassiBugSwarmMiningContinuously2019
  container-title: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
  DOI: 10.1109/ICSE.2019.00048
  event-title: 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)
  ISSN: 1558-1225
  issued:
    - year: 2019
      month: 5
  note: 'interest: 98'
  page: 339-349
  source: IEEE Xplore
  title: >-
    BugSwarm: Mining and Continuously Growing a Dataset of Reproducible Failures
    and Fixes
  title-short: BugSwarm
  type: paper-conference

- id: tonthatSciunitsReusableResearch2017
  abstract: >-
    Science is conducted collaboratively, often requiring knowledge sharing
    about computational experiments. When experiments include only datasets,
    they can be shared using Uniform Resource Identifiers (URIs) or Digital
    Object Identifiers (DOIs). An experiment, however, seldom includes only
    datasets, but more often includes software, its past execution, provenance,
    and associated documentation. The Research Object has recently emerged as a
    comprehensive and systematic method for aggregation and identification of
    diverse elements of computational experiments. While a necessary method,
    mere aggregation is not sufficient for the sharing of computational
    experiments. Other users must be able to easily recompute on these shared
    research objects. In this paper, we present the sciunit, a reusable research
    object in which aggregated content is recomputable. We describe a Git-like
    client that efficiently creates, stores, and repeats sciunits. We show
    through analysis that sciunits repeat computational experiments with minimal
    storage and processing overhead. Finally, we provide an overview of sharing
    and reproducible cyberinfrastructure based on sciunits gaining adoption in
    the domain of geosciences.
  author:
    - family: Ton That
      given: Dai Hai
    - family: Fils
      given: Gabriel
    - family: Yuan
      given: Zhihao
    - family: Malik
      given: Tanu
  citation-key: tonthatSciunitsReusableResearch2017
  container-title: 2017 IEEE 13th International Conference on e-Science (e-Science)
  DOI: 10.1109/eScience.2017.51
  event-title: 2017 IEEE 13th International Conference on e-Science (e-Science)
  issued:
    - year: 2017
      month: 10
  page: 374-383
  source: IEEE Xplore
  title: 'Sciunits: Reusable Research Objects'
  title-short: Sciunits
  type: paper-conference

- id: treatEverythingYouKnow2015
  abstract: >-
    Okay, maybe not everything you know about latency is wrong. But now that I
    have your attention, we can talk about why the tools and methodologies you
    use to measure and reason about latency are lik…
  accessed:
    - year: 2024
      month: 9
      day: 24
  author:
    - family: Treat
      given: Tyler
  citation-key: treatEverythingYouKnow2015
  container-title: Brave New Geek
  issued:
    - year: 2015
      month: 12
      day: 12
  language: en-US
  title: Everything You Know About Latency Is Wrong
  type: post-weblog
  URL: https://bravenewgeek.com/everything-you-know-about-latency-is-wrong/

- id: trisovicLargescaleStudyResearch2022
  abstract: >-
    This article presents a study on the quality and execution of research code
    from publicly-available replication datasets at the Harvard Dataverse
    repository. Research code is typically created by a group of scientists and
    published together with academic papers to facilitate research transparency
    and reproducibility. For this study, we define ten questions to address
    aspects impacting research reproducibility and reuse. First, we retrieve and
    analyze more than 2000 replication datasets with over 9000 unique R files
    published from 2010 to 2020. Second, we execute the code in a clean runtime
    environment to assess its ease of reuse. Common coding errors were
    identified, and some of them were solved with automatic code cleaning to aid
    code execution. We find that 74% of R files failed to complete without error
    in the initial execution, while 56% failed when code cleaning was applied,
    showing that many errors can be prevented with good coding practices. We
    also analyze the replication datasets from journals’ collections and discuss
    the impact of the journal policy strictness on the code re-execution rate.
    Finally, based on our results, we propose a set of recommendations for code
    dissemination aimed at researchers, journals, and repositories.
  accessed:
    - year: 2022
      month: 12
      day: 13
  author:
    - family: Trisovic
      given: Ana
    - family: Lau
      given: Matthew K.
    - family: Pasquier
      given: Thomas
    - family: Crosas
      given: Mercè
  citation-key: trisovicLargescaleStudyResearch2022
  container-title: Scientific Data
  container-title-short: Sci Data
  DOI: 10.1038/s41597-022-01143-6
  ISSN: 2052-4463
  issue: '1'
  issued:
    - year: 2022
      month: 2
      day: 21
  language: en
  license: 2022 The Author(s)
  number: '1'
  page: '60'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: A large-scale study on research code quality and execution
  type: article-journal
  URL: https://www.nature.com/articles/s41597-022-01143-6
  volume: '9'

- id: tsangWelcomeNewERA2020
  abstract: >-
    New open-source technology lets eLife authors publish Executable Research
    Articles that treat live code and data as first-class citizens.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Tsang
      given: Emily
    - family: Maciocci
      given: Giuliano
  citation-key: tsangWelcomeNewERA2020
  container-title: eLife
  issued:
    - year: 2020
      month: 8
      day: 24
  language: en
  license: >-
    © 2020 eLife Sciences Publications Limited. This article is distributed
    under the terms of the Creative Commons Attribution License, which permits
    unrestricted use and redistribution provided that the original author and
    source are credited.
  note: 'interest: 76'
  publisher: eLife Sciences Publications Limited
  title: Welcome to a new ERA of reproducible publishing
  type: post-weblog
  URL: >-
    https://elifesciences.org/labs/dc5acbde/welcome-to-a-new-era-of-reproducible-publishing

- id: turkYtMulticodeAnalysis2010
  abstract: >-
    The analysis of complex multiphysics astrophysical simulations presents a
    unique and rapidly growing set of challenges: reproducibility,
    parallelization, and vast increases in data size and complexity chief among
    them. In order to meet these challenges, and in order to open up new avenues
    for collaboration between users of multiple simulation platforms, we present
    yt (available at http://yt.enzotools.org/) an open source,
    community-developed astrophysical analysis and visualization toolkit.
    Analysis and visualization with yt are oriented around physically relevant
    quantities rather than quantities native to astrophysical simulation codes.
    While originally designed for handling Enzo's structure adaptive mesh
    refinement data, yt has been extended to work with several different
    simulation methods and simulation codes including Orion, RAMSES, and FLASH.
    We report on its methods for reading, handling, and visualizing data,
    including projections, multivariate volume rendering, multi-dimensional
    histograms, halo finding, light cone generation, and topologically connected
    isocontour identification. Furthermore, we discuss the underlying algorithms
    yt uses for processing and visualizing data, and its mechanisms for
    parallelization of analysis tasks.
  accessed:
    - year: 2022
      month: 5
      day: 3
  author:
    - family: Turk
      given: Matthew J.
    - family: Smith
      given: Britton D.
    - family: Oishi
      given: Jeffrey S.
    - family: Skory
      given: Stephen
    - family: Skillman
      given: Samuel W.
    - family: Abel
      given: Tom
    - family: Norman
      given: Michael L.
  citation-key: turkYtMulticodeAnalysis2010
  container-title: The Astrophysical Journal Supplement Series
  container-title-short: ApJS
  DOI: 10.1088/0067-0049/192/1/9
  ISSN: 0067-0049
  issue: '1'
  issued:
    - year: 2010
      month: 12
  language: en
  page: '9'
  publisher: American Astronomical Society
  source: Institute of Physics
  title: 'yt: A Multi-code Analysis Toolkit for Astrophysical Simulation Data'
  title-short: yt
  type: article-journal
  URL: https://doi.org/10.1088/0067-0049/192/1/9
  volume: '192'

- id: turmonTestsTolerancesHighperformance2003
  abstract: >-
    We describe and test a software approach to fault detection in common
    numerical algorithms. Such result checking or algorithm-based fault
    tolerance (ABFT) methods may be used, for example, to overcome single-event
    upsets in computational hardware or to detect errors in complex,
    high-efficiency implementations of the algorithms. Following earlier work,
    we use checksum methods to validate results returned by a numerical
    subroutine operating subject to unpredictable errors in data. We consider
    common matrix and Fourier algorithms which return results satisfying a
    necessary condition having a linear form; the checksum tests compliance with
    this condition. We discuss the theory and practice of setting numerical
    tolerances to separate errors caused by a fault from those inherent in
    finite-precision floating-point calculations. We concentrate on
    comprehensively defining and evaluating tests having various
    accuracy/computational burden tradeoffs, and we emphasize average-case
    algorithm behavior rather than using worst-case upper, bounds on error.
  author:
    - family: Turmon
      given: M.
    - family: Granat
      given: R.
    - family: Katz
      given: D.S.
    - family: Lou
      given: J.Z.
  citation-key: turmonTestsTolerancesHighperformance2003
  container-title: IEEE Transactions on Computers
  DOI: 10.1109/TC.2003.1197125
  ISSN: 1557-9956
  issue: '5'
  issued:
    - year: 2003
      month: 5
  note: 'interest: 95'
  page: 579-591
  source: IEEE Xplore
  title: >-
    Tests and tolerances for high-performance software-implemehted fault
    detection
  type: article-journal
  volume: '52'

- id: uhrieAutomatedParallelKernel2020
  abstract: >-
    Modern program runtime is dominated by segments of repeating code called
    kernels. Kernels are accelerated by increasing memory locality, increasing
    data-parallelism, and exploiting producer-consumer parallelism among kernels
    - which requires hardware specialized for a particular class of kernels.
    Programming this hardware can be difficult, requiring that the kernels be
    identified and annotated in the code or translated to a domain-specific
    language. This paper describes a technique to automatically localize
    parallel kernels from a dynamic application trace, facilitating further code
    optimization. Dynamic trace collection is fast and compact. With
    optimization, it only incurs a time-dilation of a factor on nine and
    file-size of one megabyte per second, addressing a significant criticism of
    this approach. Kernel extraction is accurate and performed in linear time
    within logarithmic memory, detecting a wide range of kernels. This approach
    was validated across 16 libraries, comprised of 10,507 kernels instances. To
    validate the accuracy of our detected kernels, five test programs were
    written that spans traditional kernel definitions and were certified to
    contain all the kernels that were expected.
  accessed:
    - year: 2022
      month: 4
      day: 6
  author:
    - family: Uhrie
      given: Richard
    - family: Chakrabarti
      given: Chaitali
    - family: Brunhaver
      given: John
  citation-key: uhrieAutomatedParallelKernel2020
  container-title: arXiv:2001.09995 [cs]
  issued:
    - year: 2020
      month: 1
      day: 27
  source: arXiv.org
  title: Automated Parallel Kernel Extraction from Dynamic Application Traces
  type: article-journal
  URL: http://arxiv.org/abs/2001.09995

- id: UniversityMaastrichtSays2020
  abstract: >-
    The University of Maastricht on Wednesday disclosed that it had paid hackers
    a ransom of 30 bitcoin -- at the time worth 200,000 euros ($220,000) -- to
    unblock its computer systems, including email and computers, after an attack
    that unfolded on Dec. 24.
  accessed:
    - year: 2022
      month: 5
      day: 23
  citation-key: UniversityMaastrichtSays2020
  container-title: Reuters
  issued:
    - year: 2020
      month: 2
      day: 5
  language: en
  section: Technology News
  source: www.reuters.com
  title: University of Maastricht says it paid hackers 200,000-euro ransom
  type: article-newspaper
  URL: >-
    https://www.reuters.com/article/us-cybercrime-netherlands-university-idUSKBN1ZZ2HH

- id: UsagePythonPerformance
  accessed:
    - year: 2022
      month: 4
      day: 11
  citation-key: UsagePythonPerformance
  container-title: Python Performance Benchmark Suite 1.0.4 documentation
  title: Usage — Python Performance Benchmark Suite 1.0.4 documentation
  type: webpage
  URL: https://pyperformance.readthedocs.io/usage.html#how-to-get-stable-benchmarks

- id: UserspaceRCU
  abstract: >-
    liburcu is a LGPLv2.1 userspace RCU (read-copy-update) library. This data
    synchronization library provides read-side access which scales linearly with
    the number of cores.


    liburcu-cds provides efficient data structures based on RCU and lock-free
    algorithms. Those structures include hash tables, queues, stacks, and
    doubly-linked lists.
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: UserspaceRCU
  container-title: UserspaceRCU
  note: 'interest: 53'
  title: Userspace RCU
  type: webpage
  URL: http://liburcu.org/

- id: vahdatTransparentResultCaching1998
  abstract: >-
    The goal of this work is to develop a general framework for transparently
    managing the interactions and dependencies among input files, development
    tools, and output files. By unobtrusively monitoring the execution of
    unmodified programs, we are able to track process lineage--each process's
    parent, children, input files, and output files, and file dependency--for
    each file, the sequence of operations and the set of input files used to
    create the file. We use this information to implement Transparent Result
    Caching (TREC) and describe how TREC is used to build a number of useful
    user utilities. Unmake allows users to query TREC for file lineage
    information, including the full sequence of programs executed to create a
    particular output file. Transparent Make uses TREC to automatically generate
    dependency information by observing program execution, freeing end users
    from the need to explicitly specify dependency information (i.e., Makefiles
    can be replaced by shell scripts). Dynamic Web Object Caching allows for the
    caching of certain dynamically generated web pages, improving server
    performance and client latency.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Vahdat
      given: Amin
    - family: Anderson
      given: Thomas
  citation-key: vahdatTransparentResultCaching1998
  collection-title: ATEC '98
  container-title: Proceedings of the annual conference on USENIX Annual Technical Conference
  event-place: USA
  issued:
    - year: 1998
      month: 6
      day: 15
  language: en
  page: '3'
  publisher: USENIX Association
  publisher-place: USA
  source: ACM Digital Library
  title: Transparent result caching
  type: paper-conference

- id: valletPracticalTransparentVerifiable2022
  abstract: >-
    Reproducibility crisis urge scientists to promote transparency which allows
    peers to draw same conclusions after performing identical steps from
    hypothesis to results. Growing resources are developed to open the access to
    methods, data and source codes. Still, the computational environment, an
    interface between data and source code running analyses, is not addressed.
    Environments are usually described with software and library names
    associated with version labels or provided as an opaque container image.
    This is not enough to describe the complexity of the dependencies on which
    they rely to operate on. We describe this issue and illustrate how open
    tools like Guix can be used by any scientist to share their environment and
    allow peers to reproduce it. Some steps of research might not be fully
    reproducible, but at least, transparency for computation is technically
    addressable. These tools should be considered by scientists willing to
    promote transparency and open science.
  accessed:
    - year: 2023
      month: 5
      day: 6
  author:
    - family: Vallet
      given: Nicolas
    - family: Michonneau
      given: David
    - family: Tournier
      given: Simon
  citation-key: valletPracticalTransparentVerifiable2022
  container-title: Scientific Data
  container-title-short: Sci Data
  DOI: 10.1038/s41597-022-01720-9
  ISSN: 2052-4463
  issue: '1'
  issued:
    - year: 2022
      month: 10
      day: 4
  language: en
  license: 2022 The Author(s)
  note: 'interest: 99'
  number: '1'
  page: '597'
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    Toward practical transparent verifiable and long-term reproducible research
    using Guix
  type: article-journal
  URL: https://www.nature.com/articles/s41597-022-01720-9
  volume: '9'

- id: vanderplasWhyPythonSlow
  accessed:
    - year: 2024
      month: 1
      day: 15
  author:
    - family: VanderPlas
      given: Jake
  citation-key: vanderplasWhyPythonSlow
  container-title: Pythonic Perambulations
  title: 'Why Python is Slow: Looking Under the Hood'
  type: post-weblog
  URL: http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/

- id: vandewalleReproducibleResearchSignal2009
  accessed:
    - year: 2024
      month: 9
      day: 4
  author:
    - family: Vandewalle
      given: Patrick
    - family: Kovacevic
      given: Jelena
    - family: Vetterli
      given: Martin
  citation-key: vandewalleReproducibleResearchSignal2009
  container-title: IEEE Signal Processing Magazine
  container-title-short: IEEE Signal Process. Mag.
  DOI: 10.1109/MSP.2009.932122
  ISSN: 1053-5888, 1558-0792
  issue: '3'
  issued:
    - year: 2009
      month: 5
  license: >-
    https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html
  page: 37-47
  source: DOI.org (Crossref)
  title: Reproducible research in signal processing
  type: article-journal
  URL: https://ieeexplore.ieee.org/document/4815541/
  volume: '26'

- id: vangoorFUSENotFUSE2017
  accessed:
    - year: 2023
      month: 7
      day: 25
  author:
    - family: Vangoor
      given: Bharath Kumar Reddy
    - family: Tarasov
      given: Vasily
    - family: Zadok
      given: Erez
  citation-key: vangoorFUSENotFUSE2017
  event-title: 15th USENIX Conference on File and Storage Technologies (FAST 17)
  ISBN: 978-1-931971-36-2
  issued:
    - year: 2017
  language: en
  page: 59-72
  source: www.usenix.org
  title: 'To {FUSE} or Not to {FUSE}: Performance of {User-Space} File Systems'
  title-short: To {FUSE} or Not to {FUSE}
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor

- id: vangorpSHAREWebPortal2011
  accessed:
    - year: 2024
      month: 10
      day: 4
  author:
    - family: Van Gorp
      given: Pieter
    - family: Mazanek
      given: Steffen
  citation-key: vangorpSHAREWebPortal2011
  container-title: Procedia Computer Science
  container-title-short: Procedia Computer Science
  DOI: 10.1016/j.procs.2011.04.062
  ISSN: '18770509'
  issued:
    - year: 2011
  language: en
  license: https://www.elsevier.com/tdm/userlicense/1.0/
  page: 589-597
  source: DOI.org (Crossref)
  title: 'SHARE: a web portal for creating and sharing executable research papers'
  title-short: SHARE
  type: article-journal
  URL: https://linkinghub.elsevier.com/retrieve/pii/S1877050911001207
  volume: '4'

- id: vanleerRelationUpwindDifferencingSchemes1984
  abstract: >-
    The upwind-differencing first-order schemes of Godunov, Engquist–Osher and
    Roe are discussed on the basis of the inviscid Burgers equations. The
    differences between the schemes are interpreted as differences between the
    approximate Riemann solutions on which their numerical flux-functions are
    based. Special attention is given to the proper formulation of these schemes
    when a source term is present. Second-order two-step schemes, based on the
    numerical flux-functions of the first-order schemes are also described. The
    schemes are compared in a numerical experiment and recommendations on their
    use are included.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Leer
      given: Bram
      non-dropping-particle: van
  citation-key: vanleerRelationUpwindDifferencingSchemes1984
  container-title: SIAM Journal on Scientific and Statistical Computing
  container-title-short: SIAM J. Sci. and Stat. Comput.
  DOI: 10.1137/0905001
  ISSN: 0196-5204
  issue: '1'
  issued:
    - year: 1984
      month: 3
  page: 1-20
  publisher: Society for Industrial and Applied Mathematics
  source: epubs.siam.org (Atypon)
  title: >-
    On the Relation Between the Upwind-Differencing Schemes of Godunov,
    Engquist–Osher and Roe
  type: article-journal
  URL: https://epubs.siam.org/doi/10.1137/0905001
  volume: '5'

- id: vanleerUpwindHighResolutionMethods2012
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Leer
      given: Bram
      non-dropping-particle: van
  citation-key: vanleerUpwindHighResolutionMethods2012
  collection-title: 'Session: CFD-8: Thirty Years of CFD II'
  container-title: 16th AIAA Computational Fluid Dynamics Conference
  DOI: 10.2514/6.2003-3559
  issued:
    - year: 2012
      month: 6
      day: 25
  publisher: American Institute of Aeronautics and Astronautics
  source: American Institute of Aeronautics and Astronautics
  title: >-
    Upwind and High-Resolution Methods for Compressible Flow: From Donor Cell to
    Residual-Distribution Schemes
  title-short: Upwind and High-Resolution Methods for Compressible Flow
  type: chapter
  URL: https://arc.aiaa.org/doi/abs/10.2514/6.2003-3559

- id: vannoordenOpenaccessWebsiteGets2014
  abstract: Leading directory tightens listing criteria to weed out rogue journals.
  accessed:
    - year: 2022
      month: 8
      day: 30
  author:
    - family: Van Noorden
      given: Richard
  citation-key: vannoordenOpenaccessWebsiteGets2014
  container-title: Nature
  DOI: 10.1038/512017a
  ISSN: 1476-4687
  issue: '7512'
  issued:
    - year: 2014
      month: 8
      day: 1
  language: en
  license: 2014 Nature Publishing Group
  number: '7512'
  page: 17-17
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Open-access website gets tough
  type: article-journal
  URL: https://www.nature.com/articles/512017a
  volume: '512'

- id: vanpeltHowClearEnvironment2023
  abstract: >-
    The rm(list=ls()) command allows you to remove all variables and functions
    from the current environment in R. This can be useful if you have been
    working on a project with many variables and functions and you want to start
    the project with a blank slate.
  accessed:
    - year: 2023
      month: 5
      day: 3
  author:
    - family: Vanpelt
      given: Gary
  citation-key: vanpeltHowClearEnvironment2023
  container-title: Lxadm.com
  issued:
    - year: 2023
      month: 1
      day: 31
  language: en
  title: How to Clear the Environment in R with the rm(list=ls()) Command
  type: post-weblog
  URL: https://lxadm.com/r-rm-list-ls/

- id: vardaCurlBashInsecure2015
  abstract: >-
    Take control of your web by running your own personal cloud server with
    Sandstorm.
  accessed:
    - year: 2022
      month: 8
      day: 22
  author:
    - family: Varda
      given: Kenton
  citation-key: vardaCurlBashInsecure2015
  issued:
    - year: 2015
      month: 9
      day: 24
  language: en
  title: Is curl|bash insecure?
  type: post-weblog
  URL: >-
    https://sandstorm.io/news/2015-09-24-is-curl-bash-insecure-pgp-verified-install

- id: vasicFilelevelVsModulelevel2017
  abstract: >-
    Regression testing is used to check the correctness of evolving software.
    With the adoption of Agile development methodology, the number of tests and
    software revisions has dramatically increased, and hence has the cost of
    regression testing. Researchers proposed regression test selection (RTS)
    techniques that optimize regression testing by skipping tests that are not
    impacted by recent program changes. Ekstazi is one such state-of-the art
    technique; Ekstazi is implemented for the Java programming language and has
    been adopted by several companies and open-source projects. We report on our
    experience implementing and evaluating Ekstazi#, an Ekstazi-like tool for
    .NET. We describe the key challenges of bringing the Ekstazi idea to the
    .NET platform. We evaluate Ekstazi# on 11 open-source projects, as well as
    an internal Microsoft project substantially larger than each of the
    open-source projects. Finally, we compare Ekstazi# to an incremental build
    system (also developed at Microsoft), which, out of the box, provides
    module-level dependency tracking and skipping tasks (including test
    execution) whenever dependencies of a task do not change between the current
    and the last successful build. Ekstazi# on average reduced regression
    testing time by 43.70% for the open-source projects and by 65.26% for the
    Microsoft project (the latter is in addition to the savings provided by
    incremental builds).
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Vasic
      given: Marko
    - family: Parvez
      given: Zuhair
    - family: Milicevic
      given: Aleksandar
    - family: Gligoric
      given: Milos
  citation-key: vasicFilelevelVsModulelevel2017
  collection-title: ESEC/FSE 2017
  container-title: >-
    Proceedings of the 2017 11th Joint Meeting on Foundations of Software
    Engineering
  DOI: 10.1145/3106237.3117763
  event-place: New York, NY, USA
  ISBN: 978-1-4503-5105-8
  issued:
    - year: 2017
      month: 8
      day: 21
  page: 848–853
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: File-level vs. module-level regression test selection for .NET
  type: paper-conference
  URL: https://doi.org/10.1145/3106237.3117763

- id: venkatagiriApproxilyzerSystematicFramework2016
  abstract: >-
    Approximate computing environments trade off computational accuracy for
    improvements in performance, energy, and resiliency cost. For widespread
    adoption of approximate computing, a fundamental requirement is to
    understand how perturbations to a computation affect the outcome of the
    execution in terms of its output quality. This paper presents a framework
    for approximate computing, called Approxilyzer, that quantifies the quality
    impact of a single-bit error in all dynamic instructions of an execution
    with high accuracy (95% on average). We demonstrate two uses of
    Approxilyzer. First, we show how Approxilyzer can be used to quantitatively
    tune output quality vs. resiliency vs. overhead to enable ultra-low cost
    resiliency solutions (with a single bit error model). For example, we show
    that Approxilyzer determines that a very small loss in output quality (1%)
    can yield large resiliency overhead reduction (up to 55%) for 99% resiliency
    coverage. Second, we show how Approxilyzer can be used to provide a
    first-order estimate of the approximation potential of general-purpose
    programs. It does so in an automated way while requiring minimal user input
    and no program modifications. This enables programmers or other tools to
    focus on the promising subset of approximable instructions for further
    analysis.
  author:
    - family: Venkatagiri
      given: Radha
    - family: Mahmoud
      given: Abdulrahman
    - family: Hari
      given: Siva Kumar Sastry
    - family: Adve
      given: Sarita V.
  citation-key: venkatagiriApproxilyzerSystematicFramework2016
  container-title: >-
    2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)
  DOI: 10.1109/MICRO.2016.7783745
  event-title: >-
    2016 49th Annual IEEE/ACM International Symposium on Microarchitecture
    (MICRO)
  issued:
    - year: 2016
      month: 10
  page: 1-14
  source: IEEE Xplore
  title: >-
    Approxilyzer: Towards a systematic framework for instruction-level
    approximate computing and its application to hardware resiliency
  title-short: Approxilyzer
  type: paper-conference

- id: venkateshUserAcceptanceInformation2003
  abstract: >-
    Information technology (IT) acceptance research has yielded many competing
    models, each with different sets of acceptance determinants. In this paper,
    we (1) review user acceptance literature and discuss eight prominent models,
    (2) empirically compare the eight models and their extensions, (3) formulate
    a unified model that integrates elements across the eight models, and (4)
    empirically validate the unified model. The eight models reviewed are the
    theory of reasoned action, the technology acceptance model, the motivational
    model, the theory of planned behavior, a model combining the technology
    acceptance model and the theory of planned behavior, the model of PC
    utilization, the innovation diffusion theory, and the social cognitive
    theory. Using data from four organizations over a six-month period with
    three points of measurement, the eight models explained between 17 percent
    and 53 percent of the variance in user intentions to use information
    technology. Next, a unified model, called the Unified Theory of Acceptance
    and Use of Technology (UTAUT), was formulated, with four core determinants
    of intention and usage, and up to four moderators of key relationships.
    UTAUT was then tested using the original data and found to outperform the
    eight individual models (adjusted R of 69 percent). UTAUT was then confirmed
    with data from two new organizations with similar results (adjusted R of 70
    percent). UTAUT thus provides a useful tool for managers needing to assess
    the likelihood of success for new technology introductions and helps them
    understand the drivers of acceptance in order to proactively design
    interventions (including training, marketing, etc.) targeted at populations
    of users that may be less inclined to adopt and use new systems. The paper
    also makes several recommendations for future research including developing
    a deeper understanding of the dynamic influences studied here, refining
    measurement of the core constructs used in UTAUT, and understanding the
    organizational outcomes associated with new technology use.
  accessed:
    - year: 2022
      month: 5
      day: 27
  author:
    - family: Venkatesh
      given: Viswanath
    - family: Morris
      given: Michael G.
    - family: Davis
      given: Gordon B.
    - family: Davis
      given: Fred D.
  citation-key: venkateshUserAcceptanceInformation2003
  container-title: MIS Quarterly
  container-title-short: MIS Quarterly
  DOI: 10.2307/30036540
  ISSN: '02767783'
  issue: '3'
  issued:
    - year: 2003
  page: '425'
  source: DOI.org (Crossref)
  title: 'User Acceptance of Information Technology: Toward a Unified View'
  title-short: User Acceptance of Information Technology
  type: article-journal
  URL: https://www.jstor.org/stable/10.2307/30036540
  volume: '27'

- id: victorLearnableProgramming
  abstract: Bret Victor has been provided by the management for your protection.
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Victor
      given: Bret
  citation-key: victorLearnableProgramming
  title: Learnable Programming
  type: post-weblog
  URL: http://worrydream.com/#!/LearnableProgramming

- id: vilaImpactHardwareVariability2024
  accessed:
    - year: 2024
      month: 10
      day: 5
  author:
    - family: Vila
      given: Gael
    - family: Medernach
      given: Emmanuel
    - family: Gonzalez Pepe
      given: Ines
    - family: Bonnet
      given: Axel
    - family: Chatelain
      given: Yohan
    - family: Sdika
      given: Michael
    - family: Glatard
      given: Tristan
    - family: Camarasu Pop
      given: Sorina
  citation-key: vilaImpactHardwareVariability2024
  container-title: Proceedings of the 2nd ACM Conference on Reproducibility and Replicability
  DOI: 10.1145/3641525.3663626
  event-place: Rennes France
  event-title: 'ACM REP ''24: ACM Conference on Reproducibility and Replicability'
  ISBN: '9798400705304'
  issued:
    - year: 2024
      month: 6
      day: 18
  language: en
  page: 75-84
  publisher: ACM
  publisher-place: Rennes France
  source: DOI.org (Crossref)
  title: >-
    The Impact of Hardware Variability on Applications Packaged with Docker and
    Guix: a Case Study in Neuroimaging
  title-short: >-
    The Impact of Hardware Variability on Applications Packaged with Docker and
    Guix
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3641525.3663626

- id: villanuevaNoEvidencePhosphine2021
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Villanueva
      given: G. L.
    - family: Cordiner
      given: M.
    - family: Irwin
      given: P. G. J.
    - family: Pater
      given: I.
      non-dropping-particle: de
    - family: Butler
      given: B.
    - family: Gurwell
      given: M.
    - family: Milam
      given: S. N.
    - family: Nixon
      given: C. A.
    - family: Luszcz-Cook
      given: S. H.
    - family: Wilson
      given: C. F.
    - family: Kofman
      given: V.
    - family: Liuzzi
      given: G.
    - family: Faggi
      given: S.
    - family: Fauchez
      given: T. J.
    - family: Lippi
      given: M.
    - family: Cosentino
      given: R.
    - family: Thelen
      given: A. E.
    - family: Moullet
      given: A.
    - family: Hartogh
      given: P.
    - family: Molter
      given: E. M.
    - family: Charnley
      given: S.
    - family: Arney
      given: G. N.
    - family: Mandell
      given: A. M.
    - family: Biver
      given: N.
    - family: Vandaele
      given: A. C.
    - family: Kleer
      given: K. R.
      non-dropping-particle: de
    - family: Kopparapu
      given: R.
  citation-key: villanuevaNoEvidencePhosphine2021
  container-title: Nature Astronomy
  container-title-short: Nat Astron
  DOI: 10.1038/s41550-021-01422-z
  ISSN: 2397-3366
  issue: '7'
  issued:
    - year: 2021
      month: 7
  language: en
  license: >-
    2021 This is a U.S. government work and not under copyright protection in
    the U.S.; foreign copyright protection may apply
  number: '7'
  page: 631-635
  publisher: Nature Publishing Group
  source: www.nature.com
  title: >-
    No evidence of phosphine in the atmosphere of Venus from independent
    analyses
  type: article-journal
  URL: https://www.nature.com/articles/s41550-021-01422-z
  volume: '5'

- id: vitekR3RepeatabilityReproducibility2012
  abstract: >-
    Computer systems research spans sub-disciplines that include embedded
    systems, programming languages and compilers, networking, and operating
    systems. Our contention is that a number of structural factors inhibit
    quality systems research. We highlight some of the factors we have
    encountered in our own work and observed in published papers and propose
    solutions that could both increase the productivity of researchers and the
    quality of their output.
  accessed:
    - year: 2022
      month: 6
      day: 30
  author:
    - family: Vitek
      given: Jan
    - family: Kalibera
      given: Tomas
  citation-key: vitekR3RepeatabilityReproducibility2012
  container-title: ACM SIGPLAN Notices
  container-title-short: SIGPLAN Not.
  DOI: 10.1145/2442776.2442781
  ISSN: 0362-1340, 1558-1160
  issue: 4a
  issued:
    - year: 2012
      month: 6
      day: 18
  language: en
  note: 'interest: 76'
  page: 30-36
  source: DOI.org (Crossref)
  title: 'R3: repeatability, reproducibility and rigor'
  title-short: R3
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/2442776.2442781
  volume: '47'

- id: vogelsbergerIntroducingIllustrisProject2014
  abstract: "We introduce the Illustris Project, a series of large-scale hydrodynamical simulations of galaxy formation. The highest resolution simulation, Illustris-1, covers a volume of (106.5舁Mpc)3, has a dark mass resolution of 6.26 × 106舁M⊙, and an initial baryonic matter mass resolution of 1.26 × 106舁M⊙. At z\_=\_0 gravitational forces are softened on scales of 710舁pc, and the smallest hydrodynamical gas cells have an extent of 48舁pc. We follow the dynamical evolution of 2\_×\_18203 resolution elements and in addition passively evolve 18203 Monte Carlo tracer particles reaching a total particle count of more than 18 billion. The galaxy formation model includes: primordial and metal-line cooling with self-shielding corrections, stellar evolution, stellar feedback, gas recycling, chemical enrichment, supermassive black hole growth, and feedback from active galactic nuclei. Here we describe the simulation suite, and contrast basic predictions of our model for the present-day galaxy population with observations of the local universe. At z\_=\_0 our simulation volume contains about 40舁000 well-resolved galaxies covering a diverse range of morphologies and colours including early-type, late-type and irregular galaxies. The simulation reproduces reasonably well the cosmic star formation rate density, the galaxy luminosity function, and baryon conversion efficiency at z\_=\_0. It also qualitatively captures the impact of galaxy environment on the red fractions of galaxies. The internal velocity structure of selected well-resolved disc galaxies obeys the stellar and baryonic Tully–Fisher relation together with flat circular velocity curves. In the well-resolved regime, the simulation reproduces the observed mix of early-type and late-type galaxies. Our model predicts a halo mass dependent impact of baryonic effects on the halo mass function and the masses of haloes caused by feedback from supernova and active galactic nuclei."
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Vogelsberger
      given: Mark
    - family: Genel
      given: Shy
    - family: Springel
      given: Volker
    - family: Torrey
      given: Paul
    - family: Sijacki
      given: Debora
    - family: Xu
      given: Dandan
    - family: Snyder
      given: Greg
    - family: Nelson
      given: Dylan
    - family: Hernquist
      given: Lars
  citation-key: vogelsbergerIntroducingIllustrisProject2014
  container-title: Monthly Notices of the Royal Astronomical Society
  container-title-short: Monthly Notices of the Royal Astronomical Society
  DOI: 10.1093/mnras/stu1536
  ISSN: 0035-8711
  issue: '2'
  issued:
    - year: 2014
      month: 10
      day: 21
  page: 1518-1547
  source: Silverchair
  title: >-
    Introducing the Illustris Project: simulating the coevolution of dark and
    visible matter in the Universe
  title-short: Introducing the Illustris Project
  type: article-journal
  URL: https://doi.org/10.1093/mnras/stu1536
  volume: '444'

- id: vogelsbergerPropertiesGalaxiesReproduced2014
  abstract: >-
    Previous simulations of the growth of cosmic structures have broadly
    reproduced the ‘cosmic web’ of galaxies that we see in the Universe, but
    failed to create a mixed population of elliptical and spiral galaxies,
    because of numerical inaccuracies and incomplete physical models. Moreover,
    they were unable to track the small-scale evolution of gas and stars to the
    present epoch within a representative portion of the Universe. Here we
    report a simulation that starts 12 million years after the Big Bang, and
    traces 13 billion years of cosmic evolution with 12 billion resolution
    elements in a cube of 106.5 megaparsecs a side. It yields a reasonable
    population of ellipticals and spirals, reproduces the observed distribution
    of galaxies in clusters and characteristics of hydrogen on large scales, and
    at the same time matches the ‘metal’ and hydrogen content of galaxies on
    small scales.
  accessed:
    - year: 2022
      month: 4
      day: 11
  author:
    - family: Vogelsberger
      given: M.
    - family: Genel
      given: S.
    - family: Springel
      given: V.
    - family: Torrey
      given: P.
    - family: Sijacki
      given: D.
    - family: Xu
      given: D.
    - family: Snyder
      given: G.
    - family: Bird
      given: S.
    - family: Nelson
      given: D.
    - family: Hernquist
      given: L.
  citation-key: vogelsbergerPropertiesGalaxiesReproduced2014
  container-title: Nature
  DOI: 10.1038/nature13316
  ISSN: 1476-4687
  issue: '7499'
  issued:
    - year: 2014
      month: 5
  language: en
  license: >-
    2014 Nature Publishing Group, a division of Macmillan Publishers Limited.
    All Rights Reserved.
  number: '7499'
  page: 177-182
  publisher: Nature Publishing Group
  source: www.nature.com
  title: Properties of galaxies reproduced by a hydrodynamic simulation
  type: article-journal
  URL: https://www.nature.com/articles/nature13316
  volume: '509'

- id: vuOutcomePreservingInputReduction2023
  abstract: >-
    Analysis of data is the foundation of multiple scientific disciplines,
    manifesting in complex and diverse scientific data analysis workflows often
    involving exploratory analyses. Such analyses represent a particular case
    for traditional data engineering workflows, as results may be hard to
    interpret and judge whether they are correct or not, and where
    experimentation is a central theme. Oftentimes, there are certain aspects of
    a result which are suspicious and which should be further investigated to
    increase the trustworthiness of the workflow’s outcome. To this end, we
    advocate a semi-automated approach to reducing a workflow’s input data while
    preserving a specified outcome of interest, facilitating irregularity
    localization by narrowing down the search space for spotting corrupted input
    data or wrong assumptions made about it. We outline our vision on building
    engineering support for outcome-preserving input reduction within data
    analysis workflows, and report on preliminary results obtained from applying
    an early research prototype on a computational notebook taken from an online
    community of data scientists and machine learning practitioners.
  accessed:
    - year: 2023
      month: 7
      day: 19
  author:
    - family: Vu
      given: Anh Duc
    - family: Kehrer
      given: Timo
    - family: Tsigkanos
      given: Christos
  citation-key: vuOutcomePreservingInputReduction2023
  collection-title: ASE '22
  container-title: >-
    Proceedings of the 37th IEEE/ACM International Conference on Automated
    Software Engineering
  DOI: 10.1145/3551349.3559558
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9475-8
  issued:
    - year: 2023
      month: 1
      day: 5
  page: 1–5
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Outcome-Preserving Input Reduction for Scientific Data Analysis Workflows
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3551349.3559558

- id: vuUsingSourceCode2020
  accessed:
    - year: 2022
      month: 8
      day: 10
  author:
    - family: Vu
      given: Duc Ly
    - family: Pashchenko
      given: Ivan
    - family: Massacci
      given: Fabio
    - family: Plate
      given: Henrik
    - family: Sabetta
      given: Antonino
  citation-key: vuUsingSourceCode2020
  container-title: >-
    Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications
    Security
  DOI: 10.1145/3372297.3420015
  event-place: Virtual Event
  event-title: 'CCS ''20: 2020 ACM SIGSAC Conference on Computer and Communications Security'
  ISBN: 978-1-4503-7089-9
  issued:
    - year: 2020
      month: 10
      day: 30
  language: en
  page: 2093-2095
  publisher: ACM
  publisher-place: Virtual Event
  source: DOI.org (Crossref)
  title: >-
    Towards Using Source Code Repositories to Identify Software Supply Chain
    Attacks
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3372297.3420015

- id: walshAreDockerContainers2014
  abstract: >-
    This article is based on a talk I gave at DockerCon this year. It will
    discuss Docker container security, where we are currently, and where we are
    headed. This is part of a series on Docker security, read part two.
  accessed:
    - year: 2024
      month: 4
      day: 15
  author:
    - family: Walsh
      given: Daniel J.
  citation-key: walshAreDockerContainers2014
  container-title: Opensource.com
  issued:
    - year: 2014
      month: 7
      day: 22
  language: en
  title: Are Docker containers really secure?
  title-short: Are Docker containers really secure?
  type: post-weblog
  URL: https://opensource.com/business/14/7/docker-security-selinux

- id: walshBringingNewSecurity2014
  abstract: >-
    In my previous on article on Docker Security, I wrote that containers do not
    contain. In this second part, I cover the security features that have been
    added to Docker to attempt to control processes within a container.
  accessed:
    - year: 2024
      month: 4
      day: 15
  author:
    - family: Walsh
      given: Daniel J.
  citation-key: walshBringingNewSecurity2014
  container-title: Opensource.com
  issued:
    - year: 2014
      month: 9
      day: 3
  language: en
  title: Bringing new security features to Docker
  type: post-weblog
  URL: https://opensource.com/business/14/9/security-for-docker

- id: wangAssessingRestoringReproducibility2021
  abstract: >-
    Jupyter notebooks---documents that contain live code, equations,
    visualizations, and narrative text---now are among the most popular means to
    compute, present, discuss and disseminate scientific findings. In principle,
    Jupyter notebooks should easily allow to reproduce and extend scientific
    computations and their findings; but in practice, this is not the case. The
    individual code cells in Jupyter notebooks can be executed in any order,
    with identifier usages preceding their definitions and results preceding
    their computations. In a sample of 936 published notebooks that would be
    executable in principle, we found that 73% of them would not be reproducible
    with straightforward approaches, requiring humans to infer (and often guess)
    the order in which the authors created the cells. In this paper, we present
    an approach to (1) automatically satisfy dependencies between code cells to
    reconstruct possible execution orders of the cells; and (2) instrument code
    cells to mitigate the impact of non-reproducible statements (i.e., random
    functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as
    input and outputs the possible execution schemes that reproduce the exact
    notebook results. In our sample, Osiris was able to reconstruct such schemes
    for 82.23% of all executable notebooks, which has more than three times
    better than the state-of-the-art; the resulting reordered code is valid
    program code and thus available for further testing and analysis.
  accessed:
    - year: 2022
      month: 11
      day: 14
  author:
    - family: Wang
      given: Jiawei
    - family: Kuo
      given: Tzu-yang
    - family: Li
      given: Li
    - family: Zeller
      given: Andreas
  citation-key: wangAssessingRestoringReproducibility2021
  collection-title: ASE '20
  container-title: >-
    Proceedings of the 35th IEEE/ACM International Conference on Automated
    Software Engineering
  DOI: 10.1145/3324884.3416585
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6768-4
  issued:
    - year: 2021
      month: 1
      day: 27
  page: 138–149
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: Assessing and restoring reproducibility of Jupyter notebooks
  type: paper-conference
  URL: https://doi.org/10.1145/3324884.3416585

- id: wangLprovPracticalLibraryaware2018
  abstract: >-
    With the continuing evolution of sophisticated APT attacks, provenance
    tracking is becoming an important technique for efficient attack
    investigation in enterprise networks. Most of existing provenance techniques
    are operating on system event auditing that discloses dependence
    relationships by scrutinizing syscall traces. Unfortunately, such
    auditing-based provenance is not able to track the causality of another
    important dimension in provenance, the shared libraries. Different from
    other data-only system entities like files and sockets, dynamic libraries
    are linked at runtime and may get executed, which poses new challenges in
    provenance tracking. For example, library provenance cannot be tracked by
    syscalls and mapping; whether a library function is called and how it is
    called within an execution context is invisible at syscall level; linking a
    library does not promise their execution at runtime. Addressing these
    challenges is critical to tracking sophisticated attacks leveraging
    libraries. In this paper, to facilitate fine-grained investigation inside
    the execution of library binaries, we develop Lprov, a novel provenance
    tracking system which combines library tracing and syscall tracing. Upon a
    syscall, Lprov identifies the library calls together with the stack which
    induces it so that the library execution provenance can be accurately
    revealed. Our evaluation shows that Lprov can precisely identify attack
    provenance involving libraries, including malicious library attack and
    library vulnerability exploitation, while syscall-based provenance tools
    fail to identify. It only incurs 7.0% (in geometric mean) runtime overhead
    and consumes 3 times less storage space of a state-of-the-art provenance
    tool.
  accessed:
    - year: 2023
      month: 8
      day: 24
  author:
    - family: Wang
      given: Fei
    - family: Kwon
      given: Yonghwi
    - family: Ma
      given: Shiqing
    - family: Zhang
      given: Xiangyu
    - family: Xu
      given: Dongyan
  citation-key: wangLprovPracticalLibraryaware2018
  collection-title: ACSAC '18
  container-title: Proceedings of the 34th Annual Computer Security Applications Conference
  DOI: 10.1145/3274694.3274751
  event-place: New York, NY, USA
  ISBN: 978-1-4503-6569-7
  issued:
    - year: 2018
      month: 12
      day: 3
  page: 605–617
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: 'Lprov: Practical Library-aware Provenance Tracing'
  title-short: Lprov
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/3274694.3274751

- id: wangRetractedPredictionPosttranslational2012
  abstract: >-
    The following article from the Journal of Computational Chemistry,
    “Prediction of Posttranslational Modification Sites from Sequences with
    Kernel Methods,” by Xiaobo Wang, Yongcui Wang, Yingjie Tian, Xiaojian Shao,
    Ling-Yun Wu, and Naiyang Deng, published online on 21 April 2010 in Wiley
    Online Library (wileyonlinelibrary.com), DOI: 10.1002.jcc.21526, has been
    retracted by agreement between the authors, the journal's editors, and Wiley
    Periodicals, Inc. The retraction has been agreed because a computational
    error produced results that led the authors to overstate the level of
    performance of their computing model.
  accessed:
    - year: 2023
      month: 1
      day: 19
  author:
    - family: Wang
      given: Xiaobo
    - family: Wang
      given: Yongcui
    - family: Tiang
      given: Yingjie
    - family: Shao
      given: Xiaojian
    - family: Wu
      given: Ling-Yun
    - family: Deng
      given: Naiyang
  citation-key: wangRetractedPredictionPosttranslational2012
  container-title: Journal of Computational Chemistry
  DOI: 10.1002/jcc.21526
  ISSN: 1096-987X
  issue: '17'
  issued:
    - year: 2012
  language: en
  page: 1524-1524
  source: Wiley Online Library
  title: >-
    Retracted: Prediction of posttranslational modification sites from sequences
    with kernel methods
  title-short: Retracted
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/jcc.21526
  volume: '33'

- id: wardjr.HierarchicalGroupingOptimize1963
  abstract: >-
    A procedure for forming hierarchical groups of mutually exclusive subsets,
    each of which has members that are maximally similar with respect to
    specified characteristics, is suggested for use in large-scale (n > 100)
    studies when a precise optimal solution for a specified number of groups is
    not practical. Given n sets, this procedure permits their reduction to n − 1
    mutually exclusive sets by considering the union of all possible n(n − 1)/2
    pairs and selecting a union having a maximal value for the functional
    relation, or objective function, that reflects the criterion chosen by the
    investigator. By repeating this process until only one group remains, the
    complete hierarchical structure and a quantitative estimate of the loss
    associated with each stage in the grouping can be obtained. A general
    flowchart helpful in computer programming and a numerical example are
    included.
  accessed:
    - year: 2024
      month: 2
      day: 10
  author:
    - family: Ward Jr.
      given: Joe H.
  citation-key: wardjr.HierarchicalGroupingOptimize1963
  container-title: Journal of the American Statistical Association
  DOI: 10.1080/01621459.1963.10500845
  ISSN: 0162-1459
  issue: '301'
  issued:
    - year: 1963
      month: 3
      day: 1
  page: 236-244
  publisher: Taylor & Francis
  source: Taylor and Francis+NEJM
  title: Hierarchical Grouping to Optimize an Objective Function
  type: article-journal
  URL: https://www.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845
  volume: '58'

- id: washingtonPolitiFactViceWhat
  abstract: >-
    Editor's note: Have you ever wondered if the movie you just saw &mdash; that
    claimed to be based on a real story or
  accessed:
    - year: 2022
      month: 8
      day: 29
  author:
    - family: Washington
      given: District of Columbia 1800 I. Street NW
    - family: Dc 20006
      given: ''
  citation-key: washingtonPolitiFactViceWhat
  container-title: '@politifact'
  language: en-US
  title: 'PolitiFact - Vice: What the movie gets right and wrong about Dick Cheney'
  title-short: PolitiFact - Vice
  type: webpage
  URL: >-
    https://www.politifact.com/article/2019/feb/08/vice-what-movie-gets-right-and-wrong/

- id: weibelDublinCoreMetadata2000
  accessed:
    - year: 2023
      month: 5
      day: 25
  author:
    - family: Weibel
      given: Stuart L.
    - family: Koch
      given: Traugott
  citation-key: weibelDublinCoreMetadata2000
  container-title: D-Lib Magazine
  container-title-short: D-Lib Magazine
  DOI: 10.1045/december2000-weibel
  ISSN: 1082-9873
  issue: '12'
  issued:
    - year: 2000
      month: 12
  language: en
  source: DOI.org (Crossref)
  title: >-
    The Dublin Core Metadata Initiative: Mission, Current Activities, and Future
    Directions
  title-short: The Dublin Core Metadata Initiative
  type: article-journal
  URL: http://www.dlib.org/dlib/december00/weibel/12weibel.html
  volume: '6'

- id: wheelerSecureProgrammingHOWTO2015
  abstract: >-
    This book provides a set of design and implementation guidelines for writing
    secure programs. Such programs include application programs used as viewers
    of remote data, web applications (including CGI scripts), network servers,
    and setuid/setgid programs. Specific guidelines for C, C++, Java, Perl, PHP,
    Python, Tcl, and Ada95 are included. It especially covers Linux and Unix
    based systems, but much of its material applies to any system.
  author:
    - family: Wheeler
      given: David A.
  citation-key: wheelerSecureProgrammingHOWTO2015
  issued:
    - year: 2015
      month: 9
      day: 19
  title: Secure Programming HOWTO - Creating Secure Software
  type: webpage
  URL: https://dwheeler.com/secure-programs/

- id: widyasariBugsInPyDatabaseExisting2020
  abstract: >-
    The 2019 edition of Stack Overflow developer survey highlights that, for the
    first time, Python outperformed Java in terms of popularity. The gap between
    Python and Java further widened in the 2020 edition of the survey.
    Unfortunately, despite the rapid increase in Python's popularity, there are
    not many testing and debugging tools that are designed for Python. This is
    in stark contrast with the abundance of testing and debugging tools for
    Java. Thus, there is a need to push research on tools that can help Python
    developers. One factor that contributed to the rapid growth of Java testing
    and debugging tools is the availability of benchmarks. A popular benchmark
    is the Defects4J benchmark; its initial version contained 357 real bugs from
    5 real-world Java programs. Each bug comes with a test suite that can expose
    the bug. Defects4J has been used by hundreds of testing and debugging
    studies and has helped to push the frontier of research in these directions.
    In this project, inspired by Defects4J, we create another benchmark database
    and tool that contain 493 real bugs from 17 real-world Python programs. We
    hope our benchmark can help catalyze future work on testing and debugging
    tools that work on Python programs.
  accessed:
    - year: 2023
      month: 7
      day: 8
  author:
    - family: Widyasari
      given: Ratnadira
    - family: Sim
      given: Sheng Qin
    - family: Lok
      given: Camellia
    - family: Qi
      given: Haodi
    - family: Phan
      given: Jack
    - family: Tay
      given: Qijin
    - family: Tan
      given: Constance
    - family: Wee
      given: Fiona
    - family: Tan
      given: Jodie Ethelda
    - family: Yieh
      given: Yuheng
    - family: Goh
      given: Brian
    - family: Thung
      given: Ferdian
    - family: Kang
      given: Hong Jin
    - family: Hoang
      given: Thong
    - family: Lo
      given: David
    - family: Ouh
      given: Eng Lieh
  citation-key: widyasariBugsInPyDatabaseExisting2020
  collection-title: ESEC/FSE 2020
  container-title: >-
    Proceedings of the 28th ACM Joint Meeting on European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3368089.3417943
  event-place: New York, NY, USA
  ISBN: 978-1-4503-7043-1
  issued:
    - year: 2020
      month: 11
      day: 8
  page: 1556–1560
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    BugsInPy: a database of existing bugs in Python programs to enable
    controlled testing and debugging studies
  title-short: BugsInPy
  type: paper-conference
  URL: https://doi.org/10.1145/3368089.3417943

- id: wieseNamingPainDeveloping2020
  abstract: >-
    The scientific software community's lack of computer science background and
    software engineering training takes a toll on scientists who need to develop
    software. We built a taxonomy of 2,110 reported problems and grouped them
    into three major axes: technical-related, socialrelated, and
    scientific-related problems.
  author:
    - family: Wiese
      given: Igor
    - family: Polato
      given: Ivanilton
    - family: Pinto
      given: Gustavo
  citation-key: wieseNamingPainDeveloping2020
  container-title: IEEE Software
  DOI: 10.1109/MS.2019.2899838
  ISSN: 1937-4194
  issue: '4'
  issued:
    - year: 2020
      month: 7
  note: 'interest: 86'
  page: 75-82
  source: IEEE Xplore
  title: Naming the Pain in Developing Scientific Software
  type: article-journal
  volume: '37'

- id: wilder-jamesDescriptionProjectWiki2017
  abstract: >-
    DOAP is a project to create an XML/RDF vocabulary to describe software
    projects, and in particular open source projects.


    In addition to developing an RDF schema and examples, the DOAP project aims
    to provide tool support in all the popular programming languages.
  accessed:
    - year: 2023
      month: 5
      day: 26
  author:
    - family: Wilder-James
      given: Edd
  citation-key: wilder-jamesDescriptionProjectWiki2017
  issued:
    - year: 2017
      month: 1
      day: 13
  title: Description of a Project wiki
  type: webpage
  URL: https://github.com/ewilderj/doap/wiki

- id: wilkinsonWorkflowsWhenParts2022
  abstract: >-
    The FAIR principles for scientific data (Findable, Accessible,
    Interoperable, Reusable) are also relevant to other digital objects such as
    research software and scientific workflows that operate on scientific data.
    The FAIR principles can be applied to the data being handled by a scientific
    workflow as well as the processes, software, and other infrastructure which
    are necessary to specify and execute a workflow. The FAIR principles were
    designed as guidelines, rather than rules, that would allow for differences
    in standards for different communities and for different degrees of
    compliance. There are many practical considerations which impact the level
    of FAIR-ness that can actually be achieved, including policies, traditions,
    and technologies. Because of these considerations, obstacles are often
    encountered during the workflow lifecycle that trace directly to
    shortcomings in the implementation of the FAIR principles. Here, we detail
    some cases, without naming names, in which data and workflows were Findable
    but otherwise lacking in areas commonly needed and expected by modern FAIR
    methods, tools, and users. We describe how some of these problems, all of
    which were overcome successfully, have motivated us to push on systems and
    approaches for fully FAIR workflows.
  accessed:
    - year: 2022
      month: 12
      day: 18
  author:
    - family: Wilkinson
      given: Sean R.
    - family: Eisenhauer
      given: Greg
    - family: Kapadia
      given: Anuj J.
    - family: Knight
      given: Kathryn
    - family: Logan
      given: Jeremy
    - family: Widener
      given: Patrick
    - family: Wolf
      given: Matthew
  citation-key: wilkinsonWorkflowsWhenParts2022
  container-title: 2022 IEEE 18th International Conference on e-Science (e-Science)
  DOI: 10.1109/eScience55777.2022.00090
  issued:
    - year: 2022
      month: 10
  note: 'interest: 95'
  page: 507-512
  source: arXiv.org
  title: 'F*** workflows: when parts of FAIR are missing'
  title-short: F*** workflows
  type: paper-conference
  URL: http://arxiv.org/abs/2209.09022

- id: wilsonArchitectureOpenSource
  citation-key: wilsonArchitectureOpenSource
  editor:
    - family: Wilson
      given: Greg
    - family: Brown
      given: Amy
  language: English
  number-of-pages: '392'
  source: Amazon
  title: >-
    The Architecture of Open Source Applications, Volume II: Structure, Scale,
    and a Few More Fearless Hacks
  title-short: The Architecture of Open Source Applications, Volume II
  type: book
  URL: http://aosabook.org/en/index.html

- id: wilsonSoftwareCarpentryGetting2006
  abstract: >-
    For the past years, my colleagues and I have developed a one-semester course
    that teaches scientists and engineers the "common core" of modern software
    development. Our experience shows that an investment of 150 hours-25 of
    lectures and the rest of practical work-can improve productivity by roughly
    20 percent. That's one day a week, one less semester in a master's degree,
    or one less year for a typical PhD. The course is called software carpentry,
    rather than software engineering, to emphasize the fact that it focuses on
    small-scale and immediately practical issues. All of the material is freely
    available under an open-source license at www.swc.scipy.org and can be used
    both for self-study and in the classroom. This article describes what the
    course contains, and why.
  accessed:
    - year: 2022
      month: 6
      day: 1
  author:
    - family: Wilson
      given: G.
  citation-key: wilsonSoftwareCarpentryGetting2006
  container-title: Computing in Science & Engineering
  container-title-short: Comput. Sci. Eng.
  DOI: 10.1109/MCSE.2006.122
  ISSN: 1521-9615
  issue: '6'
  issued:
    - year: 2006
      month: 11
  page: 66-69
  source: DOI.org (Crossref)
  title: >-
    Software Carpentry: Getting Scientists to Write Better Code by Making Them
    More Productive
  title-short: Software Carpentry
  type: article-journal
  URL: http://ieeexplore.ieee.org/document/1717319/
  volume: '8'

- id: wilsonSoftwareCarpentryLessons2016
  abstract: "Since its start in 1998, Software Carpentry has evolved from a\_week-long training course at the US national laboratories into a\_worldwide volunteer effort to improve researchers' computing\_skills. This paper explains what we have learned along the way, the\_challenges we now face, and our plans for the future."
  accessed:
    - year: 2024
      month: 2
      day: 23
  author:
    - family: Wilson
      given: Greg
  citation-key: wilsonSoftwareCarpentryLessons2016
  container-title: F1000Research
  container-title-short: F1000Res
  DOI: 10.12688/f1000research.3-62.v2
  ISSN: 2046-1402
  issued:
    - year: 2016
      month: 1
      day: 28
  page: '62'
  PMCID: PMC3976103
  PMID: '24715981'
  source: PubMed Central
  title: 'Software Carpentry: lessons learned'
  title-short: Software Carpentry
  type: article-journal
  URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3976103/
  volume: '3'

- id: wilsonTestingDistributedSystems2014
  abstract: >-
    Debugging highly concurrent distributed systems in a noisy network
    environment is an exceptionally challenging endeavor. On the one hand,
    evaluating all possible orders in which program events can occur is a task
    ill-suited to human cognition, rendering a pure analytic understanding of
    the control flow of such a system beyond the reach of any individual
    programmer. On the other hand, a more “empirical” approach to the task is
    also fraught with difficulty, as the dependence of severe bugs on precise
    timings or transient network conditions makes every part of the debugging
    cycle – from bug replication to verification of a fix – a Sisyphean labor
    bordering on the impossible.


    One approach which has been developed to ameliorate this situation is that
    of deterministic simulation, wherein the hardware components of the system –
    including hard disks, network links, and the machines themselves – are
    replaced in testing with software which fulfills the contracts of those
    systems, but whose state is completely transparent to the developer. This
    enables the simulation of a wide diversity of failure modes including
    network failures, disk failures or space exhaustion, unexpected machine
    shutdown or reboot, IP address changes, and even entire datacenter failures.
    Moreover, once a particular pattern of failures has been identified which
    uncovers a bug, the determinism property of the simulation means that the
    exact same series of events can be replayed an indefinite number of times,
    greatly facilitating the debugging process, and providing confidence when a
    bug has been fixed.


    Attendees of this talk will gain an understanding of the benefits,
    drawbacks, and tradeoffs involved in implementing a deterministic simulation
    framework, with frequent reference both to theory and to real-world
    engineering experience gleaned from applying this method to a complex
    distributed system. Attendees will also learn about language features which
    aid in the development of such a framework.


    Will Wilson

    FoundationDB


    Will Wilson works on the engineering team at FoundationDB
    (https://foundationdb.com). Will started his career in biotechnology,
    leading a successful R&D effort in spinal cord injury diagnostics, currently
    undergoing commercialization by a company he co-founded. Since then, Will
    has worked in a variety of technical and business roles at data science and
    data virtualization startups. Will has a degree in math and philosophy from
    Yale.
  accessed:
    - year: 2024
      month: 2
      day: 22
  author:
    - family: Wilson
      given: Will
  citation-key: wilsonTestingDistributedSystems2014
  event-title: Strange Loop Conference
  issued:
    - year: 2014
      month: 9
      day: 20
  title: Testing Distributed Systems with Deterministic Simulation
  type: speech
  URL: https://www.youtube.com/watch?v=4fFDFbi3toc

- id: winestockEternalMainframe
  abstract: >-
    In the computer industry, the Wheel of Reincarnation is a pattern whereby
    specialized hardware gets spun out from the “main” system, becomes more
    powerful, then gets folded back into the main system. As the linked Jargon
    File entry points out, several generations of this effect have been observed
    in graphics and floating-point coprocessors.


    In this essay, I note an analogous pattern taking place, not in peripherals
    of a computing platform, but in the most basic kinds of “computing
    platform.” And this pattern is being driven as much by the desire for
    “freedom” as by any technical consideration.
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Winestock
      given: Rudolf
  citation-key: winestockEternalMainframe
  title: The Eternal Mainframe
  type: post-weblog
  URL: http://www.winestockwebdesign.com/Essays/Eternal_Mainframe.html

- id: winestockLispCurse
  abstract: The expressive power of Lisp has drawbacks.
  accessed:
    - year: 2022
      month: 6
      day: 24
  author:
    - family: Winestock
      given: Rudolf
  citation-key: winestockLispCurse
  title: The Lisp Curse
  type: webpage
  URL: http://www.winestockwebdesign.com/Essays/Lisp_Curse.html

- id: winterRetrospectiveStudyOne2022
  abstract: >-
    Most software engineering research involves the development of a prototype,
    a proof of concept, or a measurement apparatus. Together with the data
    collected in the research process, they are collectively referred to as
    research artifacts and are subject to artifact evaluation (AE) at scientific
    conferences. Since its initiation in the SE community at ESEC/FSE 2011, both
    the goals and the process of AE have evolved and today expectations towards
    AE are strongly linked with reproducible research results and reusable tools
    that other researchers can build their work on. However, to date little
    evidence has been provided that artifacts which have passed AE actually live
    up to these high expectations, i.e., to which degree AE processes contribute
    to AE's goals and whether the overhead they impose is justified. We aim to
    fill this gap by providing an in-depth analysis of research artifacts from a
    decade of software engineering (SE) and programming languages (PL)
    conferences, based on which we reflect on the goals and mechanisms of AE in
    our community. In summary, our analyses (1) suggest that articles with
    artifacts do not generally have better visibility in the community, (2)
    provide evidence how evaluated and not evaluated artifacts differ with
    respect to different quality criteria, and (3) highlight opportunities for
    further improving AE processes.
  accessed:
    - year: 2023
      month: 1
      day: 31
  author:
    - family: Winter
      given: Stefan
    - family: Timperley
      given: Christopher S.
    - family: Hermann
      given: Ben
    - family: Cito
      given: Jürgen
    - family: Bell
      given: Jonathan
    - family: Hilton
      given: Michael
    - family: Beyer
      given: Dirk
  citation-key: winterRetrospectiveStudyOne2022
  collection-title: ESEC/FSE 2022
  container-title: >-
    Proceedings of the 30th ACM Joint European Software Engineering Conference
    and Symposium on the Foundations of Software Engineering
  DOI: 10.1145/3540250.3549172
  event-place: New York, NY, USA
  ISBN: 978-1-4503-9413-0
  issued:
    - year: 2022
      month: 11
      day: 9
  note: 'interest: 90'
  page: 145–156
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: A retrospective study of one decade of artifact evaluations
  type: paper-conference
  URL: https://doi.org/10.1145/3540250.3549172

- id: wirickLojbanToolEncoding2005
  abstract: >-
    As the Web evolves, the problem of enabling software to extract semantics
    from prose becomes increasingly important. Current solutions for
    representing semantic concepts mostly involve large concept hierarchies
    called ontologies in which the classes have more or less arbitrary names
    based on English words. Parsing English prose into documents that reference
    these ontologies is problematic and would rely heavily on inadequately

    accurate techniques based on artifical intelligence.


    I explore a new avenue, which involves an artificial, logical, spoken
    language called Lojban, a language that is special in that it can be used to
    write both prose and unambiguous logical statements. I demonstrate that, for
    at least a simple but non-trivial subset of Lojban, parsing prose into
    documents that reference a Lojban ontology is automatic, even trivial by
    some comparisons.


    Work that builds on this would include, but not be limited to, expanding the
    subset of Lojban I present and finding ways to use Lojban as an intermediate
    step for semantic parsing of certain types of prose in natural languages.
  author:
    - family: Wirick
      given: Brandon
  citation-key: wirickLojbanToolEncoding2005
  genre: Master of Science in Computer Science
  issued:
    - year: 2005
      month: 11
  language: en
  note: 'interest: 85'
  publisher: California Polytechnic State University
  source: Zotero
  title: Lojban as a tool for encoding prose on the semantic web
  type: thesis
  URL: https://mw.lojban.org/images/e/e1/wirick.pdf

- id: woodruffStaticAnalysisScale2019
  abstract: >-
    How Instagram develops and uses linting and codemod tools based on LibCST to
    maintain a modern codebase at scale.
  accessed:
    - year: 2022
      month: 4
      day: 18
  author:
    - family: Woodruff
      given: Benjamin
  citation-key: woodruffStaticAnalysisScale2019
  container-title: Medium
  issued:
    - year: 2019
      month: 8
      day: 16
  language: en
  title: 'Static Analysis at Scale: An Instagram Story'
  title-short: Static Analysis at Scale
  type: post-weblog
  URL: >-
    https://instagram-engineering.com/static-analysis-at-scale-an-instagram-story-8f498ab71a0c

- id: wordenSelfLickingIceCream1992
  accessed:
    - year: 2024
      month: 4
      day: 18
  author:
    - family: Worden
      given: S. P.
  citation-key: wordenSelfLickingIceCream1992
  collection-title: Cool stars, stellar systems, and the sun
  issued:
    - year: 1992
      month: 1
      day: 1
  note: 'ADS Bibcode: 1992ASPC...26..599W'
  page: '599'
  source: NASA ADS
  title: On Self-Licking Ice Cream Cones
  type: article-journal
  URL: https://ui.adsabs.harvard.edu/abs/1992ASPC...26..599W
  volume: '26'

- id: wrattenReproducibleScalableShareable2021
  abstract: >-
    The rapid growth of high-throughput technologies has transformed biomedical
    research. With the increasing amount and complexity of data, scalability and
    reproducibility have become essential not just for experiments, but also for
    computational analysis. However, transforming data into information involves
    running a large number of tools, optimizing parameters, and integrating
    dynamically changing reference data. Workflow managers were developed in
    response to such challenges. They simplify pipeline development, optimize
    resource usage, handle software installation and versions, and run on
    different compute platforms, enabling workflow portability and sharing. In
    this Perspective, we highlight key features of workflow managers, compare
    commonly used approaches for bioinformatics workflows, and provide a guide
    for computational and noncomputational users. We outline community-curated
    pipeline initiatives that enable novice and experienced users to perform
    complex, best-practice analyses without having to manually assemble
    workflows. In sum, we illustrate how workflow managers contribute to making
    computational analysis in biomedical research shareable, scalable, and
    reproducible.
  accessed:
    - year: 2022
      month: 7
      day: 7
  author:
    - family: Wratten
      given: Laura
    - family: Wilm
      given: Andreas
    - family: Göke
      given: Jonathan
  citation-key: wrattenReproducibleScalableShareable2021
  container-title: Nature Methods
  container-title-short: Nat Methods
  DOI: 10.1038/s41592-021-01254-9
  ISSN: 1548-7091, 1548-7105
  issue: '10'
  issued:
    - year: 2021
      month: 10
  language: en
  page: 1161-1168
  source: DOI.org (Crossref)
  title: >-
    Reproducible, scalable, and shareable analysis pipelines with bioinformatics
    workflow managers
  type: article-journal
  URL: https://www.nature.com/articles/s41592-021-01254-9
  volume: '18'

- id: wroeRecyclingWorkflowsServices2007
  abstract: >-
    Scientific workflows are becoming a valuable tool for scientists to capture
    and automate e-Science procedures. Their success brings the opportunity to
    publish, share, reuse and re-purpose this explicitly captured knowledge.
    Within the $^my$Grid project, we have identified key resources that can be
    shared including complete workflows, fragments of workflows and constituent
    services. We have examined the alternative ways that these resources can be
    described by their authors (and subsequent users) and developed a unified
    descriptive model to support their later discovery. By basing this model on
    existing standards, we have been able to extend existing Web service and
    Semantic Web service infrastructure whilst still supporting the specific
    needs of the e-Scientist. The $^my$Grid components enable a workflow
    life-cycle that extends beyond execution to include the discovery of
    previous relevant designs, the reuse of those designs and their subsequent
    publication. Experience with example groups of scientists indicates that
    this cycle is valuable. The growing number of workflows and services mean
    more work is needed to support the user in effective ranking of search
    results and to support the re-purposing process. Copyright © 2006 John Wiley
    & Sons, Ltd.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Wroe
      given: Chris
    - family: Goble
      given: Carole
    - family: Goderis
      given: Antoon
    - family: Lord
      given: Phillip
    - family: Miles
      given: Simon
    - family: Papay
      given: Juri
    - family: Alper
      given: Pinar
    - family: Moreau
      given: Luc
  citation-key: wroeRecyclingWorkflowsServices2007
  container-title: 'Concurrency and Computation: Practice and Experience'
  DOI: 10.1002/cpe.1050
  ISSN: 1532-0634
  issue: '2'
  issued:
    - year: 2007
  language: en
  note: 'interest: 92'
  page: 181-194
  source: Wiley Online Library
  title: Recycling workflows and services through discovery and reuse
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.1050
  volume: '19'

- id: XDOCInterestingapplications
  accessed:
    - year: 2022
      month: 9
      day: 6
  citation-key: XDOCInterestingapplications
  note: 'interest: 56'
  title: XDOC — Interesting-applications
  type: webpage
  URL: >-
    https://www.cs.utexas.edu/users/moore/acl2/current/manual/index.html?topic=ACL2____INTERESTING-APPLICATIONS

- id: xingKnuthPlassLineWrapping
  abstract: Where do we break lines to fit them to a page?
  author:
    - family: Xing
      given: A
  citation-key: xingKnuthPlassLineWrapping
  title: Knuth-Plass Line Wrapping Algorithm
  type: speech
  URL: https://www.students.cs.ubc.ca/~cs-490/2015W2/lectures/Knuth.pdf

- id: xuDXTDarshanEXtended2017
  abstract: >-
    The U.S. Department of Energy's Office of Scientific and Technical
    Information
  accessed:
    - year: 2023
      month: 10
      day: 27
  author:
    - family: Xu
      given: Cong
    - family: Snyder
      given: Shane
    - family: Venkatesan
      given: Vishwanath
    - family: Carns
      given: Philip
    - family: Kulkarni
      given: Omkar
    - family: Byna
      given: Suren
    - family: Sisneros
      given: Roberto
    - family: Chadalavada
      given: Kalyana
  citation-key: xuDXTDarshanEXtended2017
  issued:
    - year: 2017
      month: 5
      day: 8
  language: English
  publisher: Argonne National Lab. (ANL), Argonne, IL (United States)
  source: www.osti.gov
  title: 'DXT: Darshan eXtended Tracing'
  title-short: DXT
  type: report
  URL: https://www.osti.gov/biblio/1392598

- id: xuEfficientCheckpointingJava2007
  abstract: >-
    Checkpointing and replaying is an attractive technique that has been used
    widely at the operating/runtime system level to provide fault tolerance.
    Applying such a technique at the application level can benefit a range of
    software engineering tasks such as testing of long-running programs,
    automated debugging, and dynamic slicing. We propose a
    checkpointing/replaying technique for Java that operates purely at the
    language level, without the need for JVM-level or OS-level support. At the
    core of our approach are static analyses that select, at certain program
    points, a safe subset of the program state to capture and replay. Irrelevant
    statements before the checkpoint are eliminated using
    control-dependence-based slicing; the remaining statements together with the
    captured run-time values are used to indirectly recreate the call stack of
    the original program at the checkpoint. At the checkpoint itself and at
    certain subsequent program points, the replaying version restores parts of
    the program state that are necessary for execution of the surrounding
    method. Our experimental studies indicate that the proposed static and
    dynamic analyses have the potential to reduce significantly the execution
    time for replaying, with low run-time overhead for checkpointing.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Xu
      given: Guoqing
    - family: Rountev
      given: Atanas
    - family: Tang
      given: Yan
    - family: Qin
      given: Feng
  citation-key: xuEfficientCheckpointingJava2007
  collection-title: ESEC-FSE '07
  container-title: >-
    Proceedings of the the 6th joint meeting of the European software
    engineering conference and the ACM SIGSOFT symposium on The foundations of
    software engineering
  DOI: 10.1145/1287624.1287638
  event-place: New York, NY, USA
  ISBN: 978-1-59593-811-4
  issued:
    - year: 2007
      month: 9
      day: 7
  note: 'interest: 51'
  page: 85–94
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Efficient checkpointing of java software using context-sensitive capture and
    replay
  type: paper-conference
  URL: https://doi.org/10.1145/1287624.1287638

- id: yangDEEPProvenanceAwareExecutable2012
  abstract: >-
    The concept of executable documents is attracting growing interest from both
    academics and publishers since it is a promising technology for the the
    dissemination of scientific results. Provenance is a kind of metadata that
    provides a rich description of the derivation history of data products
    starting from their original sources. It has been used in many different
    e-Science domains and has shown great potential in enabling reproducibility
    of scientific results. However, while both executable documents and
    provenance are aimed at enhancing the dissemination of scientific results,
    little has been done to explore the integration of both techniques. In this
    paper, we introduce the design and development of Deep, an executable
    document environment that generates scientific results dynamically and
    interactively, and also records the provenance for these results in the
    document. In this system, provenance is exposed to users via an interface
    that provides them with an alternative way of navigating the executable
    document. In addition, we make use of the provenance to offer a document
    rollback facility to users and help to manage the system’s dynamic
    resources.
  accessed:
    - year: 2022
      month: 7
      day: 8
  author:
    - family: Yang
      given: Huanjia
    - family: Michaelides
      given: Danius T.
    - family: Charlton
      given: Chris
    - family: Browne
      given: William J.
    - family: Moreau
      given: Luc
  citation-key: yangDEEPProvenanceAwareExecutable2012
  collection-editor:
    - family: Hutchison
      given: David
    - family: Kanade
      given: Takeo
    - family: Kittler
      given: Josef
    - family: Kleinberg
      given: Jon M.
    - family: Mattern
      given: Friedemann
    - family: Mitchell
      given: John C.
    - family: Naor
      given: Moni
    - family: Nierstrasz
      given: Oscar
    - family: Pandu Rangan
      given: C.
    - family: Steffen
      given: Bernhard
    - family: Sudan
      given: Madhu
    - family: Terzopoulos
      given: Demetri
    - family: Tygar
      given: Doug
    - family: Vardi
      given: Moshe Y.
    - family: Weikum
      given: Gerhard
  container-title: Provenance and Annotation of Data and Processes
  DOI: 10.1007/978-3-642-34222-6_3
  editor:
    - family: Groth
      given: Paul
    - family: Frew
      given: James
  event-place: Berlin, Heidelberg
  ISBN: 978-3-642-34221-9 978-3-642-34222-6
  issued:
    - year: 2012
  page: 24-38
  publisher: Springer Berlin Heidelberg
  publisher-place: Berlin, Heidelberg
  source: DOI.org (Crossref)
  title: 'DEEP: A Provenance-Aware Executable Document System'
  title-short: DEEP
  type: chapter
  URL: http://link.springer.com/10.1007/978-3-642-34222-6_3
  volume: '7525'

- id: yangDeterminismOverratedWhat2013
  abstract: >-
    Our accelerating computational demand and the rise of multicore hardware
    have made parallel programs, especially shared-memory multithreaded
    programs, increasingly pervasive and critical. Yet, these programs remain
    extremely difficult to write, test, analyze, debug, and verify. Conventional
    wisdom has attributed these difficulties to nondeterminism, and researchers
    have recently dedicated much effort to bringing determinism into
    multithreading. In this paper, we argue that determinism is not as useful as
    commonly perceived: it is neither sufficient nor necessary for reliability.
    We present our view on why multithreaded programs are difficult to get
    right, describe a promising approach we call stable multithreading to
    dramatically improve reliability, and summarize our last four years'
    research on building and applying stable multithreading systems.
  accessed:
    - year: 2022
      month: 10
      day: 18
  author:
    - family: Yang
      given: Junfeng
    - family: Cui
      given: Heming
    - family: Wu
      given: Jingyue
  citation-key: yangDeterminismOverratedWhat2013
  collection-title: HotPar'13
  container-title: Proceedings of the 5th USENIX Conference on Hot Topics in Parallelism
  event-place: USA
  issued:
    - year: 2013
      month: 6
      day: 24
  note: 'interest: 87'
  page: '11'
  publisher: USENIX Association
  publisher-place: USA
  source: ACM Digital Library
  title: >-
    Determinism is overrated: what really makes multithreaded programs hard to
    get right and what can be done about it?
  title-short: Determinism is overrated
  type: paper-conference

- id: yangUISCOPEAccurateInstrumentationfree2020
  abstract: >-
    Existing attack investigation solutions for GUI applications suffer from a
    few limitations such as inaccuracy (because of the dependence explosion
    problem), requiring instrumentation, and providing very low visibility. Such
    limitations have hindered their widespread and practical deployment. In this
    paper, we present UISCOPE, a novel accurate, instrumentationfree, and
    visible attack investigation system for GUI applications. The core idea of
    UISCOPE is to perform causality analysis on both UI elements/events which
    represent users’ perspective and low-level system events which provide
    detailed information of what happens under the hood, and then correlate
    system events with UI events to provide high accuracy and visibility. Long
    running processes are partitioned to individual UI transitions, to which
    low-level system events are attributed, making the results accurate. The
    produced graphs contain (causally related) UI elements with which users are
    very familiar, making them easily accessible. We deployed UISCOPE on 7
    machines for a week, and also utilized UISCOPE to conduct an investigation
    of 6 realworld attacks. Our evaluation shows that compared to existing
    works, UISCOPE introduces neglibible overhead (less than 1% runtime overhead
    and 3.05 MB event logs per hour on average) while UISCOPE can precisely
    identify attack provenance while offering users thorough visibility into the
    attack context.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Yang
      given: Runqing
    - family: Ma
      given: Shiqing
    - family: Xu
      given: Haitao
    - family: Zhang
      given: Xiangyu
    - family: Chen
      given: Yan
  citation-key: yangUISCOPEAccurateInstrumentationfree2020
  container-title: Proceedings 2020 Network and Distributed System Security Symposium
  DOI: 10.14722/ndss.2020.24329
  event-place: San Diego, CA
  event-title: Network and Distributed System Security Symposium
  ISBN: 978-1-891562-61-7
  issued:
    - year: 2020
  language: en
  publisher: Internet Society
  publisher-place: San Diego, CA
  source: DOI.org (Crossref)
  title: >-
    UISCOPE: Accurate, Instrumentation-free, and Visible Attack Investigation
    for GUI Applications
  title-short: UISCOPE
  type: paper-conference
  URL: https://www.ndss-symposium.org/wp-content/uploads/2020/02/24329.pdf

- id: yiEvaluatingBenchmarkSubsetting2006
  abstract: >-
    To reduce the simulation time to a tractable amount or due to compilation
    (or other related) problems, computer architects often simulate only a
    subset of the benchmarks in a benchmark suite. However, if the architect
    chooses a subset of benchmarks that is not representative, the subsequent
    simulation results will, at best, be misleading or, at worst, yield
    incorrect conclusions. To address this problem, computer architects have
    recently proposed several statistically-based approaches to subset a
    benchmark suite. While some of these approaches are well-grounded
    statistically, what has not yet been thoroughly evaluated is the: 1)
    absolute accuracy; 2) relative accuracy across a range of processor and
    memory subsystem enhancements; and 3) representativeness and coverage of
    each approach for a range of subset sizes. Specifically, this paper
    evaluates statistically-based subsetting approaches based on principal
    components analysis (PCA) and the Plackett and Burman (P&B) design, in
    addition to prevailing approaches such as integer vs. floating-point, core
    vs. memory-bound, by language, and at random. Our results show that the two
    statistically-based approaches, PCA and P&B, have the best absolute and
    relative accuracy for CPI and energy-delay product (EDP), produce subsets
    that are the most representative, and choose benchmark and input set pairs
    that are most well-distributed across the benchmark space. To achieve a 5%
    absolute CPI and EDP error, across a wide range of configurations, PCA and
    P&B typically need about 17 benchmark and input set pairs, while the other
    five approaches often choose more than 30 benchmark and input set pairs
  accessed:
    - year: 2024
      month: 1
      day: 22
  author:
    - family: Yi
      given: Joshua J.
    - family: Sendag
      given: Resit
    - family: Eeckhout
      given: Lieven
    - family: Joshi
      given: Ajay
    - family: Lilja
      given: David J.
    - family: John
      given: Lizy K.
  citation-key: yiEvaluatingBenchmarkSubsetting2006
  container-title: 2006 IEEE International Symposium on Workload Characterization
  DOI: 10.1109/IISWC.2006.302733
  event-title: 2006 IEEE International Symposium on Workload Characterization
  issued:
    - year: 2006
      month: 10
  page: 93-104
  source: IEEE Xplore
  title: Evaluating Benchmark Subsetting Approaches
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/4086137

- id: yinPanoramaCapturingSystemwide2007
  abstract: >-
    Malicious programs spy on users' behavior and compromise their privacy. Even
    software from reputable vendors, such as Google Desktop and Sony DRM media
    player, may perform undesirable actions. Unfortunately, existing techniques
    for detecting malware and analyzing unknown code samples are insufficient
    and have significant shortcomings. We observe that malicious information
    access and processing behavior is the fundamental trait of numerous malware
    categories breaching users' privacy (including keyloggers, password thieves,
    network sniffers, stealth backdoors, spyware and rootkits), which separates
    these malicious applications from benign software. We propose a system,
    Panorama, to detect and analyze malware by capturing this fundamental trait.
    In our extensive experiments, Panorama successfully detected all the malware
    samples and had very few false positives. Furthermore, by using Google
    Desktop as a case study, we show that our system can accurately capture its
    information access and processing behavior, and we can confirm that it does
    send back sensitive information to remote servers in certain settings. We
    believe that a system such as Panorama will offer indispensable assistance
    to code analysts and malware researchers by enabling them to quickly
    comprehend the behavior and innerworkings of an unknown sample.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Yin
      given: Heng
    - family: Song
      given: Dawn
    - family: Egele
      given: Manuel
    - family: Kruegel
      given: Christopher
    - family: Kirda
      given: Engin
  citation-key: yinPanoramaCapturingSystemwide2007
  collection-title: CCS '07
  container-title: >-
    Proceedings of the 14th ACM conference on Computer and communications
    security
  DOI: 10.1145/1315245.1315261
  event-place: New York, NY, USA
  ISBN: 978-1-59593-703-2
  issued:
    - year: 2007
      month: 10
      day: 28
  page: 116–127
  publisher: Association for Computing Machinery
  publisher-place: New York, NY, USA
  source: ACM Digital Library
  title: >-
    Panorama: capturing system-wide information flow for malware detection and
    analysis
  title-short: Panorama
  type: paper-conference
  URL: https://dl.acm.org/doi/10.1145/1315245.1315261

- id: yooRegressionTestingMinimization2012
  abstract: >-
    Regression testing is a testing activity that is performed to provide
    confidence that changes do not harm the existing behaviour of the software.
    Test suites tend to grow in size as software evolves, often making it too
    costly to execute entire test suites. A number of different approaches have
    been studied to maximize the value of the accrued test suite: minimization,
    selection and prioritization. Test suite minimization seeks to eliminate
    redundant test cases in order to reduce the number of tests to run. Test
    case selection seeks to identify the test cases that are relevant to some
    set of recent changes. Test case prioritization seeks to order test cases in
    such a way that early fault detection is maximized. This paper surveys each
    area of minimization, selection and prioritization technique and discusses
    open problems and potential directions for future research. Copyright © 2010
    John Wiley & Sons, Ltd.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Yoo
      given: S.
    - family: Harman
      given: M.
  citation-key: yooRegressionTestingMinimization2012
  container-title: Software Testing, Verification & Reliability
  container-title-short: Softw. Test. Verif. Reliab.
  DOI: 10.1002/stv.430
  ISSN: 0960-0833
  issue: '2'
  issued:
    - year: 2012
      month: 3
      day: 1
  note: 'interest: 76'
  page: 67–120
  source: March 2012
  title: 'Regression testing minimization, selection and prioritization: a survey'
  title-short: Regression testing minimization, selection and prioritization
  type: article-journal
  URL: https://doi.org/10.1002/stv.430
  volume: '22'

- id: yooRegressionTestingMinimization2013
  abstract: >-
    Regression testing is a testing activity that is performed to provide
    confidence that changes do not harm the existing behaviour of the software.
    Test suites tend to grow in size as software evolves, often making it too
    costly to execute entire test suites. A number of different approaches have
    been studied to maximize the value of the accrued test suite: minimization,
    selection and prioritization. Test suite minimization seeks to eliminate
    redundant test cases in order to reduce the number of tests to run. Test
    case selection seeks to identify the test cases that are relevant to some
    set of recent changes. Test case prioritization seeks to order test cases in
    such a way that early fault detection is maximized. This paper surveys each
    area of minimization, selection and prioritization technique and discusses
    open problems and potential directions for future research.
  accessed:
    - year: 2022
      month: 4
      day: 7
  author:
    - family: Yoo
      given: S.
    - family: Harman
      given: M.
  citation-key: yooRegressionTestingMinimization2013
  container-title: Software Testing, Verification and Reliability
  DOI: 10.1002/stvr.430
  ISSN: 1099-1689
  issue: '2'
  issued:
    - year: 2013
      month: 10
      day: 11
  language: en
  page: 67-120
  source: Wiley Online Library
  title: 'Regression testing minimization, selection and prioritization: a survey'
  title-short: Regression testing minimization, selection and prioritization
  type: article-journal
  URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/stvr.430
  volume: '22'

- id: yorkBuildCloudAccessing2011
  author:
    - family: York
      given: Nathan
  citation-key: yorkBuildCloudAccessing2011
  container-title: Google Engineering Tools
  issued:
    - year: 2011
      month: 6
      day: 21
  title: 'Build in the Cloud: Accessing Source Code'
  type: post-weblog
  URL: >-
    https://google-engtools.blogspot.com/2011/06/build-in-cloud-accessing-source-code.html

- id: yorkBuildCloudDistributing2011
  author:
    - family: York
      given: Nathan
  citation-key: yorkBuildCloudDistributing2011
  genre: Google Engineering Tools
  issued:
    - year: 2011
      month: 9
      day: 22
  title: 'Build in the Cloud: Distributing Build Steps'
  type: post-weblog
  URL: >-
    https://google-engtools.blogspot.com/2011/09/build-in-cloud-distributing-build-steps.html

- id: youngGoDirectlyJail2006
  accessed:
    - year: 2023
      month: 6
      day: 7
  author:
    - family: Young
      given: Malcom C.
  citation-key: youngGoDirectlyJail2006
  container-title: Journal of Criminal Law and Criminology
  issued:
    - year: 2006
      month: 6
      day: 22
  title: >-
    Go Directly to Jail: The Criminalization of Almost Everything. - Free Online
    Library
  type: article-journal
  URL: >-
    https://www.thefreelibrary.com/Go+Directly+to+Jail%3a+The+Criminalization+of+Almost+Everything.-a0157035815

- id: youngRecommendedRequirementsGathering2002
  abstract: >-
    This article provides suggested conditions for performing requirements
    gathering and recommended requirements gathering practices. The author has
    conducted an extensive review of industry literature and combined this with
    the practical experiences of a set of requirements analysts who have
    supported dozens of projects. The sidebar on page 10 summarizes a set of

    recommended requirements gathering practices. Involving customers and users
    throughout the development effort results in a better understanding of the
    real needs. Requirements activities should be performed throughout the
    development effort, not just

    at the beginning of a project
  author:
    - family: Young
      given: Ralph R.
  citation-key: youngRecommendedRequirementsGathering2002
  container-title: 'Crosstalk: The Journal of Defense Software Engineering'
  issued:
    - year: 2002
      month: 4
  page: 9-12
  title: Recommended Requirements Gathering Practices
  type: article-journal

- id: youngWhyCurrentPublication2008
  abstract: >-
    John Ioannidis and colleagues argue that the current system of publication
    in biomedical research provides a distorted view of the reality of
    scientific data.
  accessed:
    - year: 2022
      month: 9
      day: 6
  author:
    - family: Young
      given: Neal S.
    - family: Ioannidis
      given: John P. A.
    - family: Al-Ubaydli
      given: Omar
  citation-key: youngWhyCurrentPublication2008
  container-title: PLOS Medicine
  container-title-short: PLOS Medicine
  DOI: 10.1371/journal.pmed.0050201
  ISSN: 1549-1676
  issue: '10'
  issued:
    - year: 2008
      month: 10
      day: 7
  language: en
  note: 'interest: 83'
  page: e201
  publisher: Public Library of Science
  source: PLoS Journals
  title: Why Current Publication Practices May Distort Science
  type: article-journal
  URL: >-
    https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0050201
  volume: '5'

- id: yuanSimpleTestingCan2014
  abstract: >-
    Large, production quality distributed systems still fail periodically, and
    do so sometimes catastrophically, where most or all users experience an
    outage or data loss. We present the result of a comprehensive study
    investigating 198 randomly selected, user-reported failures that occurred on
    Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce,
    and Redis, with the goal of understanding how one or multiple faults
    eventually evolve into a user-visible failure. We found that from a testing
    point of view, almost all failures require only 3 or fewer nodes to
    reproduce, which is good news considering that these services typically run
    on a very large number of nodes. However, multiple inputs are needed to
    trigger the failures with the order between them being important. Finally,
    we found the error logs of these systems typically contain sufficient data
    on both the errors and the input events that triggered the failure, enabling
    the diagnose and the reproduction of the production failures. We found the
    majority of catastrophic failures could easily have been prevented by
    performing simple testing on error handling code – the last line of defense
    – even without an understanding of the software design. We extracted three
    simple rules from the bugs that have lead to some of the catastrophic
    failures, and developed a static checker, Aspirator, capable of locating
    these bugs. Over 30% of the catastrophic failures would have been prevented
    had Aspirator been used and the identified bugs fixed. Running Aspirator on
    the code of 9 distributed systems located 143 bugs and bad practices that
    have been fixed or confirmed by the developers.
  author:
    - family: Yuan
      given: Ding
    - family: Luo
      given: Yu
    - family: Zhuang
      given: Xin
    - family: Rodrigues
      given: Guilherme Renna
    - family: Zhao
      given: Xu
    - family: Zhang
      given: Yongle
    - family: Jain
      given: Pranay U.
    - family: Stumm
      given: Michael
  citation-key: yuanSimpleTestingCan2014
  container-title: >-
    11th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    14)
  event-place: Broomfield, CO
  ISBN: 978-1-931971-16-4
  issued:
    - year: 2014
      month: 10
  page: 249–265
  publisher: USENIX Association
  publisher-place: Broomfield, CO
  title: >-
    Simple Testing Can Prevent Most Critical Failures: An Analysis of Production
    Failures in Distributed Data-Intensive Systems
  type: paper-conference
  URL: >-
    https://www.usenix.org/conference/osdi14/technical-sessions/presentation/yuan

- id: zhangLocalizingFailureinducingProgram2011
  abstract: >-
    Keeping evolving systems fault free is hard. Change impact analysis is a
    well-studied methodology for finding faults in evolving systems. For
    example, in order to help developers identify failure-inducing edits,
    Chianti extracts program edits as atomic changes between different program
    versions, selects affected tests, and determines a subset of those changes
    that might induce test failures. However, identifying real regression faults
    is challenging for developers since the number of affecting changes related
    to each test failure may still be too large for manual inspection. This
    paper presents a novel approach FAULTTRACER which ranks program edits in
    order to reduce developers' effort in manually inspecting all affecting
    changes. FAULTTRACER adapts spectrum-based fault localization techniques and
    applies them in tandem with an enhanced change impact analysis that uses
    Extended Call Graphs to identify failure-inducing edits more precisely. We
    evaluate FAULTTRACER using 23 versions of 4 real-world Java programs from
    the Software Infrastructure Repository. The experimental results show that
    FAULTTRACER outperforms Chianti in selecting affected tests (slightly
    better, but handles safety problems of Chianti) as well as in determining
    affecting changes (with an improvement of approximately 20%). By ranking the
    affecting changes using spectrum-based test behavior profile, for 14 out of
    22 studied failures, FAULTTRACER places a real regression fault within top 3
    atomic changes, significantly reducing developers' effort in inspecting
    potential failure-inducing edits.
  author:
    - family: Zhang
      given: Lingming
    - family: Kim
      given: Miryung
    - family: Khurshid
      given: Sarfraz
  citation-key: zhangLocalizingFailureinducingProgram2011
  container-title: 2011 27th IEEE International Conference on Software Maintenance (ICSM)
  DOI: 10.1109/ICSM.2011.6080769
  event-title: 2011 27th IEEE International Conference on Software Maintenance (ICSM)
  ISSN: 1063-6773
  issued:
    - year: 2011
      month: 9
  page: 23-32
  source: IEEE Xplore
  title: Localizing failure-inducing program edits based on spectrum information
  type: paper-conference

- id: zhaoApplyingVirtualData2006
  abstract: >-
    In many domains of science, engineering, and commerce, data analysis systems
    are employed to derive new data (and ultimately, one hopes, knowledge) from
    datasets describing experimental results or simulated phenomena. To support
    such analyses, we have developed a “virtual data system” that allows users
    first to define, then to invoke, and finally explore the provenance of
    procedures (and workflows comprising multiple procedure calls) that perform
    such data derivations. The underlying execution model is “functional” in the
    sense that procedures read (but do not modify) their input and produce
    output via deterministic computations. This property makes it
    straightforward for the virtual data system to record not only the recipe
    for producing any given data object but also sufficient information about
    the environment in which the recipe has been executed, all with sufficient
    fidelity that the steps used to create a data object can be re-executed to
    reproduce the data object at a later time or a different location. The
    virtual data system maintains this information in an integrated schema
    alongside semantic annotations, and thus enables a powerful query capability
    in which the rich semantic information implied by knowledge of the structure
    of data derivation procedures can be exploited to provide an information
    environment that fuses recipe, history, and application-specific semantics.
    We provide here an overview of this integration, the queries and
    transformations that it enables, and examples of how these capabilities can
    serve scientific processes.
  author:
    - family: Zhao
      given: Yong
    - family: Wilde
      given: Michael
    - family: Foster
      given: Ian
  citation-key: zhaoApplyingVirtualData2006
  collection-title: Lecture Notes in Computer Science
  container-title: Provenance and Annotation of Data
  DOI: 10.1007/11890850_16
  editor:
    - family: Moreau
      given: Luc
    - family: Foster
      given: Ian
  event-place: Berlin, Heidelberg
  ISBN: 978-3-540-46303-0
  issued:
    - year: 2006
  language: en
  page: 148-161
  publisher: Springer
  publisher-place: Berlin, Heidelberg
  source: Springer Link
  title: Applying the Virtual Data Provenance Model
  type: paper-conference

- id: zhaoWhyWorkflowsBreak2012
  abstract: >-
    Workflows provide a popular means for preserving scientific methods by
    explicitly encoding their process. However, some of them are subject to a
    decay in their ability to be re-executed or reproduce the same results over
    time, largely due to the volatility of the resources required for workflow
    executions. This paper provides an analysis of the root causes of workflow
    decay based on an empirical study of a collection of Taverna workflows from
    the myExperiment repository. Although our analysis was based on a specific
    type of workflow, the outcomes and methodology should be applicable to
    workflows from other systems, at least those whose executions also rely
    largely on accessing third-party resources. Based on our understanding about
    decay we recommend a minimal set of auxiliary resources to be preserved
    together with the workflows as an aggregation object and provide a software
    tool for end-users to create such aggregations and to assess their
    completeness
  author:
    - family: Zhao
      given: Jun
    - family: Gomez-Perez
      given: Jose Manuel
    - family: Belhajjame
      given: Khalid
    - family: Klyne
      given: Graham
    - family: Garcia-cuesta
      given: Esteban
    - family: Garrido
      given: Aleix
    - family: Hettne
      given: Kristina
    - family: Roos
      given: Marco
    - family: De Roure
      given: David
    - family: Goble
      given: Carole
  citation-key: zhaoWhyWorkflowsBreak2012
  container-title: 2012 IEEE 8th International Conference on E-Science (e-Science)
  DOI: 10.1109/eScience.2012.6404482
  event-place: Chicago, IL
  event-title: 2012 IEEE 8th International Conference on E-Science (e-Science)
  issued:
    - year: 2012
      month: 10
  page: '9'
  publisher: IEEE
  publisher-place: Chicago, IL
  title: Why workflows break — understanding and combating decay in Taverna workflows
  type: paper-conference
  URL: >-
    https://www.research.manchester.ac.uk/portal/en/publications/why-workflows-break--understanding-and-combating-decay-in-taverna-workflows(cba81ca4-e92c-408e-8442-383d1f15fcdf)/export.html

- id: zhuReproducibilitySoftwareDefect2023
  abstract: >-
    Software defect datasets are crucial to facilitating the evaluation and
    comparison of techniques in fields such as fault localization, test
    generation, and automated program repair. However, the reproducibility of
    software defect artifacts is not immune to breakage. In this paper, we
    conduct a study on the reproducibility of software defect artifacts. First,
    we study five state-of-the-art Java defect datasets. Despite the multiple
    strategies applied by dataset maintainers to ensure reproducibility, all
    datasets are prone to breakages. Second, we conduct a case study in which we
    systematically test the reproducibility of 1,795 software artifacts during a
    13-month period. We find that 62.6% of the artifacts break at least once,
    and 15.3% artifacts break multiple times. We manually investigate the root
    causes of breakages and handcraft 10 patches, which are automatically
    applied to 1,055 distinct artifacts in 2,948 fixes. Based on the nature of
    the root causes, we propose automated dependency caching and artifact
    isolation to prevent further breakage. In particular, we show that isolating
    artifacts to eliminate external dependencies increases reproducibility to
    95% or higher, which is on par with the level of reproducibility exhibited
    by the most reliable manually curated dataset.
  accessed:
    - year: 2023
      month: 11
      day: 7
  author:
    - family: Zhu
      given: Hao-Nan
    - family: Rubio-González
      given: Cindy
  citation-key: zhuReproducibilitySoftwareDefect2023
  container-title: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
  DOI: 10.1109/ICSE48619.2023.00195
  event-title: 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)
  ISSN: 1558-1225
  issued:
    - year: 2023
      month: 5
  page: 2324-2335
  source: IEEE Xplore
  title: On the Reproducibility of Software Defect Datasets
  type: paper-conference
  URL: https://ieeexplore.ieee.org/document/10172645

- id: zipperleProvenancebasedIntrusionDetection2022
  abstract: >-
    Traditional Intrusion Detection Systems (IDS) cannot cope with the
    increasing number and sophistication of cyberattacks such as Advanced
    Persistent Threats (APT). Due to their high false-positive rate and the
    required effort of security experts to validate them, incidents can remain
    undetected for up to several months. As a result, enterprises suffer from
    data loss and severe financial damage. Recent research explored data
    provenance for Host-based Intrusion Detection Systems (HIDS) as one
    promising data source to tackle this issue. Data provenance represents
    information flows between system entities as Direct Acyclic Graph (DAG).
    Provenance-based Intrusion Detection Systems (PIDS) utilize data provenance
    to enhance the detection performance of intrusions and reduce false-alarm
    rates compared to traditional IDS. This survey demonstrates the potential of
    PIDS by providing a detailed evaluation of recent research in the field,
    proposing a novel taxonomy for PIDS, discussing current issues, and
    potential future research directions. This survey aims to help and motivate
    researchers to get started in the field of PIDS by tackling issues of data
    collection, graph summarization, intrusion detection, and developing
    real-world benchmark datasets.
  accessed:
    - year: 2023
      month: 8
      day: 23
  author:
    - family: Zipperle
      given: Michael
    - family: Gottwalt
      given: Florian
    - family: Chang
      given: Elizabeth
    - family: Dillon
      given: Tharam
  citation-key: zipperleProvenancebasedIntrusionDetection2022
  container-title: ACM Computing Surveys
  container-title-short: ACM Comput. Surv.
  DOI: 10.1145/3539605
  ISSN: 0360-0300
  issue: '7'
  issued:
    - year: 2022
      month: 12
      day: 15
  page: 135:1–135:36
  source: ACM Digital Library
  title: 'Provenance-based Intrusion Detection Systems: A Survey'
  title-short: Provenance-based Intrusion Detection Systems
  type: article-journal
  URL: https://dl.acm.org/doi/10.1145/3539605
  volume: '55'

- id: zurbuchenSMDPolicyDocument2022
  author:
    - family: Zurbuchen
      given: Thomas H.
  citation-key: zurbuchenSMDPolicyDocument2022
  issued:
    - year: 2022
      month: 9
      day: 26
  publisher: NASA Scientific Information Policy for the Science Mission Directorate
  title: SMD Policy Document SPD-41a
  type: document
...
